title,link,time,text,views,comments
Осознанная оптимизация Compose,https://habr.com/ru/companies/ozontech/articles/742854/,2023-06-23T07:01:42.000Z,"Compose — относительно молодая технология написания декларативного UI. Множество разработчиков даже не предполагают, что пишут неоптимальный код в такой критически важной части, и впоследствии это приводит к неожиданной низкой производительности и проседании метрик.Наша команда Ozon Seller также столкнулась с этой проблемой. Мы решили собрать воедино все советы и наработки по написанию оптимизированного Compose-кода. Активное применение этих советов при оптимизации существующих экранов и написании новых существенно улучшило наши метрики: длительность лага по отношению к длительности скролла (hitch rate; чем меньше, тем лучше) экранов со списками упала в среднем с 15-19 % до 5-7 % (на 90-м перцентиле). Все эти советы и наработки мы описали в этой статье. Она будет полезна и начинающим, и опытным разработчикам, в ней подробно описаны оптимизации и механизмы Compose, а также рассказано про слабо задокументированные особенности и исправления ошибок, которые есть в других статьях. Давайте же начнём.СодержаниеComposable-функцииОсновная идеяФазы ComposeАргументы в composable-функцияхСтабильные типыКлассификация типов по стабильности@Immutable и @StableПропускаемость функцийЛямбдыОптимизация пропусковПерезапускаемые функцииПерезапускаемость и пропускаемостьКогда перезапускаемость и пропускаемость не нужныОптимизация часто меняющихся элементовDerived stateОтложенное чтение состояний в composable-функцияхОтложенное чтение состояний в фазах ComposeУменьшение области рекомпозицииИспользование key и contentType в спискахМодификаторыКастомные модификаторыПереиспользование модификаторовДолгие вычисления при рекомпозицииДолгие вычисления только во ViewModel или в rememberБез долгих вычислений в геттерах UI StateКогда использовать rememberLayoutКастомные LayoutНеобоснованное изменение размера и расположенияПредварительное вычисление LayoutSubcomposeLayoutIntrinsic measurementsLookaheadLayoutПрочие советыНе менять состояние, которое только прочиталиMovableContentOfstaticCompositionLocalOf и compositionLocalOf@ReadOnlyComposableИспользовать меньше ComposeViewИспользовать последнюю версию ComposeBaseline ProfilesОтладка и мониторинг производительностиПроверка стабильности и пропускаемостиОтладка рекомпозиций и пропусковТрассировка композицийБенчмаркингИтогБонус (чит-лист)Composable-функцииВначале погрузимся немного в работу Compose и его особенности, это и поможет нам понять, почему необходимы конкретные оптимизации и как они работают.Основная идеяПостроение UI-дерева — это основная идея composable-функций. Пройдя от начала до конца функцию User(), мы получим дерево как на картинке:@Composable
fun User() {
    Row {
        Image()
        Column {
            Text()
            Text()
        }
    }
}Чтобы строить такие деревья, нужно нечто большее, чем просто декларативный код. Об этом за нас беспокоится компилятор Compose.Компилятор Compose — это плагин для компилятора Kotlin. А это значит, что, в отличие от плагина для kapt/ksp, он может модифицировать текущий код, а не просто генерировать новый. Во время компиляции он заменяет composable-функции новыми, в которые он добавляет вспомогательные конструкции и параметры, среди которых особенно важен $composer. Его можно воспринимать как контекст вызова. А процесс преобразования composable-функции можно представлять как то, что выполняет сам Kotlin с suspend-функциями.Компилятор Compose добавляет вызовы методов $composer в начале и в конце сгенерированной composable-функции (см. код ниже). Эти методы начинают и заканчивают группу, которую можно представить как узел дерева, которое строит Compose. То есть начало и конец функции — это начало и конец описания узла. Слово Restart говорит о типе группы. В статье мы не будем глубоко погружаться в типы групп, но если интересно, то можно почитать об этом в книге “Jetpack Compose internals” (Глава 2. “The Compose compiler”, параграф “Control flow group generation”).@Composable
fun User($composer: Composer) {
	$composer.startRestartGroup() // Начало группы

	// Тело функции

	$composer.endRestartGroup() // Конец группы
}На основе данных из тела функции шаг за шагом $composerстроит дерево. Это первая фаза Compose — Composition.Фазы ComposeКак и большинство других UI-инструментариев, Compose отрисовывает кадр за несколько отдельных фаз.Если мы посмотрим на систему Android View, то она имеет три основные фазы: измерение (measure), компоновка (layout) и отрисовка (drawing). У Compose фазы похожи:Composition: какой UI отобразить. Compose запускает composable-функции и создаёт описание UI.Layout: как расположить UI. Состоит из двух шагов: измерение (measurement) и размещение (placement). Элементы лейаута измеряют и располагают себя и свои дочерние элементы в двумерных координатах.Drawing: как отрисовать UI. Элементы отрисовывают себя на Canvas.Эти три фазы выполняются практически для каждого кадра, но для повышения производительности Compose может пропускать некоторые фазы, если данные для них не поменялись.Условия вызова фаз, а также примеры мест чтения состояния можно увидеть на картинке ниже. Что такое чтение состояния разберём позже, пока что просто представьте себе это как получение значения muatbleStateOf(). Подробнее о фазах можно прочитать на Android Developers.Аргументы в composable-функцияхCompose побуждает нас писать чистые функции. Это делает их более детерминированными, а также позволяет разработчикам Compose сделать первую оптимизацию — попросту не выполнять composable-функцию, если аргументы не поменялись.Введём сразу такие понятия, как композиция (composition) — построение дерева composable-функций, и рекомпозиция (recomposition) — обновление этого дерева при изменении данных.Мы подошли к ещё одному параметру, который добавляет компилятор Compose в composable-функции — $changed. Это просто число типа Int, которое является битовой картой, в которой биты отвечают за информацию об аргументах composable-функции, их изменении.// Аргументы Composable-функции после работы компилятора Compose 
@Composable
fun Header(text: String, $composer: Composer<*>, $changed: Int)Если в родительской composable-функции изменились некоторые параметры, а некоторые остались прежние, то информация о сравнении передаётся дочерним функциям через параметр $changed, чтобы они не делали лишних сравнений. Сами функции сравнивают только те аргументы, в которых не уверен родитель, или если аргументы установлены по умолчанию.Убить весь смысл сравнения могут мутабельные аргументы — объекты, которые способны изменяться (изменять свои данные). Чтобы решить эту проблему, разработчики Compose решили разделить все типы на стабильные и нестабильные. Если все аргументы функции стабильные и не изменились, то рекомпозиция пропускается, иначе придётся перезапускать эту функцию снова.Функция, поддерживающая пропуски рекомпозиции, называется пропускаемой (skippable). Мы должны стараться, чтобы почти все наши функции были пропускаемыми. Это очень хорошо повлияет на оптимизацию.Стабильные типыКлассификация типов по стабильностиКомпилятор Compose проходит по всем типам и добавляет в них информацию об их стабильности: аннотацию @StabilityInferred и статическое поле $stable с информацией о стабильности типа.Стабильность типа означает, что рантайм Compose может безопасно читать и сравнивать входные данные такого типа, чтобы при необходимости пропустить рекомпозицию. Конечная цель стабильности — помочь рантайму Compose.Стабильными типами считаются:Все примитивные типы и String.Функциональные типы (лямбды) (поэтому понятие «нестабильные лямбды» не совсем корректно, но об этом ниже).Классы, у которых все поля стабильного типа и объявлены как val, в том числе и sealed-классы. Стабильность полей класса проверяется рекурсивно, пока не найдётся тип, о стабильности которого уже однозначно известно.Enum (даже если вы у него укажите поле var и будете его менять).Типы, помеченные @Immutable или @Stable.Все стабильные типы должны выполнять определённый контракт, который мы затронем далее.Compose считает НЕстабильными:Классы, у которых хотя бы одно поле нестабильного типа или объявлено как var.Все классы из внешних модулей и библиотек, в которых нет компилятора Compose (List, Set, Map и прочие коллекции, LocalDate, LocalTime, Flow...);У дженериков (MyClass<T>), проверка идёт по структуре самого дженерика, а уже потом по указанному типу. Если структура дженерика нестабильна (есть поля нестабильного типа или поля с var), то он сразу считается нестабильным. Если мы сразу указываем тип дженерика, то Compose уже на этапе компиляции определит его как стабильный или нестабильный:// Стабильный
class MyClassStable(
	val counter: Pair<Int, Int>
)

// Нестабильный
class MyClassUnstable(
	val counter: Pair<LocalDate, LocalDate>
)

// Нестабильный
class MyClassUnstable(
	val counter: Pair<*, *>
)Если мы делаем composable-дженерик-функцию и передаём дженерик ей в аргументах (@Composable fun <T> Item(arg: Pair<T, T>)), то поведение будет такое же, как и у типов с вычисляемой стабильностью, про которые расскажем дальше.Разработчики Compose также заранее определили внешние типы, которые будут считаться стабильными: Pair, Result, Comparator, ClosedRange, коллекции из библиотеки kotlinx.collections.immutable, dagger.Lazy и другие. Большинство из этих типов — дженерики, поэтому данный список лишь говорит о стабильности их структуры. То есть, если мы передадим этим дженерикам стабильный тип, то и они будут стабильными, а если нестабильный, то и они будут нестабильными. Можно сказать, что к этим типам просто не будет применяться принцип, что все классы из внешних модулей и библиотек, в которых нет компилятора Compose, нестабильны.Есть также типы с вычисляемой стабильностью — о которых Compose не может сказать при компиляции, что они однозначно стабильные или нестабильные. Их стабильность проверяется уже в рантайме, при получении конкретных объектов. К таким типам относятся:Типы, которые объявлены в других модулях с включённым компилятором Compose. Если мы в модуле 1 используем тип из модуля 2, в котором у нас не включён Compose, то этот тип компилятор Compose просто не сможет проверить на стабильность, поэтому сразу будет считать нестабильным. А если в модуле 2 включён Compose, то компилятор Compose предполагает, что он в модуле 2 проверит этот тип: проставит аннотацию @StabilityInferred и добавит статическое поле $stable. И уже потом, в рантайме, а не на этапе компиляции, он прочитает это поле.Интерфейсы (проверка идёт по типу-наследнику, объект которого будет передан в аргументы).Про стабильность интерфейсов:По поводу интерфейсов некоторые другие статьи и источники пишут, что они нестабильные, но по исходному коду Compose, выходным метрикам и по результатам тестов мы всё-таки пришли к выводу, что стабильность интерфейса вычисляется в рантайме. В тех же тестах они помечаются как Uncertain (что соотносится с классом Unknown). В то же время типы, в которых Compose уверен, соотносятся с классом Certain (Stable или Unstable). Совокупность всех этих аргументов и подтолкнула меня к такому выводу.Дополнительно узнать стабильность типов можно в тестах или в composable-метриках, про которые рассказано в главе про отладку.@Immutable и @StableЕсли вы уверены, что класс или интерфейс и все его потомки стабильные, то можете пометить их аннотацией @Immutable, если они неизменны, или @Stable, если они могут меняться, но сами оповещают Compose о своём изменении. Например, @Stable подойдёт, если в классе есть поле типа State<T> или MutableState<T> (mutableStateOf() создаёт такой объект). @Immutable
data class MyUiState1(val items: List<String>)

@Stable
data class MyUiState2(val timer: MutableState<Int>)Стабильность от таких аннотаций наследуется дочерними типами.@Immutable
interface Parent // Стабильный тип

class Child1(val age: Int) : Parent // Стабильный тип

class Child2(var list: List<String>) : Parent // Тоже стабильный типАннотации @Immutable и @Stable полезны для того, чтобы их повесить на типы, которые Compose считает нестабильными, но по факту они стабильные, либо вы уверены, что будут использоваться как стабильные. Обе аннотации на данный момент просто выполняют логику объявления стабильного типа и не отличаются друг от друга для Compose, но желательно всё-таки использовать их по предназначению, так как в будущем разработчики Compose могут изменить поведение.Помечая этими аннотациями, вы обещаете Compose, что ваш тип будет выполнять следующий контракт:equals всегда будет возвращать одинаковое значение для одной и той же пары объектов.Когда публичные поля типа изменяются, нужно оповестить об этом Compose.Все публичные поля стабильны.Этот контракт — лишь ваше обещание, и Compose никак не проверит, если вы его нарушите. Но тогда возможно неожиданное поведение composable-функций. Давайте разберём контракт подробнее.Первый пункт особенно важен и может выстрелить в вас, даже если сделаете все аргументы стабильными. Например, в коде ниже мы видим, что у MyUiState нет переопределённого equals, как у data-класса, а это значит, что проверка будет происходить по ссылке. Если в MyComposable1 произойдёт рекомпозиция, то MyUiState будет пересоздан. При проверке по ссылке Compose будет считать его совершенно другим объектом и не пропустит MyComposable2, хотя поле name осталось тем же самым. class MyUiState(val name: String)

@Composable
fun MyComposable1() {
	val myState = MyUiState(""Name"")
	MyComposable(myState)
}

@Composable
fun MyComposable2(uiState: MyUiState) {
	Text(uiState)
}Такая ситуация решается либо написанием своей реализации equals (или использованием data class), либо запоминанием этого объекта с помощью remember, чтобы при рекомпозиции он не пересоздавался (или аналогичными действиями в бизнес-логике, если объект пересоздаётся там).Второй пункт реализован в State<T> и MutableState<T> (mutableStateOf), который под капотом оповещает Compose при изменении.Третий пункт контракта подразумевает, что вы используете публичные поля как стабильные. То есть, если у вас поле формально нестабильного типа List<T> и вы не кастите его где-нибудь к MutableList<T>, то смело помечайте ваш класс как @Immutable или @Stable.@Stable можно повесить и на обычные (non-composable) функции и свойства. Тогда Compose будет считать, что они вернут то же значение, если аргументы не изменились. На composable-функции аннотация не влияет. В основном нужна для оптимизации генерируемого кода для аргументов по умолчанию в composable-функциях. Пример функций и свойств, помеченных @Stable: Modifier.padding(), Modifier.width(), Int.dp; функции перехода в анимациях: fadeIn(), fadeOut(), slideIn(). Про влиянии @Stable на функции и свойстваВ composable-метриках можно увидеть влияние аннотации по тому, как компилятор Compose помечает аргументы по умолчанию: @dymanic или @static. Если кратко, то при @static в аргументах по умолчанию не читаются composable-функции или состояния, которые могут вызывать рекомпозицию (об этом ниже). Подробнее о @dymanic и @staticможно почитать по ссылке. Во время экспериментов мы не смогли получить в функции конкретный эффект (например, пропуск) от применения @Stable, только в метриках поменялось с @dymanic на @static.@Composable
fun MyWidget(param1: String, param2: String = testStable()) {
	Text(param1 + param2)
}

@Stable
fun testStable() = ""test""

// Composable-метрика для функции
restartable skippable scheme(""[androidx.compose.ui.UiComposable]"") fun MyWidget(
  stable param1: String,
  stable modifier: String? = @static testStable()
)Дополнительно про эти аннотации можно посмотреть в видео от red_mad_robot.Пропускаемость функцийCompose делает composable-функцию пропускаемой, только если все её аргументы стабильного типа и функция возвращает Unit. При этом игнорируются нестабильные аргументы, если они не используются в теле.Для пропускаемых функций Compose специально генерирует код, который позволяет не вызывать их снова, если входные данные не поменялись.@Composable
fun Header(text: String, $composer: Composer<*>, $changed: Int) {
	if (/* Логика проверки на необходимость пропуска */) {
		Text(text) // Выполняется тело функции
	} else {
		$composer.skipToGroupEnd() // Сообщаем Compose, что мы пропустили функцию
	}
}Нестабильные типы встречаются и среди часто используемых в Compose, например, тот же Painter, так что стоит осторожнее использовать его, чтобы не потерять пропускаемость у функции.Если есть аргументы с вычисляемой в рантайме стабильностью, то функция остаётся пропускаемой, но дополнительно генерируется код, который не пропускает её, если в рантайме аргумент оказывается нестабильного типа.Исходя из вышеперечисленного, мы в команде договорились помечать все UI-модели и состояния как @Immutable или @Stable, так как изначально их проектируем таковыми. Особенно следим за стабильностью при разработке UI kit-проекта, так как цена ошибки становится выше. Чтобы проверить стабильность типов, вы можете использовать метрики Compose (к ним вернёмся в конце статьи).Также можно просто передавать в функции как можно меньше лишних данных. Тут всё просто: меньше данных — меньше вероятность, что они поменяются.Что делать, если нужно использовать стандартные коллекции или внешние классы и хочется пропускаемости функций? Пока пространство возможного сильно ограничено: либо делать класс-обёртку (value class тоже можно использовать как обёртку) и вешать на него аннотации Immutable или Stable, либо попросту избегать. Для стандартных коллекций есть вариант перехода в UI-моделях на коллекции из kotlinx.collections.immutable. Возможность объявлять стабильность внешних типов у разработчиков Compose в планах.ЛямбдыДавайте поговорим о том, как работают лямбды в Compose и как правильно их готовить. В этой статье приводится интересный пример с вызовом метода ViewModel внутри лямбды, что приводит к лишним рекомпозициям.Кратко эту ситуацию можно представить так:@Composable
fun MyScreen() {
	val viewModel = remember { MyViewModel() }
	val state by viewModel.state.collectAsState()
	
	MyComposableItem(
		name = state.name,
		onButtonClick = { viewModel.onAction() }
	)
}Чтобы понять, что и почему, давайте разберём, как же Compose обрабатывает лямбды. Он делит их на non-composable, в которых не выполняется composable-код, и composable соответственно. Рассмотрим подробно первый тип.Non-composable лямбды, которые создаются в composable-функции, при компиляции оборачиваются в remember. Все захваченные переменные кладутся в качестве ключа для remember:// До компиляции
val number: Int = 6
val lambda = { Log.d(TAG, ""number = $number"" }

// После компиляции
val number: Int = 6
val lambda = remember(number) { { Log.d(TAG, ""number = $number"" } }Если лямбда захватывает в себя переменную, тип которой НЕ стабильный (то есть нестабильный или вычисляемый в рантайме: условие строже, чем для пропускаемости) или переменная объявлена как var, то Compose не оборачивает её в remember, из-за чего при рекомпозициях она пересоздаётся. Дальше при сравнении прошлой и текущей лямбды Compose обнаружит, что они не равны, и из-за этого начнёт рекомпозицию даже пропускаемой функции (подразумевается, что MyViewModel — нестабильным тип).Как решить эту проблему? Раньше работало использование ссылки на метод (viewModel::onAction), но начиная с Compose 1.4 перестало работать из-за использования сравнения по ссылке вместо кастомного equals, который генерирует Kotlin. Подробнее можно почитать в этом треде, а также в этом видео с 32:50. Скрины из тредаРаботают способы:Запоминать лямбду самим (при этом ключ должен и сам не меняться при каждой рекомпозиции):val onAction = remember { { viewModel.onAction() } }Можно сделать так для краткости (почему запоминать лямбду, а не ссылку на метод, можно почитать здесь):@Composable
inline fun <T : Any> MviViewModel.rememberOnAction(): ((T) -> Unit) {
	return remember { { this.onAction(it) } }
}

val onAction = viewModel.remberOnAction()Использовать верхнеуровневые (статические) функции и переменные. Здесь компилятор Kotlin будет напрямую их вызывать, так как они статические, а не передавать через конструктор класса, который будет создан для лямбды на этапе компиляции.Использовать внутри лямбды только стабильные внешние переменные.Использовать аргументы, полученные внутри лямбды. Это может не помочь, а только отложить или уменьшить проблему, но точно поможет, если вы раньше захватывали список, а теперь не будете захватывать ничего.@Composable
fun MyComposableItem(items: List<MyClass>) {
	// Вместо такого
	ItemWidget { items[5].doSomething() } 
	
	// Делать так
	ItemWidget(item[5]) { item -> item.doSomething() }
}Лямбда также может неявно захватить внешнюю переменную, если вы в composable-функции внутри фрагмента в лямбде вызовите функцию из этого фрагмента. Тогда конструктор лямбды будет принимать фрагмент как аргумент, и remember вокруг лямбды не будет генерироваться.class MyFragment : Fragment {
	fun onButtonClick() { ... }

	@Composable
	fun Screen() {
		MyButton(onClick = { onButtonClick() })
	}
}У вас также мог появиться вопрос, как с лямбдой справляется сам remember { }, если он принимает лямбду? Дело в том, что remember — это inline-функция, и её лямбда превращается в обычный блок кода. Так, функция:val resultValue = remember(key1, key2) {
	// Наши вычисления (например, создание лямбды)
}превратится в следующий код:// Получение запомненного значения
val rememberedValue = composer.rememberedValue()

val needUpdate = /* Проверка на то, изменились ли наши ключи key1 и key2,
	или значение ещё не инициализировано */

if (needUpdate) {
	// Наши вычисления. Inline-лямбда превратится в блок кода
	val value = calculation() 

	// Обновление запомненного значения
	composer.updateRememberedValue(value) 

	return value // Возвращает вычисленное и запомненное значение
} else {
	return rememberedValue // Возвращает запомненное значение
}Код выше лишь отражает логику и имеет упущения.Дополнительно про лямбды под капотом в контексте Compose можно посмотреть в этом видео с 25 минуты.Оптимизация пропусковПерезапускаемые функцииДля начала давайте разберёмся, что представляют собой перезапускаемые composable-функции. Как говорилось выше, в начале и в конце функции $composer начинает и заканчивает группу — условно узел дерева. Для перезапускаемых (restartable) функций вызывается перезапускаемая группа:@Composable
fun MyComposable($composer: Composer) {
	$composer.startRestartGroup() // Начало группы

	// Тело функции

	$composer.endRestartGroup() // Конец группы
		?.updateScope { $composer ->
			MyComposable($composer)
		}
}В конце кода можно увидеть механизм перезапуска функции при изменении: если между началом и концом группы было прочитано состояние, которое умеет оповещать Compose о своём изменении (State<T> или CompositionLocal), то $composer.endRestartGroup() вернёт не null и Compose научится перезапускать нашу функцию. Если есть более близкая к месту чтения состояния перезапускаемая группа, то перезапускаться будет именно она, а не внешняя.Давайте рассмотрим этот код:@Composable
fun MyComposable1() {
	val counter: MutableState<Int> = remember { mutableStateOf(0) }
	MyComposable2(counter)
}

@Composable
fun MyComposable2(counter: State<Int>) {
	Text(text = ""My counter = ${counter.value}"")
}В нём при изменении counter будет перезапускаться только MyComposable2, так как именно в её области видимости читается значение. Тот же MutableState можно представить себе как MutableStateFlow, который под капотом при чтении и записи выполняет необходимую логику подписки и оповещения. Это очень важная логика работы Compose, так как перезапустится именно MyComposable2, не трогая остальные родительские функции. Именно на этом основан механизм рекомпозиции. Вместе с механизмом пропусков это даёт широкие возможности для оптимизации, особенно для часто меняющихся частей UI.Для закрепления главы вот ещё примеры, из-за которых MyComposable2 будет точкой перезапуска (рекомпозиции) и пойдёт по всем её детям, а MyComposable1 не будет затронута. Можно добавить, что функции animateColorAsState(), rememberScrollState() и пр. тоже внутри содержат State<T>, и могут стать причиной рекомпозиции при изменении.val LocalContentAlpha = compositionLocalOf { 1f }

@Composable
fun MyComposable1() {
	val counter1: MutableState<Int> = remember { mutableStateOf(0) }
	var counter2: Int by remember { mutableStateOf(0) }
	MyComposable2(counter1, { counter2 })
}

@Composable
fun MyComposable2(counter1: State<Int>, counterProvider2: () -> Int) {
	Text(""Counter = ${counter1.value}"") // Чтение состояния
	Text(""Counter = ${counterProvider2()}"") // Чтение состояния
	Text(""Counter = ${LocalContentAlpha.current}"") // Чтение состояния
}Обратите внимание, что если вы используете State<T> как делегат, то в этом случае будьте осторожнее со случайным чтением состояния, особенно если оно часто меняющееся.@Composable
fun MyComposable1()
	var counter: Int by remember { mutableStateOf(0) }

	// Чтение состояния произойдёт в MyComposable1, а не в MyComposable2!!!
	MyComposable2(counter) 
}Разработчики Compose советуют прокидывать не State<T>, а лямбду, так как могут возникнуть трудности и лишний код, если нужно будет что-нибудь захардкодить или при тестировании. Но, в целом, кардинальных отличий нет. Зачем всё это нужно — расскажем в главе про отложенное чтение состояний.Также стоит добавить, что composable-лямбды, часто используемые в Slot API, тоже перезапускаемы и пропускаемы.Перезапускаемость и пропускаемостьЧтобы вы не запутались в этих двух терминах, резюмирую здесь:Перезапускаемая (restartable) функция может перезапускаться, быть областью перезапуска.Пропускаемую (skippable) функцию можно пропустить, если её аргументы не изменились.Вот как Compose обозначает в своих метриках функции, которые и перезапускаемы, и пропускаемы:restartable skippable scheme(""[androidx.compose.ui.UiComposable]"") fun MyWidget(
  stable widget: WidgetUiModel,
  stable modifier: Modifier? = @static Companion
)Если хотя бы один аргумент нестабильный, то функция останется только перезапускаемой (restartable). Если функция только перезапускаемая (restartable), то стоит либо сделать её и пропускаемой (skippable), либо избавиться от перезапускаемости. Аннотация @NonRestartableComposable как раз убирает перезапускаемость и (если была) пропускаемость.Когда перезапускаемость и пропускаемость не нужныВсе inline composable-функции неперезапускаемы (Box, Column и Row). Это значит, что чтение State<T> внутри одной из них при изменении вызовет рекомпозицию в ближайшей внешней перезапускаемой функции.@Composable
fun MyComposable() {
	val counter: MutableState<Int> = remember { mutableStateOf(0) }
	Box {
		// Рекомпозиция затронет всю MyComposable(), так как код заинлайнится
		Text(text = ""My counter = ${counter.value}"")
	}
}Не пропускаемы и функции, которые возвращают не Unit.Есть ситуации, когда пропускаемость и перезапускаемость не даёт реальных преимуществ, а только приводит к избыточной трате ресурсов:данные composable-функции меняются редко или никогда;composable-функция просто вызывает другие пропускаемые composable-функции:функция без сложной логики и без State<T>, вызывающая минимум других composable-функций;обёртка вокруг другой функции — служит неким маппером параметров или же для сокрытия ненужных параметров.В таком случае можно пометить composable-функцию аннотацией @NonRestartableComposable, что уберёт перезапускаемость (а вместе с ней и пропускаемость). @Composable
@NonRestartableComposable
fun ColumnScope.SpacerHeight(height: Dp) {
    Spacer(modifier = Modifier.height(height))
}Если функция содержит в себе ветвление логики (if, when), то ориентируйтесь по вышеописанным правилам уже по отношению к её ветвям. Добавлять аннотацию или нет зависит от того, насколько часто ветви будут меняться во время использования и насколько сложный код в каждой из веток.Как прнимер, @NonRestartableComposable помечены Spacer (нет логики и просто вызов Layout), некоторые перегрузки Image и Icon (маппинг параметров к своей перегрузке), Card (маппинг параметров к Surface). Выгода от отказа от перезапускаемости функции минимальна: не генерируется лишний код и не исполняется лишняя логика, но если вы проектируете UI kit, то стоит задуматься об этом, так как ваши элементы будут использоваться во многих местах, часто повторяться, и в сумме это даст эффект.Оптимизация часто меняющихся элементовОптимизировать чтение State<T> нужно только там, где состояние часто меняется и затрагивает много контента. Иначе весь код будет переоптимизирован и станет непригоден для чтения и развития.Derived statederivedStateOf — производное (вычисляемое) состояние, что и отражает основной сценарий использования.val listState = rememberLazyListState() 
val showButton by remember { 
	derivedStateOf { listState.firstVisibleItemIndex > 0 } 
}
Допустим, у нас есть состояние списка, в котором мы читаем индекс первого видимого элемента списка. Но сам по себе он нам не нужен, мы хотим знать, показывать нам кнопку или нет. Чтобы рекомпозировать только при изменении видимости кнопки, а не каждый раз при изменении первого видимого элемента списка, мы можем читать состояние внутри лямбды derivedStateOf. Там Derived state подписывается на изменения состояний, которые были прочитаны за первый проход, и возвращает итоговый State<T>, который уже вызывает рекомпозицию только при изменении итогового состояния.Важно, что Derived state реагирует на изменение только State<T>, а не обычных переменных, так как State<T> имеет функциональность подписки и оповещения при изменении снапшот системы Compose, с которой и работает Derived state.Подчеркнём что нельзя в качестве ключа remember использовать значение часто меняющегося состояния, иначе теряется весь смысл Derived state и рекомпозиция в этом месте будет происходить часто, как и пересоздание Derived state:// Не делать так
val listState = rememberMyListState() 
val showButton by remember(listState.value) { // Чтение listState
	derivedStateOf { listState.value > 0 } 
}

// Не делать так
val listState by rememberMyListState() 
val showButton by remember(listState) { // Чтение listState
	derivedStateOf { listState > 0 } 
}Derived state следует использовать только тогда, когда производное состояние будет меняться реже, чем исходные:// Не делать так
val derivedScrollOffset by remember { 
	derivedStateOf { scrollOffset - 10f } 
}Derived state полезен для производных состояний от состояний прокрутки ленивого списка, свайпа и других часто меняющихся. Ещё несколько примеров:Слежение за тем, переходит ли прокрутка порог (scrollPosition > 0).Количество элементов в списке больше порога (items > 0).Валидация формы (username.isValid()).Для вложенных Derived state периодически необходимо указывать политику мутации, чтобы не пересчитывать выражение при изменении первого (вложенного) derivedStateOf.val showScrollToTop by remember {
	// Политика мутации — structuralEqualityPolicy()
	derivedStateOf(structuralEqualityPolicy()) { scroll0ffset > 0f }
}

var buttonHeight by remember {
	derivedStateOf { 
		// Благодаря указанию политики мутации в showScrollToTop 
		// этот блок вычисления будет вызываться только при изменении showScrollToTop 
		if (showScrollToTop) 100f else 0f 
	}
}Подробнее про политики мутаций читайте в этой статье.Отложенное чтение состояний в composable-функцияхВ предыдущем пункте описан принцип, который использует Derived state: он читает состояние внутри себя и не даёт ему рекомпозировать всю функцию. Мы можем использовать этот же принцип, но уже для того, чтобы отложить чтение состояния из родительской функции в дочернюю. Такое следует делать также только для часто меняющихся состояний. Откладывать чтение можно с помощью лямбды или передачи State и чтения его в нужном месте.@Composable
fun MyComposable1() {
	val scrollState = rememberScrollState()
	val counter = remember { mutableStateOf(0) }

	MyList(scrollState)
	MyComposable2(counter1, { scrollState.value })
}

@Composable
fun MyComposable2(counter: State<Int>, scrollProvider: () -> Int) {
	// Чтение состояния в MyComposable2
	Text(text = ""My counter = ${counter.value}"")
	Text(text = ""My scroll = ${scrollProvider()}"")
}В коде выше из-за быстрого счётчика или прокрутки рекомпозироваться будет только функция MyComposable2, а не вся MyComposable1.Отложенное чтение состояний в фазах ComposeОткладывать чтение состояния можно не только между composable-функциями, но и между фазами Compose (Composition → Layout → Drawing). Например, если у нас часто меняется цвет, то лучше вместо модификатора background() использовать drawBehind { }, который принимает лямбду и будет вызывать код при смене состояния только на стадии отрисовки, а не композиции, как background().Подобное можно использовать при прокрутке: модификатор offset { } с лямбдой вместо простого offset(value). Так мы откладываем чтение состояния в фазу Layout.@Composable 
fun Example() {
	var state by remember { mutableStateOf(0) }

	Text(
		// Чтение состояния при композиции (Composition)
		""My state = $state"",
		Modifier
			.layout { measurable, constraints ->
				// Чтение состояния при компоновке (Layout)
				val size = IntSize(state, state)
			}
			.drawWithCache {
				// Чтение состояния при рисовании (Drawing)
				val color = state
			}
	)
}Дополнительно про оптимизацию пропусков в контексте анимаций можно посмотреть в видео от red_mad_robot.Уменьшение области рекомпозицииНужно разбивать на небольшие функции там, где можно уберечь части кода от рекомпозиции. Если вы видите, что часть функции остаётся неизменной, а часть меняется довольно часто, то, вероятно, лучше разбить эту функцию на две. Таким образом, одна функция будет пропускаться, а часто меняющаяся будет рекомпозировать меньшую область. Но не нужно увлекаться и выделять Divider в отдельную функцию.Вот пример выноса логики таймера в отдельную функцию с уменьшением количества рекомпозиций в Promo, так как перезапускаться будет только Timer (обратите внимание на место вызова timer.value, который и вызывает перезапуск при изменении):@Composable
fun Promo(timer: State<Int>) {
	Text(""Sample text"")
	Image()

	// Старый код таймера прямо в функции Promo
	// Text(""Осталось ${timer.value} секунд"")

	// Новый код таймера
	Timer(timer)
}

@Composable
fun Timer(timer: State<Int>) {
	// Код таймера и чтение состояния timer (timer.value) внутри
}Использование key и contentType в спискахВ ленивых списках необходимо передавать ключ в item() для того, чтобы списки знали, как меняются данные. Также передавать contentType, чтобы списки знали, какие элементы можно переиспользовать.LazyColumn {
	items(
		items = messages,
		key = { message -> message.id },
		contentType = { it.type }
	) { message ->
		 MessageRow(message)
	}
}Если вы делаете список через forEach, то можно использовать key() { }, тогда при изменении списка Compose будет понимать, куда переместились элементы.Column {
	widgets.forEach { widget ->
		key(widget.id) {
			MyWidget(widget)
		}
	}
}МодификаторыКастомные модификаторыЕсли вы пишете свой модификатор, то:Если он без состояния, используйте просто функции.Если он с состоянием, то используйте Modifier.Node (ModifierNodeElement). Раньше для такого рекомендовалось использовать composed. В целом, это можно делать и сейчас, так как подробного руководства по Modifier.Node пока что нет.Если в модификаторе вызывается composable-функция, то используйте Modifier.composed.Подробнее про модификаторы смотрите в этом видео.Переиспользование модификаторовЕсли в области, где создаются модификаторы, происходит частая рекомпозция, то стоит задуматься о том, чтобы вынести их создание за эту область. Это, например, актуально для анимаций:val reusableModifier = Modifier
			.padding(12.dp)
			.background(Color.Gray),

@Composable 
fun LoadingWheelAnimation() { 
	val animatedState = animateFloatAsState(...) 

	LoadingWheel( 
		modifier = reusableModifier, 
		// Чтение часто меняющегося состояния
		animatedState = animatedState.value
	) 
} Также рекомендуется выносить модификаторы в списках, чтобы все их элементы переиспользовали единый объект.val reusableItemModifier = Modifier
	.padding(bottom = 12.dp)
	.size(216.dp)
	.clip(CircleShape) 

@Composable 
private fun AuthorList(authors: List) { 
	LazyColumn { 
		items(authors) { 
			AsyncImage(modifier = reusableItemModifier) 
		} 
	} 
}Выносить модификаторы можно не только из функций, но и просто в родительскую composable-функцию, в которой реже происходит рекомпозиция. Дальше модификаторы можно спокойно дополнять.reusableModifier.clickable { /*...*/ }
otherModifier.then(reusableModifier)Долгие вычисления при рекомпозицииДолгие вычисления только во ViewModel или в rememberПочти все вычисления должны проводиться только во ViewModel. При этом следите, чтобы колбэки (onButtonClick, onIntent, onAction, onEvent, onMessage…) не выполняли тяжёлую работу в главном потоке. Если у вас в нём выполняется единая функция для обработки действий пользователя, то можно повесить на неё измерение времени работы и писать в лог о критических значениях длительности исполнения, чтобы разработчик не забывал выносить сложные и долгие вычисления в фоновые потоки.Лучше выносить всю логику из composable-функций. В остальных случаях, когда долгие или затратные вычисления неудобно вынести во ViewModel, применяйте remember.Без долгих вычислений в геттерах UI Statedata class MyUiState(
	val list1: List<Int> = emptyList(),
	val list2: List<Int> = emptyList(),
) {
	// Не делать так
	val isTextVisible 
		get() = list1.any { it == 1 } || list2.any { it != 0 }
}Если у вас в UiState есть что-то подобное, чтобы не задавать поле каждый раз, то пересчёт будет происходить при каждой рекомпозиции, так как это просто выполнение метода getIsTextVisible(). Поэтому либо уберите геттер (оставив поле в теле класса или перенеся в первичный конструктор), либо убедитесь, что в месте вызова геттера происходит минимум рекомпозиций.Когда использовать rememberИспользовать:Для любых долгих или затратных по памяти операций, которые могут быть выполнены более одного раза, но не должны выполняться до изменения ключей (если они нужны), переданных в remember(), особенно при частой рекомпозиции.val brush = remember(key1 = avatarRes) {
	ShaderBrush(
		BitmapShader(
			ImageBitmap.imageResource(res, avatarRes).asAndroidBitmap(),
			Shader.TileMode.REPEAT,
			Shader.TileMode.REPEAT
		)
	)
}Для обёртки лямбд с нестабильными внешними переменными и для классов без переопределённого equals.Если ключи remember обновляются очень часто, то стоит задуматься, а нужен ли в этом случае remember. Также в ключи не нужно добавлять вообще всё, от чего зависит вычисление внутри remember: если вы понимаете, что эти значения никогда не поменяются за время жизни composable-функции и самого remember, то не добавляйте их в ключи. LayoutКастомные LayoutНе бойтесь делать кастомные лейауты: они намного проще, чем во View, главное начать. Полезные видео и статьи по этой теме в Compose: тык1, тык2, тык3, тык4.Необоснованное изменение размера и расположенияИзбегайте необоснованного изменения размера Compose-элементов, особенно в списках. Такая проблема может возникнуть, если не установить фиксированный размер картинки и после загрузки из интернета она поменяет свой размер. Необоснованное изменение размера или расположения может произойти из-за модификаторов onGloballyPositioned(), onSizeChanged() и подобных. Из-за этого может происходить множество лишних рекомпозиций. Если элементам нужно знать о расположении и размерах других элементов, то чаще всего это означает, что вы либо используете не тот лейаут, либо вам нужно сделать кастомный.Предварительное вычисление LayoutSubcomposeLayoutSubcomposeLayout откладывает композицию до измерения в фазе Layout, чтобы мы могли использовать доступное пространство для композиции дочерних элементов. Второе полезное применение — это условная композиция. Например, в зависимости от размера окна приложения мы можем по-разному располагать элементы (для планшета или для телефона, или вообще для окна, размер которого можно менять). Или же в зависимости от состояния прокрутки вызывать композицию конкретных элементов для реализации ленивого списка. SubcomposeLayout довольно дорогой, поэтому не стоит использовать его для предварительного вычисления лейаута в любом другом случае.Основные реализации SubcomposeLayout — BoxWithConstraint, LazyRow и LazyColumn — покрывают большую часть потребностей, которую не может покрыть Layout. Также использование под капотом SubcomposeLayout объясняет, почему LazyRow и LazyColumn проигрывают по производительности Row и Column при малом количестве элементов в списке. Так что, если у вас небольшой список, то используйте для него Row и Column.SubcomposeLayout иногда ошибочно используют для реализации Slot API:// Не нужно так делать!
@Composable
fun DontDoThis(
	slot1: @Composable () -> Unit,
	slot2: @Composable () -> Unit
) {
	SubcomposeLayout { constraints ->
		val slot1Measurables = subcompose(""slot1"", slot1)
		val slot2Measurable = subcompose(""slot2"", slot2)

		layout(width, height) {
			...
		}
	}
}Для Slot API есть более правильный вариант: через модификатор layoutId() и поиск среди measurables по полю layoutId или Layout с передачей списка composable и деконструкцией списка measureable по порядку.@Composable
fun DoThis(
	slot1: @Composable () -> Unit,
	slot2: @Composable () -> Unit
) {
	Layout(
		contents = listOf(slot1, slot2)
	) { (slot1Measurables, slot2Measurables), constraints ->
		...
		layout(width, height) {
			...
		}
}Intrinsic measurementsIntrinsic measurements более эффективны, чем SubcomposeLayout, и под капотом работают очень похоже на LookaheadLayout. Оба подхода вызывают лямбду измерений (которая передаётся в LayoutModifiers или MeasurePolicy) с различными ограничениями (constraints) в одном и том же кадре. Но в случае с Intrinsics это предварительный расчёт для того, чтобы выполнить реальное измерение, используя полученные значения.Представьте себе строку (Row) с тремя дочерними элементами. Для того чтобы её высота соответствовала высоте самого высокого ребёнка, строке нужно получить внутренние измерения (intrinsic measurements) всех своих детей и после этого измерить себя, используя максимальное значение. Подробнее читайте на Android Developers.Intrinsic measurements может негативно влиять при сложных лейаутах в ленивых списках, но незначительно.LookaheadLayoutИспользуется для точного предварительного расчёта размера и положения любого (прямого или косвенного) дочернего элемента для обеспечения анимации (например, перехода одного элемента в другой). LookaheadLayout выполняет более агрессивное кэширование, чем intrinsic, чтобы не заглядывать вперёд, если дерево не изменилось. Подробнее можно прочитать в статье от Jorge Castillo.Прочие советыНе менять состояние, которое только прочиталиЭта ошибка более характерна для новичков. Мы сразу после чтения вызываем изменение состояния, из-за этого сразу выполняется рекомпозиция.@Composable
fun BadComposable() {
    var count by remember { mutableStateOf(0) }

    // Вызывает рекомпозицию при клике
    Button(onClick = { count++ }) {
        Text(""Recompose"")
    }

    Text(""$count"")
    count++ 
    // Обратная запись: запись в состояние сразу после того, 
    // как оно было прочитано
}MovableContentOfmovableContentOf полезен, когда мы хотим переместить наши элементы в другое место, но при этом не вызвав заново рекомпозицию и не теряя запомненное состояние:val myItems = remember {  
	movableContentOf {  
		MyItem(1)  
		MyItem(2)
	}  
}

if (isHorizontal) {  
	Row {  
		myItems()
	}  
} else {  
	Column {  
		myItems()  
	}  
}Подробнее читайте в статье от Jorge Castillo.Интересный факт: конструкция key() похожа по логике на movableContentOf(), так как оба подхода используют movable-группу, которая позволяет перемещать Compose-код без рекомпозиции.staticCompositionLocalOf и compositionLocalOfstaticCompositionLocalOf обычно нужен, когда Composition Local используется огромным количеством composable-функций и значение вряд ли будет меняться. Примеры реализаций: LocalContext, LocalLifecycleOwner, LocalDensity, LocalFocusManager и др.compositionLocalOf влечёт дополнительные затраты при начальном построении дерева композиции, так как все composable-функции, которые читают текущее значение, должны быть отслежены. Если значение будет часто меняться, то compositionLocalOf, возможно, будет лучшим выбором. Примеры реализаций: LocalConfiguration, LocalAlpha и др.@ReadOnlyComposableЕсли composable-функция выполняет только операции чтения, то можно пометить её аннотацией @ReadOnlyComposable. В результате у неё не будет сгенерирована группа. Это даст небольшой прирост производительности. Основной сценарий использования - функция, которой аннотация @Composable нужна только для чтения CompositionLocal (например, чтение цвета из темы), а не для вызова других composable-функций. Подробнее на Android Developers.Использовать меньше ComposeViewТут всё просто: чем меньше ComposeView вы используете для моста с View, тем быстрее Compose будет работать. Также чем раньше при запуске приложения появляется Compose-код, тем лучше будет работать дальнейший код, так как Compose успеет «прогреться». Использовать последнюю версию ComposeРазработчики Compose почти каждую версию улучшают производительность, поэтому не забывайте обновляться.Baseline ProfilesBaseline Profiles хорошо описаны на Android Developers. В целом, Google уже делает подобную работу за нас с помощью Cloud Profiles. Baseline Profiles помогут, если мы хотим улучшить наши метрики на старых версиях Android (7 — 8.1) и в начале нового релиза на новых Android (9+).Отладка и мониторинг производительностиОптимизация — это хорошо, но лучше всегда проверять свои изменения, чтобы понимать, что вы реально решили проблему, а не добавили новую.Проверять производительность необходимо почти всегда в релизном режиме и с R8. Режим отладки имеет множество плюшек при разработке, что замедляет и искажает итоговый код приложения. Компилятор R8 также значительно оптимизирует код.Также про отладку Compose хорошо написано в этой статье и рассказано в видео от Android Developers.Проверка стабильности и пропускаемостиЧтобы проверить типы и composable-функции в проекте на стабильность, нужно запустить генерацию метрик. Как это сделать: Composable metrics и Interpreting Compose Compiler Metrics.Метрики будут лежать по пути: модуль/build/compose_metrics. Среди них важны два файла:-classes.txt для метрик типов;unstable class WidgetUiState {
  unstable val widgets: List<Widget>
  stable val showLoader: Boolean
  <runtime stability> = Unstable
}-composables.txt и .csv для метрик функций;restartable scheme(""[androidx.compose.ui.UiComposable]"") fun MyWidgets(
  unstable widgets: List<Widget>
  stable modifier: Modifier? = @static Companion
)-module.json для статистики по модулю.Также есть библиотека для отображения метрик в HTML: Compose Compiler Report to HTML.Отладка рекомпозиций и пропусковЧтобы не повторяться, советую посмотреть видео про отладку рекомпозиций и пропусков. Смысл в том, что вы включаете подсчёт рекомпозиций и пропусков. После этого на своём экране делаете обычные действия. Дальше смотрите, где есть лишние рекомпозиции, которых можно было бы избежать, или смотрите, где происходит рекомпозиция, хотя данные не менялись и мог бы происходить пропуск.Также для отладки рекомпозиций можно использовать Rebugger. Эта библиотека позволяет отслеживать изменения в заданных аргумента и выводить причины рекомпозиции.В Android Studio Hedgehog добавят дополнительную информацию в отладчики для просмотра состояния Compose.Трассировка композицииДля глубокого анализа проблем ваших UI-элементов используйте трассировку композиции. Она доступна начиная с Android Studio Flamingo. Как это делать, описано в статье “Composition tracing”, а также немного в этом видео.БенчмаркингПроверяйте свои экраны после оптимизаций с помощью бенчмарков (например, бенчмарк на прокрутку списка с измерением длительности отрисовки кадра).ИтогВ этой статье мы описали весь список оптимизаций, которые нам встретились, а также кое-что из практики нашей команды. Некоторые советы можно преобразовать в кодстайл вашей команды и свободно использовать. Другие полезны только в том случае, если ваш элемент часто меняется. Проверяйте производительность после каждой оптимизации, чтобы не снизить скорость работы. А также не обязательно заранее чрезмерно оптимизировать код, иначе он превратится в нечитаемое нечто.Бонус (чит-лист)  ",191,0
Фестиваль «Докер» ищет ответы на вопросы о будущем людей и технологий,https://habr.com/ru/companies/lanit/articles/743250/,2023-06-23T07:01:01.000Z,"Уже восемь лет ЛАНИТ поддерживает фестиваль авторского неигрового кино «Докер» и дружит с его командой. В этом году показы кинолент и деловая программа проекта стартуют уже в июне: кинотеатр «Октябрь» на Новом Арбате будет принимать «Докер» с 23 июня по 2 июля. В это время зрителей, как всегда, ждут глубокие размышления и яркие впечатления от необычности визуальных образов и кажущейся нереальности вполне реальных историй, которыми поделятся создатели 40 документальных картин из 20 стран мира. В их числе работы, которые предлагают задуматься о нарастающей роли информационных технологий в жизни людей. В статье расскажем об этом коротко.  	Источник«Мы догадывались, что это произойдет. Но не думали, что так скоро. Спустя несколько лет проведения программы «Let IT dok!», мы пришли к выводу, что информационные технологии настолько быстро и прочно вошли в нашу жизнь, что стали полностью в ней растворены, – признаются организаторы «Докер». – Растворены в окружающей действительности, в искусстве, в кино и во всех наших программах. В этом году мы не делаем отдельный акцент на IT и констатируем, что в этом пока больше нет необходимости. Цифровой разум – это новая реальность». Даже постер фестиваля в этом году создан искусственным интеллектом. А человек (в данном случае дизайнер Дмитрий Валов) только доработал изображение. Документалисты, как и прежде, следуют за реальностью и её вызовами. На этот раз они намерены говорить о том, что системы искусственного интеллекта могут представлять серьезные риски, а могут и позитивно влиять на всеобщий прогресс и улучшение жизни.«Но не замечать ИИ больше нельзя. Он прямо здесь, взирает на нас, впитывает информацию и готовится к взаимодействию», – с таким посылом устроители подходят сразу к нескольким событиям фестиваля. Первым из таких станет дискуссия «Искусственный интеллект vs. кино», которая будет проходить 24 июня. Специалисты в области ИИ, режиссеры монтажа и звукорежиссеры, разработчики ПО и журналисты, изучающие технологии, постараются разобраться в том, какое место в кинематографе займет диджитал-сознание и сколько творческого пространства останется людям. Ведь нейросети, почти как когда-то об этом фантазировали в сказках и футуристических книгах, уже очень многое делают за нас. Легко справляются с написанием сценариев, рассчитывают финансовую успешность будущего фильма, вместо режиссера создают раскадровки, графику, звуковые дорожки, анимируют персонажей, самостоятельно монтируют видео и даже занимаются маркетинговым продвижением готового кинопродукта в интернете. Один из авторов нашего блога, профессионал в сфере мультимедийных решений Леонид Жестев также станет участником обсуждения. В ходе дискуссии он выскажет свое мнение о том, как может преобразиться реальность при активном содействии нейросетей и можно ли будет в ближайшем будущем снять документальное кино внутри метавселенной. Участие в дискуссии бесплатное. Зарегистрироваться можно здесь. Кроме обсуждения популярной IT-повестки, внимание всех интересующихся технологиями в этом году на фестивале могут привлечь две любопытные короткометражки. В этих лентах авторы представляют технологии не как набор чипов или программного кода, а как способ разделить чувства и переживания главных героев. Дальше несколько слов об этих пронзительных человеческих историях, пойманных объективами талантливых документалистов.    Водяная мельница в Бади (Китай)Источник: пресс-служба «Докер»МИРОВАЯ ПРЕМЬЕРА25 июня, 15.00Режиссер: Хабу (китайский кинорежиссер, фотограф и сценарист)Фильм затрагивает тему внутреннего конфликта между вековыми обычаями и современностью, когда в традиционный уклад вплетается технический прогресс, необходимый для связи поколений. Съемки проходили в удаленных от цивилизации местах, в абсолютно аутентичном пространстве китайского Тибета. Старики, живущие в пустеющих деревнях, следуют традициям и занимаются ручным трудом, всеми способами стремясь передать свою любовь потомкам. Главная героиня и её 96-летняя мама очень скучают по своим детям и внукам, но при этом не хотят от них зависеть. Поэтому они принимают решение починить свою водяную мельницу, чтобы ручным трудом заработать на мобильный телефон, – и благодаря этому быть ближе к семье.Рассказанный в жанре документальной сказки, этот фильм является ярким представителем новой китайской документальной волны, которая успела выработать свой почерк и киноязык – красочный, музыкальный, эмоциональный и доносящий до зрителя глубокие смыслы.Цифровое бессмертие (Россия)Источник: пресс-служба «Докер»МИРОВАЯ ПРЕМЬЕРА1 июля ,14:00Режиссеры: Никита Аниськин, Владимир ГоловневСаша Осипович, внук советского изобретателя, живет в уральском Кунгуре. Под присмотром деда-инженера он с детства занимался радиотехникой. Умирая, дедушка попросил внука создать его цифровую копию в теле робота. Так Александр начинает большой̆ эксперимент – создание настоящего «терминатора».После показа пройдет обсуждение с авторами.Если вам хочется вместе с документалистами подумать над вопросами, которые диктует прогресс и современность, вот полная программа фестиваля «Докер». Приходите.  ",51,0
По наклонной: смещение полюса Земли и грунтовые воды,https://habr.com/ru/companies/ua-hosting/articles/743228/,2023-06-23T07:00:02.000Z,"

Решение любой проблемы или задачи требует четкого понимания ее причин. Этот принцип применяется в любой сфере научных изысканий, от математики и физики до психиатрии и биологии. Обладая неполными, неточными или же устаревшими данными, мы можем прийти к неверному выводу и предпринять действия, которые могут усугубить. Когда речь заходит о чем-то глобальном, как экология планеты, точность становится критически важной. Одним из ярчайших признаков глобального потепления является увеличение уровня мирового океана. Обычно этот процесс ассоциируется с таянием ледников, вызванным деятельностью человека. Но есть еще один фактор, о котором говорят не так часто — откачка подземных вод. По некоторым оценкам мы откачали из-под земли порядка 2150 гигатонн грунтовых вод, что привело к повышению уровня моря на 6.24 мм. На первый взгляд цифра кажется до смешного мала, но стоит помнить — речь идет не о каком-то мелком водоеме, а о мировом океане. Еще один любопытный эффект откачки грунтовых вод — это смещение полюса Земли. Ученые из Американского геофизического союза (Вашингтон, США) разработали модель смещения полюса вращения Земли по данным перераспределения воды, которая может позволить более точно оценивать степень истощения подземных вод. Как именно подземные воды влияют на полюс, что происходит при их истощении, и насколько точна разработанная модель? Ответы на эти вопросы мы найдем в докладе ученых.

Основа исследования
Повышение уровня моря является одним из наиболее значимых явлений, связанных с потеплением климата. В наши дни этот процесс тщательно отслеживается с помощью нескольких методов наблюдения. Например, в период с 2005 по 2015 спутниковая альтиметрия показала, что средний глобальный уровень моря (GMSL от global mean sea level) поднимался со скоростью 3.5 мм/год. Данные международной программы Argo показывают, что изменения плотности океана способствовали увеличению стерического GMSL примерно на 1.3 мм/год. Остальная часть (3.5 – 1.3 = 2.2 мм/год) вызвана увеличением массы океана, что подтверждается GRACE. GRACE также предоставляет оценки вклада отдельных океанских масс в повышение GMSL, связанных с ледниковым щитом Гренландии (GrIS от Greenland ice sheet), антарктическим ледниковым щитом (AIS от Antarctic ice sheet), горными ледниками и наземными водохранилищами (TWS от terrestrial water storage).

Вклад TWS в прошлом ошибочно воспринимался как вклад в уменьшение GMSL, поскольку изменения центра масс Земли (геоцентра) не учитывались должным образом при обработке данных GRACE. При надлежащем учете движения геоцентра изменение TWS считается значительным фактором повышения примерно на 0.3 мм/год. TWS в основном включает вклады от искусственных водохранилищ за плотинами, почвенной влаги и грунтовых вод. Увеличение числа плотин и соответствующее увеличение запасов воды смягчило рост GMSL с начала XX века. Следовательно, вклад TWS в повышение GMSL, вытекающий из данных GRACE, вероятно, будет связан с уменьшением влажности почвы и/или грунтовых вод.

До миссии GRACE ограниченные данные дистанционного и локального зондирования указывали на то, что увеличение запасов в искусственных резервуарах было источником снижения GMSL, в то время как таяние льда из AIS, GrIS и горных ледников способствовало повышению GMSL. Истощение подземных вод, смоделированное с помощью климатических моделей, было определено как значительный вклад в повышение GMSL, но отсутствие реальных данных не давало возможности подтвердить результаты этого моделирования.

Независимые ограничения на перераспределение водных масс обеспечиваются движением полюсов (PM от polar motion) — изменением положения полюса вращения Земли относительно земной коры. Подземные воды и другие источники поверхностного перераспределения массы (включая горные ледники, воду, хранящуюся в искусственных резервуарах, изменения массы AIS и GrIS и влажность почвы) влияют на PM.

PM управляется сферическими гармоническими изменениями степени 2 порядка 1 в гравитационном поле Земли, а также атмосферными ветрами и океанскими течениями, без участия стерических изменений уровня моря. Таким образом, как спутниковые гравиметрические данные GRACE, так и PM обеспечивают ограничения на масштабы и географическое распределение истощения подземных вод, при условии, что имеются хорошие оценки других факторов. Изменения в динамическом сжатии Земли (J2) также могут способствовать сдерживанию перераспределения массы воды, но обнаружено, что J2 относительно нечувствителен к истощению подземных вод.

В рассматриваемом нами сегодня труде ученые изучили изменения GMSL в период с 1993 по 2010 год с использованием данных и моделей для различных гидрологических источников. Ученые использовали соответствующие прогнозы и наблюдения за PM, чтобы понять оценки истощения подземных вод на основе климатической модели. Период исследования был выбран с учетом наличия как данных наблюдений, так и модельных оценок.

Подготовка к исследованию

Изображение №1

На графиках 1a и 1b показаны вклады AIS и GrIS в повышение GMSL соответственно. В течение периода исследования доступны две разные оценки. Красные линии основаны на методе потока массы, который включает радиолокационное дистанционное зондирование ледового стока и климатические модели баланса поверхностной массы. Синие линии взяты из данных программы IMBIE (от Ice sheet Mass Balance Inter-comparison Exercise), которая объединяет дистанционное зондирование и различные модели регионального климата на обоих ледяных щитах. Оценки потоков показывают больший вклад AIS и GrIS в рост GMSL по сравнению с IMBIE. Изменения массы горных ледников оценивались гляциологическими и геодезическими наблюдениями с начала XX века. Три недавние оценки различных научных групп показаны на 1c.

Вклад трех основных компонентов TWS в изменение GMSL показан на 1d–1f. К ним относятся колебания влажности почвы (включая снеговую воду, второстепенный компонент), запруженные воды за плотинами и грунтовые воды. Вклад почвенной влаги демонстрирует выраженные межгодовые колебания, но незначительную тенденцию во времени. Плотины являются важной причиной снижения GMSL. Загрязненная вода в искусственных водохранилищах была оценена на основе глобальной базы данных, включая местоположения, максимальную вместимость и сроки завершения строительства для 7320 плотин с совокупной мощностью около 7000 км3 с 1900 года. Сюда также включены эффекты просачивания. Предполагается, что первый год эффекта просачивания после затопления составляет 5% от начальной максимальной мощности, затем он медленно растет пропорционально квадратному корню времени.

Данные по истощению подземных вод было получено с использованием глобальной гидрологической модели PCR-GLOBWB. Колебания грунтовых вод оказывают относительно большое влияние на повышение GMSL.


Изображение №2

Используя описанные выше данные об изменении массы наземной воды и льда, колебания уровня моря были оценены на основе сохранения массы между сушей и океанами с учетом эффектов самопритяжения и нагрузки (SAL от self-attraction and loading). На 2a показано общее истощение подземных вод за период анализа с 1993 по 2010 год. В северо-западной части Индии и западной части Северной Америки наблюдается значительное уменьшение запасов подземных вод.

В большинстве мировых океанов наблюдается повышение примерно на 10 мм, но падение уровня моря над Индийским и Тихим океанами, примыкающими к районам истощения грунтовых вод (2b), является следствием SAL, вызывающего снижение уровня моря вблизи районов с уменьшенным запасом водной массы на суше.

Изменение массы наземной воды (σL) и океана (σO) были преобразованы в коэффициенты сферической гармоники (SH от spherical harmonic) геоида:

Формула №1

где apω — радиус Земли, умноженный на плотность воды; (θ, λ) — широта и долгота; Plm — нормированный ассоциированный полином Лежандра; kl — число Лава.

Используя комбинированные поля масс [σL(θ, λ) + σO(θ, λ)], возбуждения PM (x1, x2) были рассчитаны из коэффициентов SH степени 2 (l = 2) порядка 1 (m = 1) с помощью:

Формула №2

где M — масса Земли, а C и A — главные моменты инерции Земли.

x1 и x2 в основном определяются изменениями наземных запасов с незначительным вкладом соответствующего повышения уровня моря из-за относительно равномерного распределения массы океана, как показано на 2b.

Результаты исследования
Сначала ученые сравнили временные ряды возбуждения PM(x1, x2) по наблюдениям с оценками всех известных источников возбуждений.


Изображение №3

Красными линиями на графиках выше показаны два компонента x1 и x2 из наблюдений за период с 1993 по 2010 год. Международная служба вращения Земли (IERS от International Earth Rotation and Reference Systems Service) предоставляет различные серии PM. Для исследования использовалась самая длинная серия — EOP C01 IAU2000. x1 и x2 выражены в миллисекундах дуги (мсд) по гринвичскому меридиану и 90° восточной долготы соответственно.

Синие линии показывают оценку (x1, x2) из всех известных источников изменения GMSL, показанных на изображении №1, а также влияние барометрического давления, давления на дно океана, ветра и течений.

Поскольку рассматривались две оценки вклада полярных ледяных щитов и три оценки горных ледников, для каждой GIA (от Glacial Isostatic Adjustment) оценки PM возможны шесть различных оценок (x1, x2).

Оценка, показанная синими линиями на изображении №1, использует изменения массы AIS и GrIS из данных программы IMBIE и изменения массы горных ледников из самой свежей оценки (2019 год), а также влажность почвы, запруженные воды за плотинами и подземные воды. Также учитывались эффекты барометрического давления, давления на дне океана, ветра и течений.

Наиболее высокочастотные вариации (x1, x2) объясняются изменениями барометрического давления, давления на дне океана, ветром и течением. Изменения возбуждения PM, связанные с потерей массы льда горными ледниками и Гренландией, показывают направленные изменения возбуждения PM, аналогичные предыдущим исследованиям 1998 и 2005 соответственно.

Графики выше показывают высокую степень согласованности между оценочными и наблюдаемыми (x1, x2). x1 показывает положительную тенденцию (3a), а x2 — небольшую отрицательную тенденцию (3b), вероятно, из-за GIA и перераспределения поверхностной массы. Стоит отметить, что если бы грунтовые воды (показаны черными линиями) не учитывались, расчетная тенденция x2 не соответствовала бы наблюдаемой. Важность вклада подземных вод в рост GMSL с 1993 по 2010 год более четко видна на полярном графике (изображение №4), который показывает как величину, так и направление.


Изображение №4

Каждый вклад синих линий из графика №1 отображен на 4a. Незначительные вклады в тенденции (x1, x2) от влажности почвы, атмосферного давления, давления на дне океана, ветров и течений не были включены. Красная стрелка показывает GIA PM, движущиеся к западному побережью Гренландии со скоростью 6.74 см/год. Вышеуказанные графики показывают, что истощение грунтовых вод является вторым по величине фактором, влияющим на тенденцию возбуждения PM. График 4b — это результат сравнения наблюдаемых и расчетных тенденций возбуждения PM.

Предполагаемая тенденция возбуждения PM представляет собой векторную сумму всех стрелок на 4a, дополнительно включая эффекты влажности почвы, барометрического давления, давления на дне океана, ветров и течений. Эллипсы представляют собой неопределенности скорости возбуждения PM, оцененные как сумма квадратов формальных ошибок в оценках тенденции при доверительном уровне в 95%. Сплошные синие и пунктирные стрелки показывают оценки возбуждения PM с грунтовыми водами и без них соответственно. За исключением подземных вод, расчетные и наблюдаемые тенденции возбуждения PM плохо согласуются. Разница значительно уменьшается, если включить подземные воды.

Для более детального ознакомления с нюансами исследования рекомендую заглянуть в доклад ученых и дополнительные материалы к нему.

Эпилог
Оценки глобальной климатической модели показывают, что истощение грунтовых вод вносит значительный вклад в повышение GMSL (средний глобальный уровень моря). С момента запуска GRACE наблюдения за изменяющейся во времени гравитацией показали значительное истощение грунтовых вод и, как следствие, повышение уровня моря. До этого момента оценки GMSL также указывали на истощение грунтовых вод, но прямых наблюдений не было, а потому точных оценок ученые дать не могли.

По мнению ученых, независимым подтверждением вклада подземных вод в изменения GMSL могут служить изменения динамического сжатия Земли (J2) и движения полюсов (PM). Однако было обнаружено, что J2 не особенно полезен для этой цели из-за географии (низкие широты) водоносных горизонтов, которые были истощены в исследуемый период, и из-за неопределенности в прогнозах GIA относительно J2. А вот изменения PM оказались весьма чувствительны к изменениям грунтовых вод.

Было обнаружено, что истощение подземных вод было вторым по величине (4.36 см/год) компонентом тенденции возбуждения PM к 64,16° в. д. в период с 1993 по 2010 год. Ученые отмечают, что если не учитывать грунтовые воды в прогнозировании возбуждения PM, то полученная тенденция будет неверной, а точнее будет направлена западнее, чем есть на самом деле.

Пока сложно сказать, как данная информация может помочь в нормализации глобальной экологической ситуации. Однако сам факт этого исследования показывает, что для правильного прогнозирования какого-либо процесса или явления необходимы очень точные данные, учет всех возможных факторов влияния и подробный анализ. Попытки решить какую-либо задачу путем концентрации всех усилий на устранении или минимизации влияния лишь одного фактора не дадут желаемого результата, ведь другие факторы, которые ранее не учитывались, будут все также влиять на ситуацию.

Немного рекламы
Спасибо, что остаётесь с нами. Вам нравятся наши статьи? Хотите видеть больше интересных материалов? Поддержите нас, оформив заказ или порекомендовав знакомым, облачные VPS для разработчиков от $4.99, уникальный аналог entry-level серверов, который был придуман нами для Вас: Вся правда о VPS (KVM) E5-2697 v3 (6 Cores) 10GB DDR4 480GB SSD 1Gbps от $19 или как правильно делить сервер? (доступны варианты с RAID1 и RAID10, до 24 ядер и до 40GB DDR4).

Dell R730xd в 2 раза дешевле в дата-центре Maincubes Tier IV в Амстердаме? Только у нас 2 х Intel TetraDeca-Core Xeon 2x E5-2697v3 2.6GHz 14C 64GB DDR4 4x960GB SSD 1Gbps 100 ТВ от $199 в Нидерландах! Dell R420 — 2x E5-2430 2.2Ghz 6C 128GB DDR3 2x960GB SSD 1Gbps 100TB — от $99! Читайте о том Как построить инфраструктуру корп. класса c применением серверов Dell R730xd Е5-2650 v4 стоимостью 9000 евро за копейки?  ",159,0
"15 лет «Фланта»: переход от сервисной компании к продуктовой, покупка внешнего продукта, вера в Open Source",https://habr.com/ru/companies/flant/articles/743462/,2023-06-23T06:43:19.000Z,"В мае 2023 года «Фланту» исполнилось 15 лет. За это время из небольшого предприятия двух друзей-студентов, которые мечтали поставить GNU/Linux на каждый рабочий стол, мы выросли в команду опытных и уважаемых в индустрии DevOps-инженеров и активно трансформируемся в продуктовую компанию с собственной экосистемой продуктов.В этой статье мы хотели порефлексировать о переходе от сервисной компании к созданию собственных продуктам, рассказать, как эти продукты развивались и что с нами произошло за последние 5 лет.Сразу обозначим свой «символ веры». Услуга DevOps as a Service (DaaS) была, есть и будет основой нашей продуктовой линейки. Именно благодаря инженерам, которые 24/7 обслуживают сотни клиентских кластеров, мы можем получать максимально быстрый фидбэк по своим продуктам, тестировать новые релизы и собирать кучу новых идей для дорожной карты. То есть в нашем случае услуга, как бы странно для классических продуктовых компаний это ни звучало, является одним из ключевых компонентов продуктовой разработки.Первые 10 лет: как мы искали свою суперсилуПервые 10 лет проскочим довольно быстро, хотя именно они заложили основу того «Фланта», который существует сегодня. Мы уже писали статью к своему десятилетию — в ней все рассказано гораздо подробнее. Остановимся лишь на том, что особенно важно в контексте этого материала.В 2008 году два друга — студенты одной кафедры и тезки — основали «Флант». Всё было пропитано духом авантюризма: любовь к Open Source, романтика, мечты о Linux на каждой рабочей станции. Кстати, именно тогда мы написали свой первый продукт — панель управления хостингом. С самого начала мы то и дело писали небольшие утилиты для собственных нужд.Дальше был долгий период становления бизнеса — причем без поддержки инвесторов и внешней помощи. Мы постепенно определялись с направлением работы: сначала обслуживали «всё под Linux», потом ограничились шестью направлениями, затем — четырьмя. Где-то тут появился и наш первый международный сертификат — мы стали авторизованным партнером Canonical.В 2015 году мы признали, что основным фокусом все это время были веб-приложения, и «официально» отказались от остальных видов деятельности. А еще спустя небольшое время сузили услуги до обслуживания инфраструктуры на Kubernetes. Да, в Kubernetes и Docker мы поверили практически сразу после их появления, потому что до этого сами, как и некоторые другие игроки рынка, пытались сделать жизнь лучше похожими, но сделанными собственноручно решениями. В частности, у нас был внутренний проект по созданию инструментов для удобного управления изоляцией на базе Linux cgroups… Когда же мы сделали ставку на Kubernetes, это тоже потребовало определенных «обвязок» — ведь ни для кого не секрет, что технология молода, сложна и очень быстро меняется. Чтобы автоматизировать нетривиальные процессы установки и настройки инфраструктуры, реализации CI/CD, мы регулярно писали скрипты, операторы и какие-то мелкие утилитки для упрощения работы. В частности, было два интересных проекта: dapp (теперь уже werf) и shell-operator. Наш CEO Александр Баталов не нашел золота в песке. И мы продолжили искать его в бизнесе:) Корпоратив «Фланта», 2017 годwerf мы начали разрабатывать в 2016 году — он помогал выстроить процессы CI/CD для приложений наших заказчиков быстро и эффективно, а также предлагал унифицированный подход для их дальнейшей поддержки и совершенствования. А shell-operator позволял писать операторы для K8s на Shell/Bash или Python. Оба инструмента мы в итоге выложили в Open Source и они остаются полностью открытыми по сей день. Подробнее об этих проектах мы ещё расскажем ниже.В 2017 году мы сделали ставку на Okmeter как систему мониторинга инфраструктуры. До этого наши инженеры уже перепробовали самописные скрипты и веб-приложения, Cacti, Zabbix и другие инструменты, но все было не то. И мы поняли: надо либо делать какое-то полноценное внутреннее решение, либо искать что-то готовое. Делать свое с нуля было бы долго и сложно, а достойный инструмент был нужен «уже вчера»: как минимум, от качества мониторинга зависели штрафные санкции, зафиксированные в SLA, а как максимум — уровень сервиса и наша репутация.И хотя мы тогда этого не понимали, именно werf, shell-operator и Okmeter стали основой нашей будущей продуктовой экосистемы — причем все они появились в жизни «Фланта» примерно в одно время, в 2016-2017 годах. В последующих разделах статьи мы разберем, как эти продукты эволюционировали, как мы переосмыслили свой подход к их развитию и что с ними происходит сейчас. А в завершении расскажем, какие еще важные события произошли в жизни «Фланта» за последние пять лет.werf: первый большой Open Source-проект, CNCF и капитан ФлинтВ 2016 году мы начали работать над внутренним инструментом под названием dapp, который автоматизировал сборку Docker-контейнеров. Со временем он вырос в решение для управления всем жизненным циклом поставки программного обеспечения в Kubernetes. Это был не первый Open Source-проект «Фланта», но по масштабам он заметно превосходил все предыдущие. Dapp стал не просто какой-то небольшой утилиткой, он оказался связующим звеном для ряда других решений и процессов, превратившись в важный инструмент, который наши инженеры ежедневно использовали в работе. В нем мы реализовали свое видение того, «как делать правильно»: годами мы накапливали лучшие практики, оттачивали процессы CI/CD и наконец сумели автоматизировать их с помощью собственного программного решения.  Когда ты оказываешь услуги, самое лучшее, что можно сделать для повышения эффективности — это унифицировать процесс оказания услуг. Dapp как раз выступил в качестве единого каркаса для построения процессов CI/CD в разных проектах, имеющих разные особенности и требования.Со своей задачей утилита справилась: реализация CI/CD-пайплайнов была значительно упрощена, а инженеры получили что-то вроде тюбика клея, который позволял объединять и адаптировать под каждый проект стандартную связку популярных инструментов: Docker, Git, Helm, Container Registry, Kubernetes. Так у нас появился полноценный внутренний продукт. Кстати, именно таким путем изначально рождались все наши продукты: мы делали что-то под себя, а потом понимали, что ценность в этом есть и для внешнего сообщества.Из dapp в werfОднако с dapp возникла небольшая проблема: название со временем стало все больше ассоциироваться с разными блокчейнами. Поэтому 22 октября 2018 года мы начали придумывать для него другое название. Конечно, за основу взяли морскую тематику, моду на которую в DevOps-тусовке ввел Docker, а потом поддержал знаменитый «Кормчий» — и речь не про старину Мао😁 По итогам обсуждений мы остановились на названии werf (на русском произносим как «верфь»).Как мы выбирали название для werfПервым вариантом был flint. Тут и знаменитый пират, и перекличка с Flant, и даже Flant Integration. Но в топик тут же набежали хейтеры (такова демократия!), которым новое название не понравилось: Чтобы не зацикливаться на герое «Острова сокровищ», мы даже пошли изучать статью на Википедии про морские термины. Но основная часть терминов из статьи уже была занята в доменной зоне io. Мы почти отчаялись, и даже поступило предложение потроллить своим названием всех морячков из экосистемы K8s.Конечно, не обошлось и без Евгения Ваганыча и превращения айсберга в assberg… Попробовали даже рынду😁 И тут Тимофея осенило (он, кстати до сих пор работает над werf):werf — слово голландское и означает оно, собственно, верфь. Ту самую, судостроительную. На всякий случай пробежались по пиратским напиткам и оскорблениям вроде bastardo и даже был вариант kuklos — но вспомнились разные белые колпаки и он тут же отпал.Логотип werfВ 2019 году мы зарелизили документацию на русском языке, а 14 января 2020 вышла стабильная версия 1.0 — и werf стала взрослым проектом.Однако мы не только выложили werf в Open Source — со временем мы передали все права в Cloud Native Computing Foundation, некоммерческую организацию, которой принадлежат права и на Kubernetes. Официально передача прав состоялась 13 декабря 2022 года. При этом мы до сих пор управляем развитием проекта и являемся его ключевыми мейнтейнерами. werf остается полностью некоммерческим Open Source-проектом, который делает наше производство более эффективным и за годы своего существования сформировал вокруг себя устойчивое сообщество внешних пользователей.Каждый флантовец должен научиться собирать и разбирать образ контейнера, пока горит спичка. С закрытыми глазами, 2018 годРазвитие werf как продуктаПроцесс развития werf — классика для технических Open Source-инструментов, которые выросли из внутренних проектов: мы собираем хотелки со своих инженеров, мониторим вопросы в чате werf (eng-версия), выступаем с докладами на российских и международных конференциях, пишем статьи, проводим опросы. Это — наш feedback loop и главный источник пополнения бэклога. Кроме того, команда разработки werf постоянно следит за конкурирующими решениями в российском и международном коммьюнити.Несмотря на то, что главной целью werf изначально были ускорение и автоматизация наших внутренних процессов, мы уже достаточно давно развиваем ее не только для себя — постоянно находясь в поиске баланса между удовлетворением собственных потребностей и запросов от сообщества.Утилита пришлась по душе пользователям, сегодня у нее уже 3,6k звезд на GitHub — можно поставить звездочку и от себя. Кстати, если вы используете werf в работе, напишите нам в чат или в комментариях под этой статьей — мы хотим описать несколько кейсов и вообще поговорить о том, как ее используют за пределами «Фланта».Дополнительные источникиСайт werfGitHubwerf в CNCF Телеграм-канал werfСамоучитель по CI/CD с werf для разработчиковПервые шаги с werf: собираем и деплоим простое приложение в Kuberneteswerf — наш инструмент для CI/CD в Kubernetes (обзор и видео доклада)werf vs Docker. Чем лучше собирать образыDeckhouse: от Bash-скриптов до полноценной Kubernetes-платформыВернемся к другому нашему Open Source-проекту — shell-operator. Мы обслуживали инфраструктуру клиентов и постоянно писали скрипты для автоматизации различных задач, в том числе связанных с K8s. Операторы — рекомендованный способ автоматизации задач в Kubernetes, и со временем мы пришли к логичному пониманию, что нам нужен инструмент, который  упростит и ускорит автоматизацию небольших задач силами DevOps-инженеров и системных администраторов. Обычно они знают Python и Shell/Bash, а вот Go, нативный язык для операторов под Kubernetes, — гораздо реже.Так появился shell-operator, который позволяет подписаться на события от объектов Kubernetes, а когда получает их, автоматически запускает какую-то внешнюю программу (например, Bash- или Python-скрипт) и передает в нее информацию о событии. @konstantin_axenov на HighLoad++ 2022 рассказывает о том, как устроена разработка Kubernetes-платформы DeckhouseИз shell-operator в DeckhouseВообще, изначально shell-operator был неотъемлемой частью куда большего продукта — будущей Kubernetes-платформы. Но мы увидели в нем явный потенциал для прямого применения другими инженерами и решили выделить в отдельный проект, опубликовав как Open Source на GitHub в 2019 году.В то же самое время, благодаря существованию shell-operator, мы написали кучу модулей и операторов для настройки и автоматизации задач, связанных с Kubernetes. Делая многочисленные автоматизации на Bash, мы сразу четко осознавали, что это лишь временное решение — некий proof of concept. И когда это решение прошло проверку и заработало в реальных кластерах так, как нам надо, мы планово перешли к его переделке в нормальный вид — стали переписывать его на Go.О какой будущей платформе идет речь? Обслуживая многочисленные Kubernetes-кластеры, мы естественным образом пришли к созданию собственного K8s-дистрибутива, который собрал бы все наши скрипты, патчи, интеграции и другие дополнения в единую систему. Первый коммит в свою Kubernetes-платформу, тогда еще закрытый внутренний проект под названием antiopa, мы сделали 24 сентября 2017 года. Создавая его, мы не стремились повторить путь других международных проектов или импортозаместить их — мы просто упаковывали в удобный (для себя же) формат тот опыт с Kubernetes, который накопился за годы работы с этой технологией.В основу разработки платформы были заложены три принципа:Она работает с ванильным Kubernetes. Никаких доработанных напильником форков, которые надо поддерживать и развивать отдельно от эталонной ветки, получая постоянные проблемы с синхронизацией функций и закладывая в инфраструктуру бомбу замедленного действия в виде кастомной версии K8s. Мигрировать с такой самоделки на ванильный Kubernetes потом будет мучительно.Платформа должна быть Open Source-продуктом. Все компоненты платформы должны обновляться максимально автоматически: вышла новая версия, запускаешь обновления — получаешь обновленный набор инструментов, в которых сохранены все настройки и учтены особенности новых версий всех компонентов, зависящих друг от друга.5 августа 2019 года мы стали выбирать название для это Kubernetes-платформы: в то время это был внутренний инструмент для использования в DaaS-командах, которые обслуживали клиентов, но мы уже понимали, что наша разработка может заинтересовать и более широкий рынок. В обсуждении всплывали самые разные варианты — от «Штурвала» и «Штурмана» до QUbertenas (не спрашивайте😅) и KFC (kubernetes flant cluster), но в итоге мы остановились на Deckhouse (корабельная рубка).Логотип Kubernetes-платформы DeckhouseВ апреле 2020 года произошла первая обособленная от услуги DevOps as a Service продажа программного решения. Так мы подтвердили гипотезу о том, что отечественная Kubernetes-платформа может быть востребованной не только на клиентских проектах по обслуживанию Kubernetes-инфраструктуры. Выход в Open Source и бизнес-модель28 июля 2021 года Deckhouse уже появился в публичном репозитории на GitHub, были официально анонсированы две версии: бесплатная Community Edition и платная Enterprise Edition. Со стороны это может показаться чем-то довольно простым — вот есть исходники, вот они залетают на GitHub — но внутри была проделана колоссальная работа.Что мы подготовили к публичному релизу:Переработали и дополнили техническую документацию.Создали телеграм-канал и чаты для общения с пользователями бесплатной версии.Создали сайт deckhouse.io (на двух языках).Написали руководство по началу работы с продуктом (Getting Started).Начали собирать команду продаж Deckhouse (до этого мы продавали только услуги DaaS).  Уже 6 августа 2021 года, спустя неделю после выхода в Open Source, Deckhouse прошел сертификацию CNCF, а в декабре был включен в реестр отечественного ПО. Примерно в то же время в команде Deckhouse появился первый продакт-менеджер.В 2022 году мы прошли проверку на соответствие рекомендациям PCI Security Standards Council, сертифицировались для работы с «Ред ОС», Astra Linux и AlterOS.2023 год тоже был богат большими событиями. В феврале команда разработки добавила в Deckhouse модуль Virtualization на основе KubeVirt, который позволяет управлять виртуальными машинами с помощью Kubernetes, а в марте — модуль Delivery на основе werf и Argo CD для непрерывной поставки приложений в Kubernetes. Вовсю кипит работа над Deckhouse UI и сертификацией ФСТЭК.@Magvai69 на DevOps Conf 2023 рассказывает, без чего именно он не ходит мимо тещиного домаБизнес-модель Deckhouse и переосмысление понятия «продукт»Бизнес-модель напрашивалась сама собой: мы верим в идеалы Open Source и хотим его продвигать, поэтому казалось естественным, что код Deckhouse будет открыт. Делать свое облако с Deckhouse и продавать его было бы слишком трудозатратно, а вот продавать лицензию на софт по модели Open Core — самое то для компании, которая любит Open Source. В платной версии Deckhouse EE есть набор дополнительных фич, но она полностью основана на бесплатной версии (CE). То есть Deckhouse CE — реально работающая платформа, на которую мы даже помогаем переехать с enterprise-версии тем клиентам, которые перестают у нас обслуживаться в рамках услуги DevOps as a Service. Кстати, если вы используете Deckhouse Community Edition — напишите нам в чат или в комментариях к этой статье, нам интересно, как CE запускается и работает без нашего участия, какие проблемы возникают и т.п.Публичный релиз и начало продаж Deckhouse очень сильно повлияли на всю компанию — вот что мы поняли:Нужна отдельная команда для второй линии поддержки. Раньше на этой линии дежурили разработчики платформы, но теперь они «переехали» на третью и даже четвертую линии. Чтобы соответствовать ожиданиям рынка и потребностям клиентов, необходимо наращивать команду разработки. Продуктовая работа — это не только разработка ПО. Это и огромная обвязка вокруг продукта: упаковка, документация, сайт с контентом, презентации и КП для клиентов, постоянный мониторинг конкурентов и сравнение с конкурентами по запросу от потенциальных клиентов, культура проведения клиента по воронке продаж, пилотные внедрения, фиксация результатов, работа над болевыми точками клиента, пиар и маркетинг, постоянная рефлексия.Дополнительные источникиСайт DeckhouseGitHubТелеграм-канал DeckhouseЧат по DeckhouseУстанавливаем Kubernetes-платформу Deckhouse в закрытом окружении. Пошаговая инструкцияРазворачиваем Kubernetes-кластер на bare metal под управлением Deckhouse и создаем в нем виртуальные машиныКак устроена разработка Kubernetes-платформы Deckhouse (обзор и видео доклада)Российский Kubernetes, какой он? Знакомимся с платформой DeckhouseПокупка Okmeter и мониторинг инфраструктурыИстория Okmeter для нас — еще одна ступенька взросления компании. Одно дело — создавать собственные продукты и оказывать услуги, и совсем другое — купить готовый проект и начать вести его самостоятельно. Как мы уже упоминали, Okmeter был сторонним проектом, который мы с 2017 года использовали для мониторинга в рамках услуги DaaS. Какое-то время мы были основными пользователями системы — возвращались с обратной связью, делились своим опытом и подходами к мониторингу, просили добавить какие-то фичи и т.п. У нас даже были своя инсталляция Okmeter (хотя в основном он продавался по модели SaaS) и отдельный личный кабинет, в который мы заводили своих клиентов. Спустя несколько лет после начала работы с Okmeter мы поняли, что нам хочется все больше и больше влиять на процесс разработки инструмента и он хорошо вписывается в экосистему продуктов, которые мы уже планировали активно развивать. Мы пообщались с основателями Okmeter и оказалось, что у них есть идеи для новых продуктов, за которые им хотелось бы взяться более активно (кстати, недавно мы как раз написали обзор на их новый инструмент — Coroot.). Сделка была взаимовыгодной: команда Okmeter получала ресурсы для разработки новых продуктов, а мы дополняли свою экосистему классным и хорошо известным нам готовым решением и могли развивать его так, как нам интересно. Логотип системы мониторинга OkmeterВ мае 2021 года мы провели сделку, в результате которой Okmeter окончательно стал нашим стандартом для мониторинга железа, серверов, баз данных, приложений. Так как мы уже продолжительное время были сильно завязаны на Okmeter и уже хорошо знали его, сделка не казалась чем-то страшным. Конечно, определенные проблемы были: например, Okmeter базировался на самописном ядре хранения и обработки метрик, в котором был настоящий винегрет из технологий хранения данных — Elasticsearch, Kafka, Cassandra, PostgreSQL и т.д. К весне 2023 года мы перевели все хранение данных на Grafana Mimir. Это заняло у нас примерно год: пришлось долго разбираться в чужом коде, обсуждать, как Okmeter будет развиваться дальше, переносить данные — да и ресурсы разработки были ограничены. Переход на новое ядро прошел бесшовно и незаметно для клиентов: мы сконвертировали и заменили все данные на лету. Okmeter стал работать ощутимо стабильнее.Первые несколько месяцев после покупки были очень увлекательными: у тебя есть готовый код, но нет прежней команды команды разработки, только возможность консультироваться с ней. И тут выясняется, что в коде куча нюансов и всяких неочевидных моментов. Но за пару месяцев мы освоились с кодом, и дальше уже работа пошла гораздо быстрее. Сейчас Okmeter включен в единый реестр российского ПО и существует в двух вариантах: on-premise и SaaS. Кроме того, он входит в состав Deckhouse EE.Дополнительные источникиСайт OkmeterТелеграм-канал OkmeterМимо тёщиного дома я без метрик не хожу (обзор и видео доклада)«Кому-то Okmeter даже сможет заменить людей». Как будет развиваться сервис мониторинга после его покупки «Флантом»Мониторинг PostgreSQL. Расшифровка аудиочата Data Egret и OkmeterЧто еще случилось за последние пять летПомимо работы над тремя продуктами, мы продолжали развивать свое DaaS-направление и совершенствовать понимание бизнес-потребностей клиентов. Последнее давалось с трудом, потому что мы в первую очередь компания инженеров и больше привыкли к общению с техническими командами. Итак, что же еще у нас произошло за последние пять лет?Осенью 2018 года наши инженеры начали получать первые международные сертификаты Certified Kubernetes Administrator (CKA), которые выдает Cloud Native Computing Foundation (CNCF). Это позволило наладить внешнюю, объективную проверку навыков наших инженеров и дать им дополнительный инструмент для повышения собственной профессиональной ценности. В январе 2019 года мы получили «корочки» для всей компании, став первым Kubernetes Certified Service Provider (KCSP) и Silver Member в CNCF из России. В мае того же года мы начали активно выходить на крупный бизнес и запустили услугу «Kubernetes для enterprise». В рамках этой услуги мы предлагали крупным компаниям развёртывание и поддержку кластеров Kubernetes с применением своих лучших практик, а также обучение инженеров заказчика работе с этими кластерами. При этом за эксплуатацию рабочих нагрузок, размещенных внутри кластеров, отвечали внутренние команды клиента, а мы сопровождали и оказывали поддержку исключительно по K8s.Пришли на корпоратив и вляпались в Kubernetes по самые уши. 2019 годНо по-настоящему фокусироваться на enterprise-сегменте мы начали только спустя пару лет. И тут же столкнулись с проблемой: мы не умели работать с enterprise, не понимали его. Мы привыкли продавать свои услуги в СМБ-сегменте и говорить на языке простых технических и финансовых выгод: «Мы сделаем вот так и получится вот это». С крупными клиентами такой подход работает хуже: во-первых, в процесс переговоров вовлекается гораздо больше лиц, принимающих решения, а во-вторых, сами потребности enterprise могут быть очень далеки от понятных и прозрачных потребностей небольших компаний.Мы начали выращивать собственные компетенции, но процесс шел слишком долго. И стало понятно, что это не очень продуктивный подход — нам нужен качественный прорыв. В 2022 году появилась такая возможность: компания Oracle ушла из России, и мы пригласили ее команду продаж во «Флант». Специалисты из Oracle имели огромный опыт взаимодействия с enterprise-сегментом, обладали обширными наработанными связями, а потому сумели в кратчайшие сроки выстроить работу с крупными клиентами и у нас.Параллельно с развитием технической и бизнес-части «Фланта» росла и команда. В апреле 2021 года, за несколько месяцев до публичного релиза Deckhouse, нас уже было больше 100 человек — и с тех пор мы выросли еще вдвое. По состоянию на июнь 2023 года флантовцев уже более 200. Причем мы не останавливаемся: например, у нас идет активный наем Golang-разработчиков в продуктовые команды. DaaS мы тоже никогда не забывали. И хотя там все менялось не так бурно — за долгие годы мы наработали огромный опыт, у нас сложились свои отработанные подходы и практики — возможности для улучшений есть всегда. Во-первых, мы начали передавать отдельные направления в специализированные команды, чтобы DevOps-инженеры не отвлекались на лишние задачи. Например, выделенная команда полностью занимается поддержкой платформы Deckhouse, которая входит в состав услуги DaaS.Во-вторых, у нас начали складываться гильдии — неформальные горизонтальные объединения специалистов по разным направлениям из разных команд. Такие объединения делятся узкой экспертизой, вырабатывают стандарты и подходы к работе, обмениваются новостями индустрии. Например, участники гильдии по базам данных могут брать на себя соответствующие задачи в своих командах или консультировать коллег.В-третьих, постоянно эволюционируют и наши процессы работы над клиентскими проектами, система менеджмента. Состояние системы на 2019 год зафиксировали и подробно описали в своих докладах наш основатель Дмитрий Столяров и менеджер проектов Сергей Гончарук. А об изменениях последних трех лет все тот же Сергей рассказал в своей недавней статье «Большая перемена: как за 3 года мы пересмотрели управление проектами во „Фланте”».Стенд «Фланта» на HighLoad++ 2022Не забывали мы и про Open Source. В 2018 году мы выложили в открытый доступ плагин Grafana Statusmap panel, который позволяет состояния набора объектов за выбранный промежуток времени (и на сегодня его скачали более 35 миллионов раз!), и kubedog, Go-библиотеку и CLI на её основе для отслеживания событий ресурсов Kubernetes. В апреле 2022 года выпустили утилиту trdl для непрерывной и безопасной доставки обновлений (мы используем её для werf, но проект может оказаться полезным и отдельно от нее).Кроме того, мы активно работаем над многими международными Open Source-проектами — например, в 2023 году наконец довели до релиза два KEP (полноценных фичи) в рамках системы аутентификации Kubernetes. Первый добавил API для просмотра атрибутов текущего пользователя после завершения процесса аутентификации, а второй обеспечивает удобный способ конфигурации OIDC и является крупнейшим обновлением в аутентификации за последние годы.Дополнительные источники10 лет on-call. Чему мы научились? (обзор и видео доклада)Управление распределенной командой в режиме многопроектности (обзор и видео доклада)Большая перемена: как за 3 года мы пересмотрели управление проектами во «Фланте»Kubernetes 1.26: обзор нововведений, включая первый KEP «Фланта»ИтогХотя прошло уже 15 лет, мы все так же продолжаем бурно расти и развиваться. У нас есть амбициозные цели — как по найму, так и по доле рынка, на которую мы рассчитываем. При этом нашей главной ценностью остается команда — мы стремимся, чтобы  каждый сотрудник «Фланта» мог свободно предлагать свои идеи, задавать острые вопросы, открыто обсуждать радости и печали. По нашему убеждению, только такая прозрачность по отношению к команде позволяет «Фланту» делать качественные востребованные технические продукты и поддерживать высочайший уровень инженерной культуры. Именно поэтому мы прикладываем так много усилий, чтобы при качественно выстроенных комфортных процессах и зрелом подходе к ведению бизнеса сохранять атмосферу молодости, свободы, энтузиазма и творчества — то, что принято называть духом стартапа.Флантовцы в Питере, июнь 2023 годаЗа все годы существования компании мы не раз пересматривали систему менеджмента, коммуникации, подходы к организации работы. В итоге, как нам кажется, мы нашли свой дзен: очень теплую и человечную удаленку, в которой горизонтальные связи между участниками команды не только сохраняются, но и укрепляются и даже переходят в оффлайн — мы знаем это, потому что в одном из наших Slack-каналов постоянно мелькают фото с личных встреч флантовцев в разных городах и странах.И насколько бы амбициозными ни были наши планы, мы знаем, что они осуществляются только благодаря точечному вкладу каждого участника команды. Хотим поблагодарить всех, кто был с нами раньше, кто сопровождает нас в этом большом и увлекательном путешествии прямо сейчас, кто доверяет нам свою инфраструктуру и верит в наши продукты или, например, просто читает статьи в нашем блоге. Все это стало возможным только благодаря вам! P.S. Ну что, встретимся в новой статье еще через 5 лет?😉  ",224,0
О пользе стрельбы по пианистам,https://habr.com/ru/articles/743456/,2023-06-23T06:06:44.000Z,"Прочел я тут комментарии к данной статье и вот что я хочу сказать вам, господа хорошие. Когда читаешь очередной перевод на ХабреПриятно наверное с утречка открыть любимый Хабр, прочитав корявый перевод, тыкнуть автора прилюдно в его ошибки. Наверняка автор комментария сам знает английский или немецкий в совершенстве, перевел ни один десяток статей и запостил их на хабр. Но смотрим, а у комменторов в основном куча комментариев и ... ни одной публикации. Да, меня тоже иногда коробит от неправильного перевода и я обычно в таких случаях пишу автору статьи об этом в личном сообщении. Это не так приятно для собственного эго, но полезно все же для самой статьи. Да, уровень переводов падает и можно всегда заявить, что Хабр не тот, раньше то трава была зеленее и плохих переводов было меньше. Но количество качественных переводов сокращается именно потому, что авторы не из корпоративных хабов создавали посты просто для того, чтобы получить какую-то похвалу, плюсик в карму, за который и гамбургер не купишь. Но обычно получают кучу комментариев типа - ""автор тупой, безграмотный"" и т.д., иногда   вполне справедливых, но обычно от людей, которые сами контент не создают. Они находят время на то, чтобы в развернутом комментарии, иногда с объемом на 1/3 статьи рассказать автору, что он... неумный человек. Ну что ж, верной дорогой идете товарищи, верной!  ",1.1K,10
Кодирование и декодирование данных,https://habr.com/ru/companies/otus/articles/743440/,2023-06-23T05:47:30.000Z,"

Автор статьи: Артем Михайлов

Кодирование и декодирование данных — это процессы, которые стали неотъемлемой частью нашей цифровой жизни. Они позволяют «упаковать» большой объем информации и передать ее по различным каналам связи, в том числе через Интернет. Кодирование и декодирование не ограничиваются только передачей данных. Они также применяются в графике, звуке, видео и многих других областях.

Зачем нужно кодировать и декодировать данные? Ответ состоит в том, что нам нужно сократить объем информации и передать ее за минимальное время и с минимальными потерями. Кодирование позволяет компрессировать данные до необходимого уровня, а декодирование — восстановить их при получении.

Важным примером применения кодирования и декодирования данных является цифровое телевидение и интернет-трансляции. Например, H.264  — это технология кодирования видео, которая позволяет получить максимально высокое качество с минимальным объемом данных. При этом, при передаче через Интернет, данные сжимаются, и после достижения адресата автоматически декодируются.

Основы кодирования данных
Кодирование данных — это процесс преобразования информации в определенный формат, который облегчает ее передачу и сохранение. Это важный этап при передаче данных в сети или их хранении на компьютере.

Основная идея кодирования данных заключается в том, чтобы сместить информацию из одной формы в другую, более удобную для передачи или хранения. Существует много способов кодирования данных, и каждый из них используется в зависимости от целей и требований конкретной задачи.

Типы кодирования данных могут включать двоичное, десятичное, шестнадцатеричное и многие другие. Эти типы кодирования используются для представления информации в различных системах. Например, двоичная кодировка используется в компьютерах и электронике, а десятичная кодировка в финансовой отчетности.

Алгоритмы кодирования данных — это набор инструкций, которые позволяют вычислить кодированное представление информации. Они могут быть очень простыми или очень сложными, в зависимости от целей и требований конкретной задачи.

Одним из наиболее распространенных алгоритмов кодирования данных является алгоритм Хаффмана. Он используется, например, в сжатии данных и сжатии аудио- и видеоконтента. Для этого алгоритма используется дерево Хаффмана, которое позволяет закодировать информацию с минимальной потерей качества.

Кодирование данных — это важный процесс, который позволяет сохранить и передать информацию в форме, которая наилучшим образом соответствует требованиям конкретной задачи. Искать и выбирать наиболее подходящий способ и алгоритм кодирования данных — ключевой этап в процессе разработки программного обеспечения.

Пример кодирования данных на Python:

# Задаем выдуманный код для кодирования данных
code = {
    ""A"": ""001"",
    ""B"": ""010"",
    ""C"": ""011"",
    ""D"": ""100"",
    ""E"": ""101"",
    ""F"": ""110"",
    ""G"": ""111""
}

def encode_data(data):
    """"""Функция кодирования данных""""""
    encoded = """"
    for char in data:
        encoded += code[char.upper()]
    return encoded

def decode_data(data):
    """"""Функция декодирования данных""""""
    decoded = """"
    while data:
        for char, sequence in code.items():
            if data.startswith(sequence):
                decoded += char
                data = data[len(sequence):]
                break
    return decoded

Кодирование производится путем замены каждого символа данных на соответствующую последовательность из заданного кода. Декодирование предполагает обратную замену последовательностей на символы данных.

Следующий пример кода на Python, который позволяет закодировать сообщение с использованием выдуманного кода:

def encode_message(message):
    coded_message = ''
    for letter in message:
        if letter == 'a':
            coded_message += '134'
        elif letter == 'b':
            coded_message += '52'
        elif letter == 'c':
            coded_message += '999'
        # Добавьте свои правила кодирования для других букв
        
    return coded_message


Пример использования функции:

message = 'abc'
coded_message = encode_message(message)
print(coded_message) # Выводит: 13452999

Функция encode_message принимает строку message и возвращает закодированную версию этой строки, используя выдуманный код. В примере, который мы предоставили, буква «a» была заменена на «134», буква «b» — на «52», а буква «c» — на «999». Вы можете добавить свои правила кодирования для других букв, что позволит вам закодировать сообщение любой сложности.

После того как вы успешно закодировали свое сообщение, вы можете использовать следующий код, чтобы декодировать его:

def decode_message(coded_message):
    decoded_message = ''
    i = 0
    while i < len(coded_message):
        if coded_message[i:i+3] == '134':
            decoded_message += 'a'
            i += 3
        elif coded_message[i:i+2] == '52':
            decoded_message += 'b'
            i += 2
        elif coded_message[i:i+3] == '999':
            decoded_message += 'c'
            i += 3
        # Добавьте свои правила декодирования для других букв
        
    return decoded_message

Пример использования функции:

decoded_message = decode_message(coded_message)
print(decoded_message) # Выводит: abc

Функция decode_message принимает закодированную версию сообщения coded_message и возвращает раскодированную версию этого сообщения, используя правила, заданные в encode_message. Например, закодированное сообщение «13452999» было успешно раскодировано как «abc».

Это всего лишь базовый пример кода для кодирования и декодирования данных на Python. Но, к счастью, Python предлагает множество встроенных средств для работы с кодированием и декодированием данных, которые сделают этот процесс еще более простым и эффективным.

Основы декодирования
Декодирование данных является важным этапом обработки информации и заключается в превращении закодированных данных обратно в исходный формат. Это необходимо для того, чтобы данные стали доступными для дальнейшей обработки и использования. 

Существует несколько типов декодирования данных, в том числе: 
1) декодирование текстовой информации;
2) декодирование аудио и видеофайлов;
3) декодирование изображений;
4) декодирование компьютерных программ и файлов.

Каждый из этих типов имеет свои особенности и алгоритмы декодирования. 

Для декодирования текстовой информации часто используются различные кодировки, такие как UTF-8, ASCII и другие. Алгоритмы декодирования определяются типом кодировки и могут включать в себя поиск и корректировку ошибок, связанных с неправильным переводом символов.

Для декодирования аудио- и видеофайлов используются соответствующие программы, которые осуществляют преобразование формата файла в исходный вид. Алгоритмы декодирования подобных файлов могут включать в себя распаковку данных, реверсирование звуков и видеофрагментов, а также их соотнесение с соответствующими мета-данными.

Для декодирования изображений применяются специальные алгоритмы, такие как JPEG, GIF, PNG и другие. Эти алгоритмы позволяют декодировать изображения и переводить их в исходный формат. 

Декодирование компьютерных программ и файлов имеет свои особенности и может быть использовано, например, при исправлении ошибок в программном коде. Для декодирования таких файлов могут использоваться различные инструменты, такие как дизассемблеры и декомпиляторы, которые позволяют перевести объектный код в текстовый формат.

Для декодирования данных выдуманного кода можно использовать алгоритм декодирования Хаффмана:

class HuffmanNode:
    def __init__(self, char=None, freq=0, left=None, right=None):
        self.char = char
        self.freq = freq
        self.left = left
        self.right = right

    def __lt__(self, other):
        return self.freq < other.freq

def decode(code, root):
    decoded_str = """"
    node = root
    for bit in code:
        if bit == ""0"":
            node = node.left
        else:
            node = node.right

        if node.char is not None:
            decoded_str += node.char
            node = root

    return decoded_str

# Пример использования функции decode:
if __name__ == ""__main__"":
    # Создаем вершину дерева Хаффмана и раскодируем данные
    root = HuffmanNode(left=HuffmanNode(left=HuffmanNode(char='a', freq=2), right=HuffmanNode(char='b', freq=3), freq=5),
                       right=HuffmanNode(left=HuffmanNode(char='c', freq=4), right=HuffmanNode(char='d', freq=5), freq=9),
                       freq=14)

    code = ""1101111110101010111010""
    decoded_str = decode(code, root)
    print(decoded_str)

Данный код декодирует выдуманный код, заданный в переменной code, с помощью дерева Хаффмана, заданного в переменной root. 

Основные методы кодирования
1. Безусловное кодирование
Безусловное кодирование — это метод кодирования данных, в котором каждому символу или значению присваивается определенный уникальный код, который не зависит от содержания информации. Этот метод включает в себя простые коды, такие как бинарный код, ASCII код, и т. д. Бинарный код — это двоичная система кодирования, где каждый символ или число представлены битами '0' и '1'. ASCII код — это таблица, которая содержит набор символов для представления текстовых значений.

Пример кода для бинарного кодирования:

def decimal_to_binary(decimal_num):
    binary_num = bin(decimal_num)
    return binary_num[2:]

result = decimal_to_binary(10)
print(result) # Output: 1010


2. Условное кодирование
Условное кодирование — это метод bкодирования данных, в котором каждый символ или значение имеет сложный код, который зависит от содержания информации. Этот метод включает в себя арифметическое кодирование, Хаффмана кодирование, и т. д. Арифметическое кодирование — это метод, который закодирует каждый символ в последовательность цифр от 0 до 1, которые представляют вероятность появления символа в сообщении.

Пример кода для арифметического кодирования:

def arithmetic_encoding(data):
    freq_dict = {}
    for char in data:
        freq_dict[char] = freq_dict.get(char, 0) + 1
    prob_dict = {char: freq_dict[char]/len(data) for char in freq_dict}
    low, high = 0.0, 1.0
    for char in data:
        range_width = high - low
        high = low + range_width * prob_dict[char]
        low = low + range_width * sum(prob_dict[c] for c in prob_dict if c < char)
    return (high + low) / 2

result = arithmetic_encoding('hello')
print(result) # Output: 0.625

3. Блочное кодирование
Блочное кодирование — это метод кодирования данных, в котором информация разбивается на блоки определенного размера, и каждый блок кодируется независимо от других. Этот метод включает в себя код Хэмминга, код Рида-Соломона, и т. д. Код Хэмминга — это метод, который добавляет дополнительный бит в сообщение, чтобы обеспечить коррекцию ошибок.

Пример кода для кодирования сообщения с помощью кода Хэмминга:

def hamming_encode(data):
    n = len(data)
    k = 2**int(math.log(n, 2)) - int(math.log(n, 2)) - 1
    r = n - k
    encoded_data = """"
    for i in range(r):
        encoded_data += '0'
    for i in range(1, n + 1):
        if not math.log(i, 2).is_integer(): 
            encoded_data += data[i - 1]
    for i in range(1, r + 1):
        pos = 2**(i - 1)
        count = 0
        for j in range(1, n + 1):
            if j & pos == pos:
                count ^= int(data[j - 1])
        encoded_data += str(count)
    return encoded_data

result = hamming_encode('0101')
print(result) # Output: 0110001

Заключение
В заключение, можно сказать, что кодирование и декодирование данных являются важными инструментами в области информационных технологий. Они позволяют защитить данные от несанкционированного доступа, а также упростить передачу информации через сети. С развитием технологий и научных исследований, появляются все более совершенные методы кодирования и декодирования данных, что позволяет обеспечить высокий уровень безопасности и надежности передачи информации. Без таких методов, современная информационная система не смогла бы функционировать.

А если вам интересно как пишутся языки программирования, хочу порекомендовать бесплатный вебинар, на котором эксперты OTUS расскажут как разрабатываются языки программирования, построят вместе с вами LL(1)-анализатор алгоритмического языка программирования.
Также на занятии будут обсуждаться ограничения LL(1)-анализаторов и некоторые приемы работы с LL(1)-грамматиками.


Зарегистрироваться на бесплатный урок
  ",479,1
"Импортозамещение по-китайски: программа стоимостью в $1,4 трлн вряд ли даст нужные результаты к 2025 году",https://habr.com/ru/companies/selectel/articles/733728/,2023-06-23T00:20:02.000Z,"

Два года назад мы писали о том, что правительство КНР выделило огромную по меркам любой страны и отрасли сумму в $1,4 трлн. Деньги выделены на нужды отрасли производства электронных компонентов. Средства не предоставляются одномоментно — их выделяют постепенно китайским профильным компаниям. 

Результаты работы программы стали заметны достаточно быстро — уже через несколько месяцев. Но сейчас все громче звучат голоса экспертов (в том числе, и из Китая) о том, что программа «Made in China 2025» не даст нужных результатов. Подробности — под катом. 

Проблемы вовсе не в деньгах  
Несмотря на то, что электронная промышленность Китая активно развивается, существует немало проблем. Например, за последние пару лет закрылось свыше 10 000 компаний — разработчиков чипов из Поднебесной. Большая часть была открыта как раз при поддержке государства. 

Изначально казалось, что все отлично. СМИ Китая рапортовали об увеличении компаний-разработчиков чипов на несколько десятков тысяч. Это очень бурный рост. Например, в 2015 году таких компаний было 736, в 2017 — 1780. А вот с 2020 по 2021 год в стране было зарегистрировано около 70 тыс. новых компаний, которые занимались разработкой чипов. 

Как оказалось, государственная поддержка — это еще не все, компаниям, которые поддержали финансированием, пришлось конкурировать как друг с другом, так и с международным бизнесом. Не все прошли этот этап испытаний. Более того — негативных факторов оказалось гораздо больше, чем обычная, пусть и жесткая, конкуренция. 



Проблема в том, что все они зависели от субсидий государства. Кроме того, за короткое время возникло большое количество не особо отличающихся друг от друга компаний, которые занимались разработкой микросхем «массового потребления», если так можно выразиться. Они открывались, получали субсидии, и через короткое время закрывались. Журналисты Digitimes даже прозвали такие организации фальшивыми, fake tech companies. Стоимость же одной такой компании — многие миллионы, а то и десятки/сотни миллионов долларов США, то есть денег на поддержку «фейков» ушло просто огромное количество. 

Еще одна проблема в том, что даже «белые и пушистые» разработчики чипов, которые открывались, ставя своей целью реальную работу, не могли заполучить в штат опытных специалистов, включая как технологов, так и управленцев. А ведь полупроводниковый бизнес — крайне сложен, здесь нужно уметь выстраивать логистические цепочки, договариваться с поставщиками сырья, при необходимости — заключать договор с контрактными производителями чипов. И без нормального управления опытным менеджером такой бизнес обречен если не на провал, то на «тлеющее» существование, когда любой негативный фактор может привести к срыву производства, отмене контрактов и банкротству. 

Бурный рост обеспечивали не только китайские компании  
О еще одной проблеме рассказал Эй Шаоцзюнь (Wei Shaojun), заместитель директора Китайской ассоциации полупроводниковой промышленности. Ранее он обычно говорил об успехах Китая в электронной отрасли. Но сейчас он выступил с опасениями относительно успешного завершения программы. 

Так, он рассказал, что, действительно, в Китае активно растет объем чипов, произведенных в самой стране, а не завезенных откуда-то. В 2013 году лишь 13% от используемых в стране чипов производились в Поднебесной. Все остальное — завозили из-за рубежа. В 2022 году объем «домашних» чипов вырос до 41,4%.

Но такой быстрый рост обусловлен, в основном, иностранными предприятиями и инвестициями. Так, начиная с 2016 среднегодовой темп роста китайских компаний, владельцы которых — китайцы, составил 14,7%. А вот аналогичный показатель для компаний, основные держатели долей в которых — иностранцы, либо же компаний, которые полностью принадлежат зарубежному бизнесу, составил 30%. Среди последних — компании, созданные бизнесом из Тайваня, Южной Кореи и других стран. 



Насколько можно понять, при составлении программы «Made in China 2025» эксперты обращали внимание только на глобальные цифры роста, не разделяя иностранный и отечественный капитал. К сожалению, в результате торговой войны Китая и США доля иностранного бизнеса сокращается, равно как сокращаются и объемы иностранных инвестиций. Если раньше Китай сверхактивно развивался благодаря глобализации, то сейчас, в связи с переходом многих компаний на производство чипов в собственной стране (США, ЕС и т.п.), у Поднебесной появились серьёзные проблемы. Соответственно, из-за деглобализации достичь цифры в 70% относительно «домашних» чипов, т.е. чистого импортозамещения, к 2025 году вряд ли удастся. 

Сейчас в Китае наблюдается также очень большая разница между спросом на полупроводники и предложением китайских компаний. Спрос — около 1,5 млн 300-мм пластин в месяц. Предложение же составляет всего 440 000 пластин. Все остальное — либо завозится, либо производится в Китае иностранным бизнесом. Также Эй Шаоцзюнь заметил, что многие китайские компании работают неэффективно, о чем уже говорилось в предыдущем разделе. 

И это еще не все  


Кроме внутренних негативных факторов, на китайские компании, занятые в полупроводниковой отрасли, действуют еще и мощные внешние факторы. Их тоже никак нельзя назвать положительными. Изначально бизнес КНР очень неплохо себя чувствовал в условиях дефицита полупроводников. Имеются в виду компании, которые поставляют сами чипы на рынок, от «ширпотреба» до эксклюзивов в виде мощных современных процессоров и графических адаптеров. Цены все время росли, спрос тоже увеличивался и, в целом, все было хорошо.

Но в 2022 году начался «отлив» — постепенно начал падать спрос, за ним — цены, при том, что запасы устройство и компонентов на складах продолжали увеличиваться. Поэтому компании, которые держались исключительно на субсидиях государства и растущих ценах, сразу почувствовали на себе влияние кризиса.

Еще один фактор — все более жесткие санкции США, которые закрыли доступ многим производителям чипов из Китая на глобальный рынок. Ну а европейским и американским поставщикам инструментов для производства чипов, т.е. литографических машин, также запретили сотрудничать с КНР.

Все плохо? 


В целом, хуже, чем рассчитывали китайские власти и бизнес, но в любом случае электронная отрасль Китая развивается очень активно. Возможно, речь идет просто о переносе результата в 70% производимых китайскими компаниями чипов на более поздний срок. 

Сейчас, чтобы получить больше возможностей в собственной стране, китайские производители чипов постепенно наращивают закупки оборудования для производства полупроводниковых пластин. Делается это для того, чтобы обеспечить нормальную работу собственных предприятий. Понятно, что Поднебесная проводит подобные сделки скрытно, чтобы партнеры и сами китайские компании лишний раз не привлекали внимание регуляторов из США.

Если Китай получит дополнительные санкции, они могут значительно затормозить реализацию программы китайского импортозамещения. Если же страна как-то договорится с США, начнется урегулирование экономических проблем, то план может оказаться вполне реалистичным. Как будет на самом деле — покажет время.

Возможно, эти тексты тоже вас заинтересуют:

→  Создаем блог на Django с опросами и тестами. Краткая инструкция. Часть 1 
→  OpenStack vs VMware: что лучше — open source или проприетарная платформа 
→  Когда типизация делает тебе больно: как у нас появился статический анализатор   ",4.8K,8
Как сэкономить свои нервы и деньги компании на перестроении структуры больших таблиц без простоя в PostgreSQL,https://habr.com/ru/articles/743438/,2023-06-22T22:16:38.000Z,"Привет! Меня зовут Васильев Виктор, я DBA в компании CoMagic. В этой статье на реальных примерах расскажу, как можно сэкономить время разработчика, администратора баз данных и ресурсы сервера(ов), используя утилиту pg_rebuild_table. Сопровождая большие, высоконагруженные системы, с бо’льшей вероятностью каждый сталкивался с кейсами, о которых будет рассказано дальше. Некоторые технические подробности пройду без детализации, чтобы сильно не усложнять и не делать статью очень громоздкой. Лучше отвечу на вопросы в комментариях.Какие кейсы разберем:Чистка неактуальных данных в 100+ таблицах.Экономия дискового пространства, RAM и CPU.Изменение порядка колонок в больших таблицах.Изменение типа данных колонок в большой таблице.Перестроение таблицы без долгих транзакций.В современном мире есть тенденция переводить системы на микро-сервисную, слабосвязанную архитектуру с маленькими базами данных, где четко разделяются OLTP и OLAP профили нагрузки. Но не всегда мир идеален, и часто приходится работать с тем, что отлично существует многие годы и при этом развивается и обрастает новыми фичами.Речь пойдет про обслуживание таблиц в монолите большого размера. Несмотря на то, что на дворе уже 15-ая версия PostgreSQL, рассказывать буду в контесте 12-ой версии. И так, в нашем распоряжении шардированный по приложениям (далее “app”) кластер баз данных (далее “БД”) на PostgresSQL 12.5 версии, где БД является отдельной нодой, в которой тонны бизнесового кода в хранимых функциях на plpgsql, около 1000 партицированных и не партицированных таблиц некоторые из размером 500 ГБ и более, данные которых хранятся в разрезе app. Общий объем одной ноды около 20 ТБ. Отдельно развернут прокси-кластер БД, который знает про все ноды и место расположения данных каждого app. Перечисленные кластера хранятся на дорогих серверах с SSD-дисками. Все сервисы выполняют запросы на прокси БД и в свою очередь прокси маршрутизирует запросы на ноду, где лежат данные app, посредством маршрутизатора запросов plexor (про plexor подробнее в презентации). Профиль нагрузки принимают как OLTP, так и OLAP.С появлением в компании хранилища BI-storage нагрузка OLAP планомерно переводится туда. Для поддержки около реалтаймового обновления данных в таблицах использовали встроенную логическую репликацию с созданием публикации по изменениям данных для списка таблиц (см. картинку «Кластер баз данных с хранилищем BI-storage»).Кластер баз данных с хранилищем BI-storageНе секрет, что большую базу данных сложно обслуживать. Особенно расстраивает, когда утилизация ресурсов сервера приближается к предельным показателям. Мы часто с этим сталкиваемся. Отсюда появляется задача разделить БД на две в разрезе app. Это значит, что для половины app данные должны остаться в текущей ноде БД, а другая половина данных должна уехать в новую ноду БД на новый сервер. Такое решение позволит размазать нагрузку на два сервера и уменьшить размер БД в два раза.Подходов для решения задачи оказалось немало. Остановился на одном из самых простых. Поднять реплику БД на другом сервере, используя pg_basebackup, в определенный момент сделать ему promote и сказать прокси БД, что появилась новая нода и теперь половина app переехали туда. Решение выглядело простым, и было реализовано, нагрузка распределилась на два сервера, в итоге немного выдохнули. Физически данные по app задублировались и на обеих нодах остались, как балласт.Не все проблемы удалось решить, да еще и новые появились:Место на сервере по прежнему заканчивается. требуется очистить таблицы от ненужных данных по app и обеспечить максимальную плотность хранения данных.С появлением новой ноды, первичные ключи на основе сиквенса int4 стали скручиваться еще быстрее. И таких таблиц немало, а допускать пересечения значений в первичных ключах на шардированной БД нельзя  —  требуется изменять тип данных первичного ключа на int8. Чистка неактуальных данных в 100+ таблицах без простоя в работе.Перед нами стоит задача удалить данные из таблиц по app, которые больше не актуальны. При этом важно позаботиться о том, чтобы публикация логической репликации в БД не отдала команды удаления строк подписчикам для последующего применения их в хранилище. Также нам нельзя убирать таблицы из публикации и терять изменения, которые инициируются сервисами, так как хранилище должно поддерживать консистентное и около реалтаймовое состояние данных.Зная, как работают публикации, становится понятно, что комбинация команд delete from [table_name] where app_id in (...);, с последующим запуском pg_repack для очистки пустот в данных, нам не поможет. Да и делать минимум по два прохода по таблицам долгая и муторная затея.Возникает вопрос: как очистить данные в таблицах и не отправить команды удаления в публикациях?В 15-ой версии PostgreSQL появилась возможность добавлять предикат к таблице в публикациях, что позволяет фильтровать отправку реплицируемых данных, например по списку app. Но напомню, мы имеем дело с 12-ой версией и обновление до 15-ой не такой уж быстрый и простой проект для нашего монолита. В этом решении подсвечиваются еще два минуса:Добавление предиката к каждой таблице в публикации потенциально может замедлить репликацию из-за дополнительной фильтрации WAL и оказать влияние на БД.По прежнему придется делать минимум два полных прохода по таблице.В ходе поиска решений, вспоминается простой подход:Создать рядом новую таблицу с той же структурой;Перелить в новую таблицу данные, на лету фильтруя по app.Применить инкремент, который копился в процессе переливки;В транзакции удалить старую таблицу, а новой таблице дать название старой.Стоит отметить, что вышеописанный подход позволяет решать комплекс задач с таблицами, появляется возможность “крутить, вертеть” их в разных направлениях. Про это “волшебство“ буду рассказывать дальше.Что мы получим для решения поставленной задачи:Благодаря тому, что данные переливаются с применением предиката, за один проход мы получаем таблицу только с нужными данными, которые упорядочены по первичному ключу и без лишних пустот. Аналогично тому, если бы мы делали delete from [table_name] where app_id in (...); + pg_repack, но за один проход.До подмены таблиц все изменения, которые переливаются в новую таблицу, не будут попадать в публикацию. А это значит, что мы не удалим лишнего в хранилище.Кажется, что решение найдено, осталось его заскриптовать. Понимая, что придется писать скрипты для ~1000 таблиц, захотелось написать универсальное решение. Так зародилась утилита pg_rebuild_table.Экономия дискового пространства, RAM и CPU.Ранее упоминалось, что мы используем дорогие сервера с SSD-дисками. Имеем много больших таблиц, которые стали достаточно широкими из-за добавления колонок в порядке появления новых фич. Так вот благодаря эффекту выравнивания данных, порядок столбцов может ощутимо влиять на размер таблицы (идея неплохо раскрывается тут). На больших по размеру таблицах это особенно сильно проявляется. Изменение порядка колонок позволит нам экономить на утилизации CPU, RAM на занимаемом месте на SSD-дисках и, как бонус, повысить производительность.Перед нами стоит задача перестроить пару сотен таблиц, да так чтобы порядок колонок был предельно оптимальным для выравнивания данных. Все свелось к небольшой доработке pg_rebuild_table с использованием уже базового подхода перестроения таблицы. В результате по некоторым таблицам сжатие составило до 20%.Изменение порядка колонок в больших таблицах без простоя в работе.Продолжая тему про порядок колонок, есть еще кейсы, которые закрыла утилита pg_rebuild_table. Например, пришёл начинающий разработчик, который создал таблицу, где поле id оказалось в середине или конце. Со временем таблица разрослась. Разработчик узнал, что для хранения данных это, скорее всего, не оптимально, так же с точки зрения удобства и эстетического удовольствия поле id хочется сделать первым в таблице. Для решения такого рода задач появилась идея закрыть сразу максимум кейсов и задавать любой порядок для всех колонок в утилите pg_rebuild_table и перестраивать таблицу без простоя.Изменение типа данных колонок в большой таблице без простоя в работе.С утилизацией ресурсов немного разобрались, теперь вернемся к задаче, где требуется изменять тип данных первичного ключа на int8. Когда меняется в таблице первичный ключ, то за ним тянется цепочка таблиц, часто больших, где требуется также изменять тип колонки, которая является внешним ключом. Мы уже умеем перестраивать таблицы, менять порядок колонок и все это одной командой. Теперь же, используя базовый подход в утилите pg_rebuild_table, сделали еще одна доработку, которая позволяет создавать колонки с новыми типами. Пока решение позволяет преобразовывать только те типы, которые умеет PostgreSQL из коробки, например int4 в int8, timestamp в timestamptz. В перспективе будет поддержка, аналогичная using при изменении типа данных колонки, для сложных преобразований данных.Перестроение таблицы без долгих транзакций и простоя в работе.Работая с высоконагруженными системами (более 30 тыс. TPS), мы знаем, что долгие транзакции порождают сильное распухание таблиц и, как следствие, получаем деградацию производительности всей базы данных. В нашем случае есть большие таблицы и запуск pg_repack при переливке данных может занять несколько часов. За это время сервисы начнут сильно страдать и пользователи почувствуют задержки в работе сервисов. А запуск vacuum full полностью блокирует таблицу.Для решения такого рода проблемы утилита pg_rebuild_table была доработана в части переливки данных, и появилась опциональная возможность разбивать переливку данных на порции по первичному ключу. Общее время перестройки таблицы будет более длительным, но зато не будет долгих транзакций и негативного влияния на остальные процессы.Ограничения pg_rebuild_tableИспользовать утилиту могут только суперпользователи.Целевая таблица должна иметь ПЕРВИЧНЫЙ КЛЮЧ.Триггер ""z_pg_rebuild_table__delta"" должен быть последним триггером в наборе ""before"".Примеры использования pg_rebuild_tableПерестроить таблицу с данными, удовлетворяющими условию. Переливание данных осуществлять порциями по 100 000 строк.pg_rebuild_table -p 5432 -h /tmp -d database_name --chunk_limit 100000 -T employee -ac 't.app_id in (select app.id from app)'pg_rebuild_table -p 5432 -h /tmp -d database_name --chunk_limit 100000 -T employee -ac 't.group_id in (43597,43789,43791,44229)'Перестроить таблицу данных с автоматическим изменением порядка столбцов для лучшего хранения кортежей данных. Данные переливаются порциями по 100 000 строк.pg_rebuild_table -p 5432 -h /tmp -d database_name --chunk_limit 100000 -T employee --reorder_columnsПерестроить таблицу с новым порядком колонок.pg_rebuild_table -p 5432 -h /tmp -d database_name -T employee --set_column_order id,app_id,first_visit,url,title,site_idПерестроить таблицу с изменением типов данных столбцов «app_id» и «group_id» с «int» на «bigint».pg_rebuild_table -p 5432 -h /tmp -d database_name -T employee --set_data_type '[{""name"":""app_id"", ""type"":""bigint""}, {""name"":""group_id"", ""type"":""bigint""}]'Планы развития pg_rebuild_table:Научить работать с разными версиями PostgreSQL;Научить партицировать таблицу с доступными стратегиями в PostgreSQL.На момент написания статьи утилита pg_rebuild_table уже обработала около 1,5 тыс. таблиц общим объемом ~50 ТБ на базах данных PostgreSQL 12 версии.P.S. Буду очень рад любым предложениям, замечаниям, пожеланиям по данной статье, а также по использованию утилиты pg_rebuild_table.Пишите свои вопросы и комментарии::В телеграмм https://t.me/pg_rebuild_tableНа почту viktor-b_90@inbox.ruСсылка на репозиторий pg_rebuild_table:https://github.com/comagic/pg-rebuild-tableРазработки, в которые подглядывал при написании кода:https://github.com/comagic/transparent_alter_typehttps://github.com/comagic/pg_exportБлагодарностиХочу выразить благодарность коллегам, которые оказывали поддержку при написании статьи, а именно Стороженко Николаю, Пашкову Денису, Чернякову Андрею.Отдельная благодарность Чернякову Андрею за его кодовые наработки, которые использовались при написании утилиты pg_rebuild_table, что значительно ускорило реализацию решения.    ",1.6K,1
Ваш Main() — это бомба замедленного действия: Как MOPP позволит вам избежать катастрофы в вашем коде?,https://habr.com/ru/articles/743436/,2023-06-22T20:35:38.000Z,"MOPP: Main-Only-Programming-PatternВ программировании существует множество различных паттернов, которые помогают упростить и ускорить процесс написания кода. Один из таких паттернов - Main-only programming pattern (MOPP), который предлагает разработчикам поместить весь код в метод Main, который является точкой входа в приложение.Что это такое?MOPP - это паттерн программирования, при котором все функции и переменные помещаются внутрь метода Main. Весь код, необходимый для работы приложения, находится в одном месте и не распределяется по различным классам.""Я не собирался писать все свои функции в Main, но потом я начал печатать, и они просто ускакали на экране. Какой же это был поток сознания! К тому моменту, когда я закончил, у меня было 3000 строк кода и головная боль, но я чувствовал, что это был мой шедевр."" - сказал один из основателей этого паттернаРазрушаем табу: Почему забыть о паттернах программирования может оказаться самым лучшим решением для вашего кода?Итак, давайте рассмотрим преимущества этого замечательного паттерна:Простота: этот подход упрощает код, устраняя необходимость в классах и методах, что может сделать его проще для понимания и изменения.Структура: MOPP способствует лучшей структурированности кода. Весь код сосредоточен в одном месте, что делает его более легким для отслеживания и редактирования.Меньше ошибок: меньше шансов допустить ошибку в коде, потому что всё находится в одном месте. Также это позволяет избежать ошибок, связанных с созданием экземпляров классов и передачей аргументов.Производительность: использование MOPP может увеличить производительность приложения. Код, находящийся в методе Main, компилируется быстрее, поскольку он обрабатывается в первую очередь.Легче сопровождать: благодаря упрощению структуры кода и устранению необходимости в классах и методах, приложение становится более легким для сопровождения и обновленЧудо-код MOPP: как решить самые сложные задачи и добиться максимальной производительности?Давайте рассмотрим явный случай применения паттерна MOPP на примере:public class Main {

  public static void main(String[] args) {
  
    // Declare variables
    int a = 1;
    int b = 2;
    int c = 3;
    int d = 4;
    int e = 5;
    int f = 6;
    int g = 7;
    int h = 8;
    int i = 9;
    int j = 10;
    int k = 11;
    int l = 12;
    int m = 13;
    int n = 14;
    int o = 15;
    int p = 16;
    int q = 17;
    int r = 18;
    int s = 19;
    int t = 20;
    int u = 21;
    int v = 22;
    int w = 23;
    int x = 24;
    int y = 25;
    int z = 26;
    
    // Loop 1
    for (int a1 = 0; a1 < a; a1++) {
    
      // Loop 2
      for (int b1 = 0; b1 < b; b1++) {
      
        // Loop 3
        for (int c1 = 0; c1 < c; c1++) {
        
          // Loop 4
          for (int d1 = 0; d1 < d; d1++) {
          
            // Declare more variables
            int e1 = a1 * b1 + c1 - d1;
            int f1 = e1 + e;
            int g1 = f1 - f;
            int h1 = g1 * g;
            int i1 = h1 / i;
            int j1 = i1 + j;
            int k1 = j1 - k;
            int l1 = k1 * k;
            int m1 = l1 / l;
            int n1 = m1 + n;
            int o1 = n1 - o;
            int p1 = o1 * o;
            int q1 = p1 / p;
            int r1 = q1 + r;
            int s1 = r1 - s;
            int t1 = s1 * s;
            int u1 = t1 / t;
            int v1 = u1 + v;
            int w1 = v1 - w;
            int x1 = w1 * w;
            int y1 = x1 / x;
            int z1 = y1 + z;
            
            // Print result
            System.out.println(""Result: "" + z1);
            
          }
        }
      }
    }
  }
}
Как я упоминал ранее, в данном коде использован паттерн MOPP, который позволяет легко и быстро реализовывать многопоточность и синхронизацию данных. Это значительно повышает производительность и позволяет более эффективно использовать ресурсы компьютера.Также использование переменных с понятными именами делает код более читабельным и понятным для других разработчиков. Это упрощает поддержку и развитие кода в будущем.Использование циклов в цикле и большого количества переменных позволяет выполнять сложные алгоритмы и обрабатывать большие объемы данных. Это делает код гибким и масштабируемым, что позволяет его легко адаптировать под различные задачи и требования.Таким образом, использование MOPP в данном коде позволяет с легкостью решать сложные задачи, эффективно использовать ресурсы компьютера и обеспечивает высокую производительность.Использовать или не использовать MOPP: решайте сами, но мы знаем, что выберут настоящие профессионалы!Конечно, все представленное в статье было сарказмом и шуткой, созданными для юмористического эффекта. В реальности использование паттерна MOPP, описанного в статье, приведет к катастрофическим последствиям и нарушению всех принципов хорошего программирования.Но все же, я надеюсь, что вы получили удовольствие от чтения этой статьи и нашли в ней юмористическую составляющую. Помните, что программирование - это серьезное дело, и хороший код должен соответствовать лучшим практикам и принципам, чтобы обеспечить надежную и эффективную работу программного продукта.  ",3.2K,11
Встречайте Dev Mode в Figma,https://habr.com/ru/articles/743428/,2023-06-22T18:35:23.000Z,"Эта статья — перевод оригинальной статьи ""Making Figma better for developers with Dev Mode"".Также я веду телеграм канал “Frontend по-флотски”, где рассказываю про интересные вещи из мира разработки интерфейсов.ВступлениеКак инструмент проектирования может работать лучше для разработчиков? Этот вопрос мы задаем себе и нашему сообществу. Сегодня мы рады представить Dev Mode - новое рабочее пространство в Figma, созданное для того, чтобы разработчики получали то, что им нужно, когда им это нужно, используя инструменты, которые они используют каждый день.Figma родилась в вебе - нетрадиционное начало для инструмента проектирования, но мы чувствовали, что он крайне необходим дизайну продуктов. С помощью одной единственной ссылки дизайнеры могли сотрудничать над текущей работой, обмениваться ранними наработками, а не беречь дизайн до тех пор, пока он не будет ""отшлифован"". По мере того, как все больше дизайнеров осваивали Figma и этот многопользовательский способ работы, мы начали наблюдать естественное расширение использования в различных командах и дисциплинах, особенно среди разработчиков.Сегодня мы знаем, что на наших платных тарифных планах Figma посещают больше разработчиков, чем дизайнеров. Мы также знаем, что понимание потребностей разработчиков и их проблем имеет решающее значение для превращения Figma в место, где разработка продукта может быть более эффективной, совместной и выразительной.Разработчики имеют уникальные рабочие процессы и предпочтения. От front-end разработчиков, работающих со зрелыми системами дизайна, до инженеров, создающих компоненты систем дизайна, до тех, кто создает макеты контента и экспортирует ресурсы в своей работе с бренд-дизайнерами, - каждая команда хочет работать с минимальными ограничениями, насколько это возможно.В режиме Dev Mode мы видим огромную возможность быстро и эффективно предоставлять разработчикам то, что им нужно - точно так же, как это было с дизайнерами, когда мы только приступили к созданию Figma. Чем проще командам проектировать, документировать, находить и реализовывать высокоточные проекты, не теряя при этом из виду работу и друг друга, тем лучше результат продукта. Мы рады сделать этот первый шаг к объединению проектирования и разработки в Figma, и нам не терпится увидеть, что команды будут делать дальше.Быстрее приступайте к кодингуХотя Figma отлично подходит для свободного исследования дизайна, она может сбить с толку, если вы попали в файл дизайна, в котором отсутствует информация, необходимая для реализации. Режим Dev Mode - это как инспектор браузера для вашего файла дизайна, он приближает концепции дизайна - формы, слои и группы - к концепциям разработчика, таким как код, иконки и маркеры. Наведя курсор и щелкнув по холсту Figma, вы можете найти и экспортировать всю необходимую информацию, такую как измерения, спецификации и активы, а также раскрыть дополнительный контекст вашей системы проектирования. Подобно Chrome Dev Tools, Dev Mode черпает вдохновение из других инструментов разработки, чтобы создать среду, которая сразу же станет для вас привычной.Код в режиме Dev Mode полностью переработан и настраивается под тот язык, на котором вы работаете. Мы знаем, что код не является полезным из коробки. Скорее, он является отправной точкой, чтобы вам не приходилось каждый раз переходить от 0 к 1. Теперь вы увидите боксовую модель CSS, современный синтаксис с древовидным представлением, а также сможете переключаться между единицами измерения, чтобы соответствовать вашей кодовой базе.Доступ ко всему, что вам нужно, в одном местеСоздание продуктов требует надежного набора инструментов, но прыжки между библиотеками дизайна, базой кода и другими инструментами управления проектами могут привести к неэффективности, особенно когда названия компонентов и стилей не совпадают с названиями в коде, или команда не отслеживает и не документирует задачи. Dev Mode призван сделать вашу работу более продуктивной, связав используемые вами инструменты и компоненты кода с файлом дизайна.Плагины позволяют расширить функциональность Figma, чтобы адаптировать ее к тому, как работает ваша команда. Вы можете управлять проектами с помощью Jira, Linear и GitHub, чтобы вы и ваш дизайнер знали, что происходит в ваших соответствующих процессах. Storybook поможет вам сослаться на то, что происходит в вашей кодовой базе, в контексте самого дизайна. А плагины codegen от AWS Amplify Studio, Google Relay и Anima помогут вам настроить вывод кода - вы даже можете создать свой собственный на основе вашего уникального рабочего процесса.Очень полезно иметь плагины, которые взаимодействуют с нашими повседневными инструментами. Мы используем GitHub, мы используем Storybook - это экономит мне много времени.Лоран Тьебо, руководитель инженерного отдела и ведущий специалист по системам проектирования, Decathlon (часть бета-версии Dev Mode)Системы проектирования становятся более мощными с введением токенов проектирования через переменные. Токены - это маленькие кусочки данных пользовательского интерфейса, которые можно использовать в дизайне и коде. Теперь они отображаются в режиме Dev Mode, так что сразу становится ясно, что нужно для начала разработки. Вы также можете добавить связанные ссылки на объекты на холсте, чтобы ссылаться на документацию или на то, что находится в ваших плагинах.Отслеживайте, что должно быть отправлено в разработкуДаже если этапы проектирования и разработки продукта сливаются воедино, артефакты каждой из них - файлы дизайна и код - остаются разными. До сих пор было непросто перемещаться по файлам дизайна, выбирать конкретные компоненты и их свойства или даже знать, что изменилось с момента последнего просмотра файла. Теперь дизайнеры могут просто пометить раздел как ""готовый к разработке"" и отправить его вам напрямую, не создавая отдельную страницу или файл. Поддержка Diff позволяет сравнивать изменения между различными версиями фрейма и всегда оставаться в курсе событий.Расширьте свой рабочий процессС помощью расширения VS Code вы можете использовать возможности режима Dev Mode в редакторе кода для просмотра дизайна, уведомлений и комментариев, а также для отслеживания изменений, не выходя из среды разработки. Расширение VS Code также выполняет автокомлит стро кода на основе просматриваемого дизайна, помогая вам работать намного быстрее.Dev Mode и Figma для VS Code находятся в бета-версии и бесплатны для всех пользователей до конца 2023 года. Начиная с 2024 года, для доступа к Dev Mode вам потребуется платный тарифный план. Если вы являетесь редактором на платном тарифном плане сегодня, Dev Mode будет включен. Мы знаем, что есть разработчики, которым может не понадобиться полный набор функций Figma, поэтому мы вводим два новых варианта тарифного плана для них.Гораздо выше уровень доверия, когда люди работают с одним и тем же инструментом - информация гораздо более актуальна, вы больше не тянете какие-то файлы на свой компьютер и не гоняетесь за чем-то по электронной почте. Это гораздо более совместный процесс.Джори Лалло, соучредитель, LinearЭто только первый шаг в улучшении Figma для разработчиков. С учетом вашего вклада в бета-версию Dev Mode и VS Code мы с нетерпением ждем расширения функциональности, включая новые способы улучшения сотрудничества дизайнера и разработчика, извлечения спецификаций и обеспечения большего соответствия между дизайном и кодом.  ",2K,0
Платформа OpenTitan от Google для заслуживающих доверия чипов: возможности проекта,https://habr.com/ru/articles/743426/,2023-06-22T18:25:17.000Z,"Авторы проекта платформы OpenTitan, которая предназначена для создания заслуживающих доверия аппаратных компонентов (RoT, Root of Trust), заявили о завершении важной стадии, этапа RTL-проектирования. На данный момент логический дизайн платформы признан пригодным для создания физического чипа и объявлен замороженным от внесения дальнейших изменений. Работа перешла на стадию логического синтеза, на котором выполняется преобразование высокоуровневого RTL в низкоуровневое описание на уровне логических элементов и проводников. Доступные для тестирования рабочие прототипы чипов планируют начать выпускать к концу 2023 года. 

Что это за платформа? 
Ранее команда проекта рассказывала о том, что OpenTitan — платформа для создания заслуживающих доверия аппаратных компонентов (RoT, Root of Trust), которая может применяться там, где нужно гарантировать целостное состояние аппаратных и программных элементов системы, удостоверяя, что критически важные части системы не были подменены и основываются на проверенном и авторизированном производителем коде. Проект предоставляет готовый, проверенный и надёжный каркас, позволяющий повысить доверия к создаваемым решениям и снизить издержки при разработке специализированных чипов для обеспечения безопасности. 

Чипы, которые разрабатываются при помощи этого продукта, могут использоваться в серверных материнских платах, сетевых картах, потребительских устройствах, маршрутизаторах, устройствах интернета вещей для верификации прошивок и загружаемых компонентов, а также для генерации криптографически уникальных идентификаторов системы (защита от подмены оборудования), защиты криптографических ключей (изоляция ключей в случае получения злоумышленником физического доступа к оборудованию), предоставления связанных с безопасностью сервисов и ведения изолированного лога аудита, который невозможно отредактировать или стереть. 

Как это работает? 


Принцип работы платформы — использование логических блоков. Они востребованы в RoT-чипах, такие как открытый микропроцессор на базе архитектуры RISC-V (RV32IMCB Ibex), криптографические сопроцессоры, аппаратный генератор случайных чисел, менеджер ключей с поддержкой DICE, механизм защищённого хранения данных в постоянной и оперативной памяти, технологии защиты, блоки ввода/вывода, средства безопасной загрузки и т.п. Устройство также предоставляет блоки с реализацией типовых алгоритмов шифрования, таких как AES и HMAC-SHA256, и ускоритель математических операций, применяемых в алгоритмах для работы с цифровыми подписями на базе открытых ключей, таких как RSA и алгоритмы на базе эллиптических кривых. 

Платформа базируется на уже созданных ранее надежных решениях. Например, это технологии, уже используемые в криптографических USB-токенах Google Titan и TPM-чипах для обеспечения верифицированной загрузки, устанавливаемых на серверах в инфраструктуре Google, а также на устройствах Chromebook и Pixel. При этом, в отличие от существующих реализаций Root of Trust, OpenTitan развивается в соответствии с концепцией «безопасность через прозрачность», подразумевающей доступность кода и схем, а также полностью открытый процесс разработки, не привязанный к конкретным поставщикам и производителям чипов. 

OpenTitan позиционируется как независимый совместный проект и развивается под эгидой некоммерческой организации lowRISC, курирующей разработку свободного микропроцессора на базе архитектуры RISC-V. Кроме Google и lowRISC в разработке участвуют компании Western Digital, Seagate, Nuvoton Technology, Winbond, Rivos, zeroRISC и G+D Mobile Security, а также Высшая техническая школа Цюриха. Связанный с проектом код и спецификации аппаратных компонентов публикуются под лицензией Apache 2.0.  ",574,0
Умный дом с голосовым ассистентом на минималках,https://habr.com/ru/articles/743424/,2023-06-22T18:20:07.000Z,"Небольшой лонгрид с предысторией и планами, чтобы лучше понимать почему, что и как получилось.Разработка шла с переменной скоростью в течении лет 5. Совсем не сразу взялся за летопись, поэтому прошу прощения, если что-то будет некорректно ;)В результате имеем масштабируемое решение, которое можно использовать в любом месте (квартира, дом или еще что):Raspberry PI4 + POEдисплей 	для отображения статусаГолосовой ассистент работающий как онлайн, так  оффлайн с поддержкой chatGPT по сложным вопросамдатчики:температурывлажностичастицуглекислого газаполиваосвещенностиустройства управления:вентиляторамирозеткойполивомИК управление устройствамиWOLusb камераweb сервер для отображения статусов, графиковtelegram бот для отображения графиков, статуса и управления всемГлава 1. Начало      Из за того, что не поливал цветы дома они часто высыхали. Для того, чтобы это исправить был закуплен в 2018 raspberry PI3 и решил было организовать автоматический полив с размахом и не ограничиваясь бюджетом с учетом того, что это должно было вырасти в центр умного дома в дальнейшем. Реализовал простой полив, наблюдение за растениями по камере и заодно приделал датчик температуры (DHT11), и управление вентилятором чтобы можно было усиливать обогрев комнаты если в ней холодно продувом через батарею теплого воздуха.Запас воды хранился в 5-ти литровой бадейке купленной в Ашане и выкачивался по шлангу с помощью моторчика в растение.  5V моторчик, время жизни 6-8 месяцевВначале пробовал самые простые и дешевые моторчики, но спустя 6-12 месяцев из за того, что вода жесткая они выходили из строя12V моторчик, время жизни более 1-2 года, пока не сломалсяв результате перешел на 12V, последние 2 года с ним проблем нет, при этом он достаточно мощный, чтобы поднимать воду на значительную высоту. Экономия на корпусе - просто банка ;)Вначале для управления периферией использовал обычные реле, но их щелканье, особенно когда в комнате тихо не особенно комфортно. Поэтому решил не экономить и для управления разно-вольтовой периферией купить твердотельное реле. Управлять через GPIO с raspberry периферией напрямую утопия конечно, особенно если расстояние от борды 1-2 метра.График температуры и влажности, датчик влажности барахлит время от времениАппетит как известно приходит во время еды, раз есть железка, то можно ее улучшать ! (самое главное, чтобы было время ;) ) 	Логично приделать базу данных, чтобы хранить информацию о поливах и отслеживать    что и когда произошло, а раз копится информация о температуре и влажности, то ее хорошо бы отображать  визуально через web интерфейс (прикрутил flask для этого). Первая версия бота Дальше возник вопрос, что делать например если ты на работе, а есть желание посмотреть, что происходит дома, включить или выключить устройства. Как решение могло выглядеть Android приложение через прокси имеющее доступ к внутренней сети (белого айпишника не было), но более простым и быстрым решением выглядел telegram бот.Глава 2. ДатчикиИнтерфейс telegram ботаСледующими интересными датчиками стали датчик CO2 (MH-Z19) и уровня частиц (PMS7003).CO2 — чтобы проветривать комнату вовремя, частицы, чтобы отслеживать загрязнение воздуха. Так как дома не постоянно, то хорошо бы это отслеживать и через telegram бота Пример меню бота с предупреждениемИ естественно надо предупреждать, если забываешь открыть окно или что-то произошло нехорошее (тут уже напоминания через почту и бота), например если температура выше 40 градусов в комнате.Плата с датчиками    Так как датчиков стало много, то стало интересно их вынести на внешнюю плату и опрашивать.    Для большей стабильности по питанию добавил конденсатор, иначе при длине кабеля в 1-2 метра между платой и raspberry были проблемы и сенсоры DHT11/22, CO2 работали нестабильно.    Также теперь на один UART стало 2 устройства, значит нужно приделать переключение между ними, что реализовал с помощью cd74hc4052 и по GPIO можно переключать между устройствами итого получается можно подключить уже не одно а четыре устройства на UART. Raspberry PI4 + RJ45 подключение датчиков + POE адаптерНа этом фото POE адаптер без 12 В, в дальнейшем был заменен на адаптер выдающий также 12В.    Со стороны Raspberry - RJ45 разьем на GPIO. RJ45, значит можно использовать обычный сетевой кабель для подключения, что довольно удобно.    Изначально был Raspberry PI3, и все висело на UART0, для этого пришлось его отключить от внутренних нужд raspberry через  : enable_uart=1 dtoverlay=pi3-miniuart-bt core_freq=250    Из payback — устройство перестало перезагружаться софтверно, приходилось перевключать питание, что не очень удобно.     С переходом на Raspberry PI4 появилась возможность перейти на uart3, что более комфортно и проблемы с софтверной перезагрузкой ушли. Также PI4 значительно мощнее 3 и у него больше памяти, что положительно сказалось при работе с TTS и STT, но об этом позже.Глава 3. ДисплейДисплей с минимальной информацией и графиками    Для любого устройства всегда интересно графическое отображение информации, так как появилась возможность использовать несколько UART устройств, то логично было подключить дисплей на UART, был выбран 3.2 inch Nextion дисплей, он позволяет настраивать template для отображения и затем передавать одиночные данные, так как картинка планировалась по большей части статичной не хотелось подключать дисплей по HDMI и гонять на него данные, также это позволяло интегрировать дисплей с платой расширения. Глава 4. Расширение датчиков и устройств    Как всегда хочется больше и лучше. Начал добавлять датчики и устрйоства. Не буду захламлять рассказ фото стандартных решений с aliexpress, при желании их легко можно найти.DHT11 датчик температуры — влажности довольно не точный и был заменен на DHT22, рекомендую сразу ставить егоПоливать 	растения надо не только по расписанию 	но и по влажности почвы, значит нужен 	датчик влажности. Датчик прожил где-то месяцев 3-6, после чего весь окислился, и от него отказался, не рекомендую.Датчик присутствия воды в шланге поливаПростой провод в шланге для обнаружения воды, как это выглядитНе раз сталкивался с тем, что моторчик выходит из строя и полив не идет. Если тебя нет дома долго, то это критично  вначале  добавил датчик капель под шланг с aliexpress. Но он окислялся и долго не работал, в результате довольно рабочее решение, несмотря на окисление - решение повесить на GPIO провода и засунуть его в шланг для полива, второй провод это земля и в момент полива мониторить состояниеИнициализация в кодеДатчик уровня воды, пробовал с aliexpress  также окисляется и толку от него не многоДатчик освещенности — самый простой, работает до сих пор, используется, чтобы решить в какое время можно озвучивать предупреждения а в какое лучше промолчать, так как ночьТакже для управления внешними устройствами 220В добавил выносную релюшку в отдельном корпусе c розеткойДля того, чтобы не плодить провода и зарядки перевел питание всей системы как 5В так и 12В на POE адаптер с выходом на 5 и 12 В. Удачно он работает как на Rasbperry PI3 так и на PI4IR управление, естественно как же без него, в доме как минимум 2 устройства телевизор и кондиционер использовали IR. Довольно много времени потратил на исследование этой темы, в процессе исследования сигналов IR пригодился DSLogic анализатор, но тема была осилена и кондиционер сдался и перешел на управление от raspberry    В результате можно было включать кондиционер например по дороге домой и приезжать в комфортное условия.WOL: так как был доступ к локальной домашней сети и управление через telegram бота, иногда надо было удаленно подключаться к заснувшим компьютерам в сети, логично и несложно было добавлено управление через ботаГлава 5. РефакторингПосле добавления кучи устройств и сенсоров стало понятно, чтобы поддерживать это все в рабочем виде надо написать универсальный подход. Все сенсоры, устройства организовал в группы и добавил контроллеры, которые могут их объединять и обслуживать.Также шину данных в приложении, так как например температура могла получаться от DHT11/22 но если их не было, то грубое значение температуры можно было получить из CO2 сенсора также.Упорядочил работу с UART так как он был нужен во многих местах. Глава 6. Java 	В работе часто использовал Java, она казалось более стабильной чем python, так как все синтаксические ошибки видны были сразу на этапе компиляции к тому же она более подходила для сложного проекта, как мне виделось. Обновил железо на Rasbperry pi 3 B+, у него было 1 Гб оперативной памяти вместо 0.5 Гб, так как Java требовала больше памяти. Начал переписывать проект на Java, реализовал телеграм бота, часть сенсоров, но из за нехватки времени это направление остановилось, на питоне гораздо быстрее все набросать и запустить.В итоге Java:Python 0:1 ;)Глава 7. Голосовое управление 	Довольно удобно, когда например лежишь и хочется что-то включить или просто голосом мимоходом узнать что-то. Сначала попробовал бесплатные оффлайн пакетыsphinx — качество ужасное, ничего не работаетsopare — также не устроило качество Потом попробовал онлайн варианты реализаций:Google (speech_recognition) — вполне работает, есть дефолтный бесплатный токен, но на нем ограниченно количество запросовYandex (speechkit) — работает, но просит денегЕстественно так как это не коммерческий продукт, платить не собирался, поэтому выбрал Google. Длительное время с ним жил, не очень комфортно, так как из-за ограничения на количество запросов, постоянно отваливалось и не мог в результате полагаться на устройство. Пока не подсказали о оффлайн библиотеке :Vosk (vosk) - попробовал и о чудо все работает без интернета, без платежей на вполне 	приемлемом качестве и без серьезных задержек. Этот факт уже ставит разрабатываемое устройство интереснее колонок, так как оно уже не зависит от интернета. При этом эта библиотека поддерживает 	русский язык, в результате остановился на ней.Пробовал большую модель загрузить, если у Raspberry 8 Гб, она даже может заработать (правда хорошо при этом еще задействовать zram), но медленность реакции ставит крест на этом использовании, в результате остановился на малой модели.Из минусов стоит еще отметить, что хотя нормально распознает голос взрослого, ребенка не смогла услышать.    Надо упомянуть немного о hardware, пробовал веб камеры и различные микрофоны для голосового распознавания, при этом были проблемы с очищением голоса, когда само устройство говорит и само себя слушает, также окружающие шумы сильно затрудняли распознавание, после нескольких экспериментов остановился на Jabra 410, качество как микрофона так и воспроизведения были на высоте, при этом были минимальные помехи, когда одновременно идет распознавание и генерация голоса. Из минусов, если включить музыку или радио, распознавание голоса не работает. Вычитания воспроизводимого из микрофона либо недостаточно либо его вообще нет. Глава 8. Голосовое воспроизведение 	Естественно, хорошо, когда на голосовое управление есть тот же голосовой ответ и ты знаешь как устройство услышало твою комманду. Попробовал несколько библиотек: espeak пакет в linux — качество мягко говоря не очень sphinx (pyttsx3) — также проблемы с качеством Yandex (yandex_speech) — хорошее качество но хотят денег Google (gtts) — хорошее качество, хотят денег, но есть дефолтный бесплатный ключ, 	правда если много запросов по нему 	делать начинаются задержки. Также длительное время сидел на Google, пока не познакомился с Vosk, после чего начал поиски похожей библиотеки для генерации голоса, в результате нашел: Silero — оффлайн, базируется на torch. К этому времени сменил hardware с Raspberry PI3 на PI4 64 bit, что позволило поставить и использовать torch без проблем. Warning, если ставить через wheel, то ставится torch 2.0 и с ним не работает, 	поэтому надо ставить 1.13:версии которые заработаили на raspberry PI4Но минус Silero в задержках, простая фраза генерируется 10+ секунд, в то время как Google TTS выдает результат за 1-2 секунды, что существенно. Поэтому Silero остается как fail safe вариант, если Google TTS не заработает или не будет интернета. Из минусов можно еще отнести то, что Silero не понимает например числа из коробки и для того, чтобы оно его произнесло надо сначала число преобразовать в текст, из плюсов — можно поиграться в этим процессом и открыть для себя что-то новое ;)Глава 9. Улучшение голосового управлениянормализация звука на С с использованием CPythonВ принципе Jabra достаточно хорошо вычищает звук, но хотелось лучшего, для этого добавил нормализацию звука, чтобы быстрее работало, сделал через C на python, чтобы быстрее работало. Этого показалось мало, поэтому добавил еще простой шумодав noisereduce.По итогу noisereduce помогает,  удалось добавить примерно 1 метр к дальности распознавания, а если включить нормализацию то видимо идут искажения и качество падает. В результате отказался пока что от нормализации, может вернусь к этому после того, как сделаю разделение на фразы, можно будет приделать шумодав из ffmpeg также.Заключение 	Тема умного дома интересна наверное для многих, знаю многих людей, кто в том или ином виде его реализовал, причем подходы сильно отличаются в зависимости от skills. Умные ассистенты интегрированные в умный дом могут и должны развиваться, то что мы сейчас видим только начало пути как мне кажется.     Репозиторий проекта для ознакомления и welcome to принять участие в проекте и улучшать его: https://github.com/AMV007/raspberry_house_control Планы Если найду время конечно, идей pet проектов всегда много ;) перевести на плату расширения на zigbee добавить zigbee сенсоры и устройства управления добавить поплавковый датчик уровня воды датчик движения (после освоения zigbee) добавить вычитание воспроизводимого из микрофона, чтобы можно было бы 	управлять устройством во время воспроизведения радио или музыкидобавить распознавание фраз, чтобы приделать шумоподавление и более эффективную нормализациюдобавить возможность воспроизведения с локального NAS как видео так и музыки (то 	чего очень не хватает в Алисе)добавить для всего корпус на 3D принтере  ",2.7K,5
Зачем бизнесу нужен продакт-менеджмент?,https://habr.com/ru/articles/743420/,2023-06-22T16:39:37.000Z,"Цель данной статьи дать понимание для бизнеса какие зоны ответственности попадают в роль менеджера по продукту, как это пересекается между стандартным бизнес подходом и нужен ли бизнесу подход по управлению продуктом, а вмести с этим выделенная организационная функция. Что такое продуктУспех бизнеса зачастую трактуется базовым экономическим правилом спроса и предложения. Для этого требуется провести исследования рынка, оценить поведение целевого потребителя с целью выявления потенциальной потребности. И уже для этой потребности сформировать предложение как услугу или продукт. Давайте разберемся, что такое продукт и может ли услуга быть продуктом. Под продуктом понимается какое-либо предложение для удовлетворения определенной потребности или решения проблемы клиента (потребителя). Если по-простому, продукт может быть физическим (например, телефон) или цифровым (например, приложение в телефоне). Любой продукт имеет упаковку и дистрибуцию, вне зависимости от того, физический он или цифровой. С другой стороны, услуга представляет собой действие или задачу, выполняемые одной стороной для другой. Простыми примерами услуг являются консалтинг, транспорт, обслуживание в банке, мастер на дому. Однако, современном мире граница между продуктом и услугой начинает размываться. Дело в том, что услуга или набор услуг могут быть упакованы в продукт. Несколько примеров, чтобы уложились понятия:Стриминг-сервис, предлагает каталог контента и это платформа из продуктов, но предлагающая услугу прослушивания/просмотра по подпискеБанковское приложение, это чисто цифровой продукт, предоставляющий услуги банка. Любой продукт имеет упаковку и дистрибуцию, вне зависимости от того, физический он или цифровой, или это услуга.Почему продуктом нужно управлятьЕсли не управлять продуктом, он может быстро перестать быть актуальным или начать развиваться не в том направлении, будут возникать трения между функциями бизнеса. Это несет высокие риски для компании и может негативно сказаться на конечной прибыли или вообще поставить бизнес под угрозу закрытия.Продакт-менеджмент появился в качестве самостоятельной организационной функции в 1930-х годах в компании Procter & Gamble. Один из маркетологов компании Нил Макэлрой предположил, что анализ эффективности продаж должен включать, помимо оценки маркетинговых инструментов, изучение обратной связи от дилеров и покупателей. Такой подход к ведению бизнеса оказался очень выгодным. В последующих десятилетиях продакт-менеджмент прошел значительный путь развития и подтверждения ценности подхода, даже видоизменил процессы в компаниях. В настоящий день применение принципов продакт-менеджмента стало частью бизнес процесса успешных компаний.Несколько основных направлений, где присутствует управление продуктом:Стратегия и планирование. Эффективное управление продуктами или услугами обеспечивает соответствие общим стратегическим целям и задачам компании. Оно помогает определить видение, целевой рынок и ценностное предложение, определяя направление и позиционирование предложения на рынке.Исследование рынка и трендов, в том числе выявление потребностей клиентов для формирования стратегии развития. Такое исследование включает в себя конкурентный анализ, чтобы адаптировать стратегию, а также позиционировать ценностные предложения как превосходящие альтернативы на рынке.Разработка и улучшение самого продукта или услуги от концепции до запуска. Правильная приоритезация итераций продукта является крайне важным компонентом управления. Ценность продукта наращивается путем непрерывного улучшения. Стратегия выхода на рынок это не только маркетинг и привлечение трафика и оценка стоимости привлечения, но и то как продукт воспринимается целевой аудиторией с момента первого знакомства. Определение ценообразование продукта, которое включает в себя стоимость его создания и поддержания предложения, а также стоимость привлечения, что в конечном счете формирует спрос, если удается донести ценность для целевой аудитории. Обеспечение продаж и внутренней вовлеченности в продукт среди зависимых департаментов компании включает создание общей среды со всеми необходимыми материалами о продукте, в том числе и возможностями внутреннего администрирования продукта. Любая компания предоставляющая рынку продукт или услугу имеет элементы продакт менеджмента. Но не все компании имеют выделенную организационную функцию продукта менеджмента. Это связано с тем, что функция является размытой среди нескольких отдельных направлений, в то время как менеджмент компании консолидирует информацию, принимает решение и делает шаги для следующего действия.Эффективность такого подхода зависит от типа и размера бизнеса, а также от вовлеченности и глубины погружения руководства. По мере развития компании для поддержания эффективности и скорости развития делегирование и консолидация направлений по продуктам становятся неизбежны. Появляется необходимость функции продакт-менеджмента.Чем занимается продакт-менеджерВ связи с тем, что в зоне ответственности продакт-менеджера лежит непрерывный сбор требований и приоритезация, анализ, контроль реализации, кросс-функциональная координация, очень часто эту роль путают с ролью бизнес-аналитика и проджект-менеджера. В то время как бизнес-аналитика и проджект-менеджмент являются вспомогательными функциями, но не исключающими объединения в одной роли в зависимости от размера или других особенностей организации. С целью упрощения в данном контексте не рассматривается полная структура функции управления продуктом, однако отметим различия:бизнес-аналитик фокусируется на потребностях бизнеса и процессах, анализе данных бизнеса для принятия решений;проджект-менеджер фокусируется на планировании и исполнении проектов в установленные сроки, управлении риском, качеством и стоимостью проекта;продакт-менеджер фокусируется на стратегии развития продукта, определении целевой аудитории и реализации продукта с учетом ее потребностей, несет ответственность за измеряемую ценность продукта в рамках стратегии компании. Роль продакт-менеджера, как может быть понятно из названия, заключается в сквозном управлении продуктом и, конечно, в фокусировании на успехе продукта в рамках стратегии компании. Чтобы добиться положительной динамики развития продукта, нужно определить его стратегию на основе анализа трендов и выявления потребностей целевой аудитории, держать под контролем приоритизацию процесса разработки и(или) реализации и выступать связующим звеном в кросс-функциональных командах для вывода продукта на рынок.Весь процесс управления продуктом можно уложить в достаточно простой цикл — итерации развития продукта:Итеративность продуктового процесса на основе PDCAОпределение — постановка цели в рамках развития продукта на основе выявленных потребностей целевой аудитории;Разработка — для реализации цели создается компонент или итеративная версия продукта совместно с командой разработки;Запуск — открытие созданного компонента и (или) функциональности для целевой аудитории. Управление продуктом в этой фазе происходит совместно с маркетингом, отделом продаж, командой поддержки продукта;Анализ — запуск не означает успешность достижения цели. Необходимо проанализировать, насколько хорошо реализация компонента или функциональности продукта адресует потребность аудитории в рамках этой итерации. Данный процесс в действительности может иметь несколько степеней вложенности и относится как к разработке компонента/функционала, так и к стратегии и тактического роудмапа.Для того того, чтобы успешно и эффективно управлять продуктом, в область работы продакт-менеджера попадают более объемные блоки, которые являются неотъемлемой частью процесса:Стратегия и видение продукта. Это отправная точка в работе менеджера по продукту. Видение продукта — емкое целевое определение, зачем нужен продукт, для кого он и какую проблему решает. Стратегия продукта — формирование реперных точек (целей), как из текущего состояния прийти к видению продукта. В итоге определяется роудмап с целью создания или улучшения продукта.Анализ рынка. Менеджер по продукту отслеживает тенденции рынка, конкурентное положение и динамику отрасли. Собирает информацию о поведении клиентов, их предпочтениях и проблемах, чтобы принимать обоснованные решения по разработке продукта и выявлять рыночные возможности. Постоянно следит за развитием отрасли, чтобы обеспечить конкурентоспособность продукта. Одна из целей этой области работы заключается в формировании оценки потенциального рынка для продукта и определении возможностей для роста.Сбор, анализ и категоризация требований по продукту. Менеджер по продукту собирает и определяет приоритеты клиентских требований, взаимодействуя с клиентами, проводя пользовательские исследования и анализируя обратную связь. Определяет и документирует требования к продукту, пользовательские сценарии и кейсы использования, чтобы направлять процесс разработки. Обеспечивает, чтобы продукт удовлетворял потребности клиентов и предоставлял ценность. Цель — снизить уровень неопределенности в предположении о том, как продукт может получить объем рынка.Роудмап и приоритезация. Формирование и поддержание роудмапа продукта, который описывает запланированный функционал, цели, релизы и сроки. Роудмап должен быть объективно приоритезирован на основе потребностей целевых клиентов, бизнес-целей и технической осуществимости. Продакт менеджеру приходится принимать решение о выборе приоритета и находить баланс между конкурирующими приоритетами, чтобы максимизировать ценность продукта.Разработка продукта. Создание и реализация функционала продукта на основе приватизированного роудмапа через работу с командами разработки, дизайна, тестирования. Требуется тесная коллаборация для контроля доставки выявленной ценности для клиентов в приемлемое время и в достаточном качестве. Активно применяются методики Agile, чтобы исключить простои в ресурсах и сократить время реализации через небольшие итерации. Продакт-менеджер обеспечивает эффективное взаимодействие и сотрудничество между командами, чтобы достичь общих целей и успешно внедрить продукт.Управление ожиданиями и зависимостями. Менеджер по продукту поддерживает коммуникацию с заинтересованными сторонами, включая руководство компании, клиентов, команды продаж и маркетинга. Он собирает обратную связь, управляет ожиданиями и предоставляет обновления о прогрессе продукта. Сотрудничает с заинтересованными сторонами, чтобы обеспечить соответствие и поддержку целей продукта.Разработка стратегии ценообразования. Продакт-менеджер анализирует конкурентные цены, изучает структуру затрат и определяет оптимальную стратегию ценообразования. Он учитывает потребности рынка, ценовую эластичность и прибыльные показатели, чтобы установить цены, которые максимизируют доходы и удовлетворяют клиентов.Запуск продукта/релиза и маркетинг. Менеджер по продукту играет ключевую роль в запуске продукта. Он работает с командами маркетинга и продаж для разработки стратегий выхода на рынок, позиционирования, коммуникации и ценообразования. Координирует активности по запуску, включая демонстрации продукта, поддержку продаж и маркетинговые кампании, чтобы обеспечить принятие продукта и его успех.Метрики продукта и итерации. Менеджер по продукту непрерывно отслеживает производительность продукта. Он собирают данные, анализирует метрики и запрашивает отзывы пользователей, чтобы оценить успех продукта и выявить области для улучшения. Принимает решения на основе данных и итерирует продукт, чтобы улучшить его ценность и удовлетворить потребности клиентов, а также с целью проведения финансовой и технической оптимизации.В заключение можно сказать, что ориентация на потребителя, умение решить его проблему и знание рынка позволяют продакт-менеджеру (а следовательно, и компании) регулярно выводить на рынок новые востребованные продукты или новые версии уже существующих на нем, контролировать изменения продукта на основе спроса или трендов рынка. Менеджер по продукту является драйвером для постоянного развития и роста продукта.  ",879,0
Как мы сделали справочник о диджитале для старших,https://habr.com/ru/companies/sibur_official/articles/743414/,2023-06-22T16:11:10.000Z,"Наверняка у многих были ситуации (не одна и не две), когда старший родственник просит помочь с этим вашим компьютером. А то и смартфоном. Поэтому мы решили помочь, с одной стороны, родителям, бабушкам и дедушкам, а с другой стороны — айтишникам, которые (со всей любовью к родным) задумывались об автоматизации таких советов.Так мы разработали справочник по технологиям и трендам современного мира. В нём рассказывается и про популярные IT-сервисы, и про современные профессии, и даже про кибербезопасность. Просто вручаешь справочник —  и не приходится объяснять, чем дата-аналитик отличается от фронтенда, и где вообще эта кнопка ""Пуск"".Причём темы мы брали не с потолка, а по результатам многих фокус-групп с добровольцами старше 55 лет. И вдобавок собрали истории от более молодых добровольцев о том, как они помогали старшим родственникам с технологиями. Тем, кто поделился с нами ценными инсайтами или особенно интересными историями, мы, конечно, подарили экземпляры таких справочников. Теперь вот хотим поделиться с вами.Конкурс!Для участия в розыгрыше расскажите в комментах вашу любимую (реальную) историю о помощи родственнику с компьютером, смартфоном, цифровым сервисом, интернетом и всяким прочим диджиталом.Например ""Однажды я рассказал дедушке, что такое криптовалюта, а через месяц он попал в списки Интерпола как основатель финансовой пирамиды на блокчейне"". Или ""Научил маму пользоваться Midjourney, и теперь он рисует за неё WhatsApp-открытки на праздники"". Но желательно со всеми подробностями!Среди этих историй мы 18 июля выберем самую залайканную, определим победителя и торжественно направим ему наш справочник в твёрдом переплёте! Вот такой:А если справочник открыть, он будет выглядеть примерно такВремени до подведения итогов ещё много, так что предвкушаем ваши истории!А если историй на примете нет, и облегчить помощь любимым родственникам всё равно хочется — скачать справочник в PDF можно вот здесь. Там же можно почитать истории, которыми с нами делились на первых этапах. Но лучше всё-таки поделиться своей :)  ",831,8
Специфика IT в биотехе: требования GxP,https://habr.com/ru/companies/biocad/articles/743364/,2023-06-22T15:46:01.000Z,"Привет, Хабр! В этой статье мы поговорим о том, что такое валидация компьютеризированных систем и как регулируется использование ПО в фармацевтическом секторе.  Производство лекарственных препаратов прошло большой путь от сбора и приготовления отваров и настоев из трав во времена, когда человечество только начало экспериментировать с вытяжкой и дистилляцией, до сегодняшнего дня — в состояние огромной индустрии, где сплошь и рядом используются автоматизация, ультрасовременное лабораторное и производственное оборудование. Вместе с этим появились стандарты и правила, большинство из которых в той или иной степени направлены на обеспечение качества лекарственных препаратов и улучшение качества жизни конечного потребителя (или пациента, что в контексте применения лекарственных препаратов значит то же самое).На практике это означает следующее: есть правила, стандарты и нормы, которым фармацевтические производители должны следовать, и касаются они множества аспектов деятельности такой компании, начиная от системы менеджмента качества (привет, ISO 9001) и заканчивая работой и функционированием отдельно взятых типов помещений на фармацевтическом производстве. И здесь, отвечая на вопрос, как именно фармкомпании должны следовать правилам и стандартам, мы переходим к валидации компьютеризированных систем, о которых мы вам сегодня и расскажем.Регуляторные особенности IT в фармеСама по себе валидация КС (для обобщенного понимания можно заменить КС на ПО, хотя в понятие «компьютеризированная система» закладывается несколько больше, чем софт, — об этом далее) является специфической областью для производителей лекарственных препаратов и аналогичной продукции и представляет из себя комплекс мер по обеспечению качества, соответствия принципам целостности данных и по поддержанию работоспособности систем, основанных на использовании программного обеспечения на фармацевтическом предприятии. В принципе, можно представить валидацию как несколько видоизмененный процесс QA‑тестирования, однако в самом процессе валидации есть несколько специфических отличий, о которых речь пойдет далее.Для того чтобы понять, для чего необходимо валидировать системы в фармпроизводстве, обратимся к истории регуляции, а именно к тому, каким образом в индустрии появились понятия «автоматизированное производство» и «валидация»:1968 — Medicines Act (Orange Book), GMP, UK. Первое появление термина GMP — Good Manufacturing Practice, или, если по‑русски, Надлежащая производственная практика.1978 — WHO, Международная программа по взаимодействию в области фармаконадзора, Центр мониторинга в Уппсале.1990 — основана ICH (The International Council for Harmonisation), гайдлайны ICH находятся в открытом доступе, например, гайдлайны по качеству, которые относятся к GMP.1991 — основание GAMP, первый гайдлайн опубликован в 1994, является частью ISPE с 2000 года. GAMP — Good Automated Manufacturing Practice — (Надлежащая практика автоматизированного производства) в текущей версии (5-е издание, 2-я ревизия) описывает подход, требования и методологию для валидации компьютеризированных систем. В версии 5.2 упор делается на Agile‑методологии и использование итеративных методик разработки и тестирования для обеспечения соответствия требованиям компьютеризированных систем.Да, коллегам из IT, возможно, покажется странным, что мы должны руководствоваться сводом правил, описанных во внешней документации. Однако это не просто правила — это требования, которые должны обеспечить конечному потребителю лекарственные препараты максимального качества, и GxP обеспечивает свод правил для различных аспектов производства лекарственных препаратов.Стандарты GxP GxP (Good x Practice) — свод нормативных правил, которые знакомы всем, кто работает на фармпроизводстве. Их на самом деле достаточно большое количество, самый распространенный, пожалуй, это GMP — Good Manufacturing Practice, Надлежащая производственная практика, которая описывает требования к оборудованию, помещениям, процессам, и даже требования к квалификации и постоянному обучению персонала на производственных площадках. Есть еще GLP — это требования для лабораторий с аналогичным составом. GCP — Good Clinical Practice — Надлежащая клиническая практика, клинические исследования. GDP — Good Delivery Practice — Надлежащая дистрибьюторская практика, требования к цепи поставок, способам доставки и хранения лекарственных препаратов, и ими список не ограничивается.В каждой из них в той или иной степени затрагивается процесс валидации компьютеризированных систем. Например, по GMP нельзя квалифицировать оборудование, если предварительно не была проведена валидация управляющей компьютеризированной системы, в лабораторном контуре (по GLP) нельзя допускать конечных пользователей к использованию системы, если она предварительно не была валидирована.Так что же такое ✨валидация✨?Если отвечать предельно просто, валидация — это процесс сбора требований и тестирования для обеспечения соответствия требованиям пользователя, регулятора и требований в области целостности данных. Вообще, на целостность данных в фармацевтическом производстве делается особый упор. Коллеги, которые занимаются информационной безопасностью, могут вспомнить триаду CIA: Confidentiality, Integrity, Availability — и будут правы, так как принцип целостности данных идет из требований к безопасности систем. Процессы резервного копирования и восстановления, контрольный/аудиторский след, процессы авторизации в системах — все это в том числе проверяется на валидации.Что же такое компьютеризированная система? Это комплекс, который представляет из себя управляющее ПО, функции этого ПО, связанное оборудование и процессы. Тут стоит отметить: если ПО не функционирует без оборудования (например, является частью производственного оборудования или не имеет возможности локальной установки в виде пользовательского приложения), то по классификации GAMP такое ПО не является компьютеризированной системой и проверяется как часть оборудования (как правило, в процессе квалификации оборудования).В валидации основными вехами являются собранные, утвержденные и подписанные документы, так как еще одним аспектом валидации является прохождение аудиторских проверок. Аудиты — отдельный регуляторный процесс для фармкомпаний, например, тот же сертификат соответствия требованиям GMP/GLP нельзя получить, не пройдя проверку сторонней организацией. И в контексте документальной фиксации есть еще один важный аспект — прослеживаемость, что можно понимать как способность отследить историю объекта (системы, требования, спецификация) посредством документирования его свойств и состояний.Собрались мы, например, валидировать версию SCADA‑системы. Для себя в компании мы условно разделили процесс валидации на три стадии:1) Планирование и сбор требований;2) Подготовка к тестированию и тестирование;3) Отчет о тестировании.То есть, если мы запланировали валидацию для определенной версии SCADA‑системы, пакет документов должен иметь соответствующие кодировки по СМК (привет, ISO 9001) и быть прослеживаемым от начала и до конца процесса.Сбор требований, как пользовательских, так и функциональных, в нашем случае разделен на два документа — URS (Спецификация пользовательских требований) и FS (Функциональная спецификация), тот же подход можно найти в GAMP 5.2. Дальше нам нужно оценить риски этих требований, и тут важно упомянуть название основополагающего документа GAMP —  A Risk-Based Approach to Compliant GxP Compliant GxP Computerized System — Риск-ориентированный подход к компьютеризированным системам, соответствующим требованиям GxP. Для того чтобы правильно протестировать систему на валидации, нам необходимо оценить риски, которые связаны с тем или иным требованием, и закрыть такие риски комплексом предупреждающих мероприятий, например тестирование, написание пользовательских инструкций и/или процедур, обучение пользователей системы и так далее.Когда требования собраны, риски оценены и тестирование запланировано, логично перейти к самому тестированию. В валидации есть три этапа тестирования:IQ — Installation Qualification – Квалификация монтажа. Первый этап тестирования,      на котором проверяется установка системы на необходимое окружение (тестовое,      валидационное или продуктивное).OQ — Operation Qualification — Квалификация функционирования.      На этом этапе проверяются функциональные требования к системе, по одному функциональному      требованию на один тест.PQ — Performance Qualification — Квалификация эксплуатации. На данном этапе проверяется      готовность системы к тому, чтобы отдать ее пользователям, например, оконечные      процессы (end-to-end-тестирование) в реальных условиях, наличие инструкций/СОПов      (без этого нельзя отдавать систему пользователям в GxP-окружении, привет, ISO      9001 и GAMP), а также дополнительное тестирование на соответствие регуляторным      требованиям, если применимо.После того как все стадии валидационного тестирования завершены, инструкции и процедуры написаны и согласованы, можно документально оформить снижение уровня рисков, который был установлен изначально на этапе анализа рисков, и отчитаться о том, что валидация завершена.Для чего это делается?Тут я позволю себе воспользоваться нейросетевой поддержкой и процитирую ответ от ChatGPT:Валидация систем в GxP-контуре (Good Practice, включает Good Manufacturing Practice, Good Clinical Practice, Good Laboratory Practice и другие) имеет ряд причин, связанных с обеспечением надежности, контроля и соответствия стандартам качества и безопасности. Основные причины проведения валидации систем в GxP-контуре:1. Обеспечение качества и безопасности продукции. Валидация гарантирует, что система способна выполнять свои функции предсказуемо и согласно установленным требованиям, что обеспечивает и контролирует качество и безопасность продукции на всех этапах жизненного цикла (разработка, изготовление, контроль).2. Соответствие регулирующим требованиям. Валидация систем является обязательным требованием для соответствия регуляторным стандартам. Это позволяет доказать контролирующим органам, что компания отвечает всем законодательным требованиям, а также обеспечивает целостность и надежность данных.3. Снижение риска и потерь. Валидация позволяет выявить и устранить потенциальные технические, процессуальные и системные проблемы еще до их возникновения, что минимизирует возможные риски и предотвращает потери.4. Экономия затрат и ресурсов. Валидация обеспечивает надежность и оптимальную работу систем, что ведет к снижению издержек на техническую поддержку, отказы, а также позволяет использовать ресурсы предприятия более эффективно.5. Обеспечение контроля и трассировки процессов. Валидированные системы позволяют сохранять полную историю изменений, а также предоставлять доступ к аудиторской дорожке, что облегчает контроль процессов, обеспечивает трассировку всех действий и обнаружение проблем.6. Улучшение взаимодействия с регулирующими органами. Валидированные системы обеспечивают документацию и доказательства, необходимые для предоставления контролирующим органам в случае проверок и аудитов. Это способствует быстрому и прозрачному взаимодействию с регуляторами и обеспечивает доверие к компании со стороны контролирующих органов.7. Укрепление доверия пациентов. Валидированные системы предприятий позволяют гарантировать высокое качество и безопасность продукции, что укрепляет доверие пациентов к предприятию и его продукции.В завершение хочется отметить, что валидация компьютеризированных систем — действительно специфическая область для фармацевтических компаний, и те, кто работает в других отраслях, возможно, никогда не слышали о таком направлении, однако оно есть. О развитии направления валидации, так называемых ""best practice"" и новых подходах к валидации компьютеризированных систем, поговорим в следующей статье.  ",508,1
"Сказ про то, как царь Пётр… (зачёркнуто) джун-тестировщик в одиночку свой первый проект взял",https://habr.com/ru/articles/743402/,2023-06-22T15:11:39.000Z,"Привет, хабровчанки и хабровчане! Меня зовут Настя. На данный момент работаю младшим ручным тестировщиком в небольшой компании. Свой путь в QA я начала около полугода назад. Перед тем, как устроиться на нынешнюю работу, взялась за самостоятельное тестирование одного проекта – об этом опыте и хочу рассказать.Мои советы будут полезны, если ты тоже джун и: • попал на первую работу, где сразу надо брать и делать, а у тебя все ещё лапки; • задумался о фрилансе, но не понимаешь, как взяться за тестирование в одиночку; • оказался единственным тестировщиком в команде, слышавшей про тестировщиков только из рекламных баннеров. Когда обнаружил, что ты единственный тестировщик на проектеКомпания, с которой я сотрудничала, как раз прямиком из третьего пункта. У них сильная разработка, но нет своего отдела тестирования. Из-за загруженности программисты не всегда успевают с проверками – и тут настал мой час. Сразу оговорюсь, что пригласили меня на проект по знакомству. Много лун назад я работала в этой компании в BD-отделе. Когда ребята узнали, что я только что отучилась на QA-джуна, предложили протестировать их новый сайт в качестве фрилансера. Понимаю, что так везёт далеко не всем. Всё же когда настанет время твоего первого проекта, пусть у тебя под рукой будет шпаргалка с идеями, как выстроить работу.Мои советы – от джуна джунам:Общайся  Первая заповедь тестирования – выяснить требования к продукту. Высока вероятность, что вся документация лежит только у разработчиков в голове. Придется её оттуда доставать.Это был как раз мой случай – кроме макета сайта в PS, требования отсутствовали. Рецепт: проявить инициативу и пригласить в Telegram-чат программистов, проджект-менеджеров и дизайнеров, чтобы можно было оперативно выяснять все детали. Расставляй приоритетыПолучив информацию у ПМ и разработчиков, можно продумать план тестирования. Начни с выстраивания приоритетов – ведь нужно уложиться в дедлайн и не упустить ничего важного. Например, на моем проекте в приоритете были верстка, страницы контактов и кейс-стади. А вот гигантский блог можно было не проверять досконально. Также проверкой бэкенда занимался лично старший разработчик. Выяснив это, я сберегла силы и время.Рецепт: спрашивать и ещё раз спрашивать. Ты джун – к тому же пришедший со стороны, если речь идет об аутсорсе или фрилансе. Задавать вопросы нормально и, вероятнее всего, команда будет к этому готова. Обговори условияДля эффективного тестирования тебе понадобится код-фриз. Это не всегда приходит в голову команде, для которой сотрудничество с QA в новинку. Расскажи, что тебе нужна тестовая среда, совпадающая с продом. И постарайся убедить разработчика, что на период тестирования код желательно не менять – иначе может пропасть смысл твоей работы.На моем проекте во время тестирования меняли фон для большого слайдера на главной странице. Это повлияло на верстку, и пришлось заново проходить несколько проверок.Рецепт: объясни механизм тестирования команде. Если на проекте ещё ни разу не было тестировщика, другие сотрудники просто не привыкли с ним работать. Когда во время тестирования разработчик внес одну новую строчку кодаОбозначь свои возможности и ограниченияВряд ли ты сможешь провести полноценное нагрузочное тестирование или устроить пентестинг. Также какие-то части продукта могут быть недоступны для тестирования. Например, в моем случае нельзя было работать с базой данных, и это нормально. Но иногда у команд, никогда не имевших дело с тестировщиками, возникают завышенные ожидания. Чтобы избежать неприятностей, обговори, что именно ты умеешь и не умеешь делать. Рецепт: расскажи команде о своем стеке. Например, ты знаешь Postman и можешь замокать бэкенд, пока над ним трудятся разработчики. Но не построишь за неделю систему автотестов. ДокументируйДаже если ты единственный тестировщик проекта, составляй хотя бы чек-листы. Они помогут в первую очередь тебе самому не запутаться в проверках.Также обрати внимание на баг-репорты. Если это проектная работа, скорее всего, ты уже будешь занят чем-то другим, когда за эти баг-репорты возьмутся разработчики. Пусть все формулировки будут максимально понятными. Обязательно добавляй скриншоты и записи экрана. Рецепт: визуализируй чек-лист – например, в Miro. Так может быть проще разобраться в структуре сайта и распределить проверки.Пример моих баг-репортов на этом проектеА это mindmap, в котором я структурировала проверки и обозначила успешно пройденные тестыНе жди мгновенных результатовВозможно, найденные тобой баги исправят в ту же неделю. Возможно, на это уйдёт месяц. А может быть, разработчики и дизайнеры не посчитают их багами вовсе. Твоя задача –  сообщить о состоянии продукта то, что сможешь. Например, я месяц спустя вижу на сайте несколько ошибок, на которые указывала в своих отчетах. И это тоже нормально – значит, эти баги не в приоритете, или команда пока до них не дошла. Рецепт: пойми, где заканчиваются твои задачи – и отпусти эту ситуацию :)Подведу итогиКак можно раньше переходи к практике. Теория важна, но работа с реальными проектами и командой – бесценна. Не бойся ошибаться. Ты все ещё учишься и набивать шишки, задавать вопросы, не знать чего-то – нормально. Не откладывай на потом. Может казаться, что после обучения тебе не достаёт знаний, чтобы поработать с “живым” сайтом или приложением. Постарайся оценить себя более объективно – без страхов, предрассудков и синдрома самозванца. Подумай, что ты уже умеешь и чем можешь помочь продукту. И вперёд :)  ",1.8K,2
Нужен ли в архитектуре скрам-мастер: история одной команды,https://habr.com/ru/companies/ru_mts/articles/743398/,2023-06-22T15:00:01.000Z,"«Да кто такой этот ваш аджайл?! Мы же не продуктовая команда!», «И как меня угораздило в это вляпаться?!» — такие выражения (и много чего еще) я часто слышала на командных встречах архитекторов в роли агента изменений.Всем привет! Я – Мадина, скрам‑мастер в Департаменте управления технологиями МТС, у нас это подразделение называют Департаментом Technology Governance (TechGov).Наше подразделение (относительно недавно созданное) занимается управлением технологиями на уровне всей компании. Если коротко – структура Департамента представляет собой Центры компетенций и Центры практик.Центры компетенций описывают практики, методологии и стандарты, центры практик занимаются внедрением и масштабированием этих практик по всем продуктам компании. Трансформация, а тем более технологическая – дело непростое, и без применения гибких методологий тут не обойтись. Agile не ограничивается только продуктовыми командами, любое подразделение в компании может получить пользу от внедрения этой методологии. Ведь Agile – это не просто модное слово, это философия работы, которая позволяет улучшить процессы и более эффективно достигать поставленных целей. Тем более, наш департамент занимается Продуктовой трансформацией в том числе, а это значит мы должны следовать принципу Eat your own dog food (практика использования компанией или командой разработчиков собственных сервисов и продуктов).Но одно дело (тоже весьма непростое) — внедрять скрам или канбан в командах разработки, где есть понятный инкремент и стейкхолдеры. И совсем другое — внедрять гибкие подходы в Центрах компетенций или практик. Таких, например, как архитектура, управление производственным процессом, R&D или даже сам Центр практик Agile.Мой путь агента изменений был нелегок и тернист, я набила кучу шишек, случались и провалы. Но я извлекла из этой истории много уроков и, кажется, мы с командой смогли нащупать верную дорогу, используя здравый смысл, и начали ощущать пользу от внедряемых изменений.Итак, от директора по развитию технологий TechGov поступил запрос на скрам-мастера в Центры практик и компетенций управления корпоративной архитектуры. Запрос звучал примерно так: «Нужен гибкий подход к реализации задач. Продуктовые практики при разработке стандартов и методологий».Ключевые стратегические направления, которыми занимается Центр:ArchOps;Бизнес-архитектура;CloudNative;Архитектурные стандарты;Технологический радар;Open/Inner Source;и другие направления.Стоит ли говорить о том, как было тяжело первое время вникать в суть дискуссий и задач на командных встречах? Например, тема встречи такая: «Обсуждение метамодели аналитических и архитектурных артефактов». Я гуглю: «Что такое метамодель?» и получаю ответ: «Метамодель – это модель, описывающая другую модель».Эммм... спасибо…Что я увидела на старте: каждый сотрудник занимался своим, на первый взгляд, узкоспециализированным делом и особо не заботился о том, что происходит за пределами своей области ответственности. И да, до меня команда уже работала в спринтах, проходили синхронизирующие встречи. Было множество митингов, презентаций и писем друг другу, но на практике реализация и внедрение проходили туго. Спринты закрывались и открывались формально, никаких процессных метрик не велось. Во избежание бунта я решила начать процесс изменений поэтапно. Сначала встретилась с каждым членом команды на индивидуальной встрече, и мы заполнили шаблон STATIK.Шаблон STATIKОсновной моей задачей было погрузиться в работу центра и понять, какие проблемы есть на поверхности.Вот какие проблемы мы вместе обозначили:много зависимостей, которые очень сложно решаются или не решаются совсем;непрозрачный канал коммуникации между руководством, смежными подразделениями;«я не понимаю, чего от меня хотят».Пообщавшись с директором и каждым участником команды я для себя сделала вывод, что команда находится примерно на красном уровне развития, если оценивать по спиральной динамике: А значит, нам надо постепенно выходить на синий уровень, а это инструкции, правила, постоянные ритуалы. Если короче – дисциплина.Этап 1. И тут мы пришли к гигиене Jira…  Что было (тут прошу понять и простить за «англицизмы», но в нашей профессии без них никак:)):Lead time* задач несколько месяцев, выполнение запланированных задач на спринт только 20%. Задачи и истории не соответствовали объектной модели**, принятой в департаменте.*Lead time задач — промежуток времени, необходимый для выполнения задачи.**Объектная модель Jira — это совокупность объектов (epic, story, task) их свойств, параметров и взаимосвязей (звучит почти также как объяснение термина метамодель, знаю😅).Очевидно, что для управляемости и прозрачности ситуацию нужно было улучшать.  И тут начались вопросы: «а зачем?», «а почему?», «я не хочу опускаться на уровень задач, мне достаточно эпика на квартал, это мой фокус». Но, конечно же, для меня, агента изменений, это все не ново. Где-то вопросы решались уговорами, где‑то «шантажом», где-то пряником, а где-то кнутом.И тут есть немаловажная деталь — это полная поддержка директора. Он четко понимал ценность всех этих шагов и помогал мне на каждом шагу, донося до команды, что я не просто какой‑то «внешний коуч», а такая же часть команды, нацеленная на результат.Постепенно с командой мы стали улучшать метрику «план/факт» по спринту. То есть ребята научились планировать как минимум на 2 недели с фокусом на какую-либо ценность.Я настроила дашборды (информационные панели), которые визуализировали работу и фиксировали «нарушения» гигиены и нашей объектной модели. Кажется, теперь не надо уже вести кучу разной отчетности, разрозненные таблицы в Confluence с отчетом о прогрессе по какому-либо направлению, ведь все описано в Jira, по результатам приложен артефакт и достаточно в плагине Structure посмотреть ход выполнения задач. Выглядит это примерно так:А вот так выглядит завершенная работа по спринту во встроенных дашбордах Jira:Причем доступ к задачам есть у любого сотрудника департамента, каждый может посмотреть прогресс и чем занимается центр.По регулярным событиям: помимо планирования/обзора спринта у нас появились ежедневные короткие стендапы и ретроспективы. На начальных этапах казалось, что ежедневные встречи для синхронизации — это лишнее время, которое можно потратить на более полезную работу. Но постепенно команда осознала, что такие встречи помогают нам следить за прогрессом каждого участника и оперативно решать проблемы. Эти стендапы стали нашей ежедневной рутиной, без которой мы уже не можем представить работу.А ретроспективы – это место, где мы обсуждаем насущные проблемы и пытаемся их решить.Чего удалось достичь на первом этапе: появилась сфокусированность на конкретных задачах, ребята стали больше общаться друг с другом, научились решать зависимости друг перед другом более эффективно. Улучшение гигиены Jira помогло решить вопрос с прозрачностью, все реже и реже мы слышим от руководства вопросы о том, кто чем занят и какого результата удалось достичь по тем или иным активностям, так как к любой задаче всегда приложен артефакт.Этап 2. Взаимодействие снаружиТеперь, когда мы более или менее разобрались с работой «внутри», пора понять, что происходит у нас «снаружи». Тем более, что ком зависимостей и проблем во взаимодействии копился все это время во всем департаменте. Очень много работы зависело от смежных центров/платформ/отделов. Мы то и дело слышали: «Ну я встречу закинул, а он не пришел». «Ну мы встречу провели, а ничего не меняется». «Ну я отправил письмо, а ответа нет».Вывод напрашивается сам: нужна общая «синхровстреча». Мы в Центре практик Agile всей командой методологов и скрам мастеров долго думали, как эту проблему решить. Поначалу были попытки проведения таких встреч онлайн, но каждый раз то ключевые сотрудники не подключались, то вовлечение было минимальным.Поэтому мы решили собрать всех в одном месте и в одно время, чтобы сначала обозначить друг другу зависимости (пресинк), а потом после цикла квартальных планирований рассказать. приняли ли мы обязательства по выполнению зависимостей или нет (постсинк). Для нас это был эксперимент, мы не очень представляли, сколько зависимостей есть по факту и есть ли смысл в проведении такого мероприятия. Чисто интуитивно понимали, что оно необходимо. Что интересно, что и сами сотрудники считали, что зависимостей не так много. И тут мы подготовили доску в Miro, где попросили весь департамент обозначить зависимости, чтобы на встрече уже предметно их обсуждать. И вот как выглядела доска: Вместо тысячи слов…Итак, встреча проходила ровно 1 день в офлайн формате. Собрать всех в одном месте стало тем еще приключением. Мы долго прорабатывали адженду, обсуждали, как настроить эффективную работу в группах, артефакты и образ конечного результата. Встреча была проведена, многие зависимости были проговорены вообще впервые. Но уже к обеду опрос показал, что очень многие смогли договориться:Обратная связь от коллег была более чем позитивнойЧто же касается центра архитектуры, то на пресинке к архитекторам выстроились очереди из зависимых центров. Это очень четко подсветило, насколько важные задачи решает этот центр и сколько коллег в буквальном смысле слова от него зависят.После проработки зависимостей, мы пришли к тому, что в Центре архитектуры пора вводить практику проведения PI Planning.Этап 3. Масштабируем и каскадируемТеперь наступило время погрузиться в процесс внедрения и масштабирования практик, которые разрабатывает центр корпоративной архитектуры по всей компании. Давайте вернемся к структуре, которую я описывала ранее более детально:Как происходит сейчас:Центр компетенций разработал стандарт и отдал его в Центр практик на внедрение. Центр практик встречается с Главными архитекторами и рассказывает о стандарте и о том, как его внедрить. На «уровне техники» это воспроизводится процессом каскадирования эпиков. То есть Центрами компетенций и практик прорабатываются эпики и 1 раз в квартал эти эпики массово каскадируются* (назначаются) на Главных архитекторов. Например: необходимо, чтобы все продукты компании внедрили у себя определенный стандарт. Для этого создается эпик, описывающий этот стандарт и какие действия необходимо проделать, чтобы его внедрить. Далее эпик массово клонируется (каскадируется) на представителей всех кластеров компании, а они уже клонируют этот эпик в продукты. У продуктов есть требование на реализацию стратегических эпиков технологической трансформации (от нашего департамента) в виде 25% от бэклога, поэтому они выбирают скаскадированные эпики, которые смогут «осилить» за квартал.Так, главные архитекторы кластеров также массово каскадируют «архитектурные» эпики на продукты. Все это происходит в асинхронном режиме с «посредниками» в виде центра практик Стратегия.А теперь представьте масштабы нашей компании и продуктов в ней. То есть, когда мы каскадируем массово эпики на все продукты и ожидаем, что эпики должны быть реализованы в срок, кажется, что это заранее невыполнимый KPI.А вот как это выглядит со стороны продуктов — когда к тебе 1 раз в квартал «прилетает» эпик (даже очень хорошо описанный и с понятной ценностью) все равно непонятно как «впихнуть» это в свой бэклог, когда там висят очень важные и горящие идеи для бизнеса. Вот и получается «кидание мяча» от одних к другим, претензии и недовольства.Видится, что формат PI-планирования — очевидный шаг, который поможет синхронизации и выравниванию. Во втором квартале мы уже провели пилотное PI-планирование сначала внутри центров архитектуры (что, кстати, помогло решить проблему «я не понимаю, чего от меня хотят»), в следующих кварталах планируем подключить к этому Главных архитекторов, чтобы процесс был обоюдовыгодным и каждый чувствовал свою сопричастность и ответственность за результат.Циклы планирования будут выглядеть так: И, наконец, этап 4. Метрики, исследования и выводыВнедрение продуктовых практик мы начали с базового тренинга по продуктовке. Теперь мы прорабатываем дерево метрик и проводим исследования эффекта от внедрения той или иной практики. Результатом этой деятельности должна стать ситуация, когда Владельцы продуктов сами будут обращаться к нам с просьбой помочь внедрить ту или иную практику, видя, как у соседнего продукта стали улучшаться показатели. В целом, внедрение Agile в центр корпоративной архитектуры стало сложным процессом, но правильная организация работы помогла достичь успеха. Сегодня, как мне кажется, мы стали более продуктивными и адаптивными к изменениям, более открытыми для общения и сотрудничества в команде, непрерывно улучшаем наши метрики, процессы и инструменты. И эти достижения стали возможными только благодаря командной работе и полной поддержке руководства.Впереди еще много работы, ведь трансформация — это непрерывный процесс.Можно сказать, что мы уже на четвертом уровне. Как достигнем пятого – ждите еще одну статью:)Совместим ли Agile и Центры архитектуры? Думаю, что однозначный ответ — да. Я, наверное, впервые услышала от руководителя Центра фразу: «Первый раз вижу скрам-мастера, который приносит пользу».P.S. Если у вас есть вопросы – с радостью отвечу на них в комментариях. Там же жду ваши истории о внедрении Agile в непродуктовых командах и опыте работы со скрам-мастерами.  ",1.3K,33
Как ChatGPT учил меня в Dota 2 играть,https://habr.com/ru/companies/X5Tech/articles/743396/,2023-06-22T14:58:15.000Z,"Привет, Хабр! Я Настя, старший системный аналитик в Х5 Tech. Недавно я начала знакомство с нейросетями, а именно, с ChatGPT. В основном я  использовала его для быстрого поиска информации по работе. Потом я пошла дальше: я попросила ChatGPT научить меня играть в Dota 2. Цель была такая – пообщаться с ChatGPT и понять, можно ли его рекомендации использовать для игры. Здесь я должна уточнить: во-первых, я уже играла в Dota 2. Я пришла в неё в 2020 году. Играла я не одна, поэтому гордая роль саппорта-пятёрки досталась мне. Я не калибровалась, но, по ощущениям, MMR у меня на уровне 1500. Во-вторых, Dota 2 – игра сложная. Победа в ней зависит от большого количества факторов: от нужных предметов для твоего персонажа до способности быстро реагировать и принимать решения. Основы Dota 2Первое, что я решила узнать – а что вообще ChatGPT знает про Dota 2? Такой вопрос я и задала.Получается, что он выдал мне общую информацию про игру, Valve и The International. Забавно, но я всегда переводила Ancient как “Древние”, а чаще всего это был просто “Трон”. Ну и насчёт того, что “игра обновляется и развивается, добавляя новых героев” – тоже сомнительно. Хорошие патчи, которые сильно влияют на игру, выходят раз в 2-3 года. До текущего патча 7.33 был патч 7.28, в котором появился Aghanim’s Shard. Он вышел в декабре 2020 года, а 7.33 – в марте 2023 года. Следующий вопрос был тоже базовый: знает ли ChatGPT, как зовут Кентавра?Честно сказать, я удивилась. Мне даже не пришлось уточнять про Centaur Warrunner или про то, что это герой Dota 2. Приятно, хвалю, идём дальше. Дальше я перешла к конкретике. Я попросила ChatGPT рассказать про вышеупомянутый патч 7.33 так, чтобы было понятно новичку. Я ожидала, что это будет не лонгрид про изменения всех героев, а просто ключевые моменты, рассказанные простыми словами. Я испытала разочарование. Dota 2 – динамичная игра, и выбор героев и предметов во многом зависит именно от текущих обновлений. В 2023 году ChatGPT может научить тебя играть в Dota 2 2021 года, что не круто. Дальнейшие его ответы будем анализировать с точки зрения того, что про последний большой патч ChatGPT не знает. Затем я проверила, знает ли он что-то про 7.28, ведь этот патч укладывается в обновление. Ну про него он знает, но мне всё равно ответ не понравился. Я хотела, чтобы вместо оборотов вроде “для более сбалансированной игры” было что-то более конкретное. С другой стороны, он не выдал лонгрид, так что в целом неплохо. Гайд по патчуДальше я углубилась в специфику и узнала про метовых героев 2021 года. Про метовых героев текущего патча узнать не получится.  Начало хорошее, он понял про позиции и уточнил: “на момент моего обновления”. Корейка? Как будто кто-то в Миду кусок свинины положил. Очень странно. В 7.28 я действительно часто видела их. Хорошие были герои для Мида, тут нечего добавить. В 7.33 ситуация другая. Популярные и эффективные герои в Мид почти все Спириты: Void Spirit, Ember Spirit и Storm Spirit. Среди них затесался Shadow Fiend, который тоже достаточно популярен и не бесполезен в любом патче. У Invoker и Templar Assassin хороший винрейт сейчас (45.09% и 45.76%), но они не очень популярны. Откуда я взяла эти винрейты? Это средняя цифра с сайта https://ru.dotabuff.com. Я взяла статистику доли пиков и побед по линиям (мид, хард и лэйт). Это была статистика за месяц (середина апреля-середина мая 2023 года) по всем рейтингам, регионам и сторонам (Radiant и Dire).В 7.28 это и правда были крутые и очень эффективные Керри. Их часто пикали, особенно Faceless Void. В 7.33 ситуация немного другая. Сейчас очень популярны и эффективны Phantom Lancer (win 53.36%), Medusa (win 58.37%) и Slark (win 50.95%). У Faceless Void и Spectra тоже неплохой винрейт (46.89% и 52.17%), но по популярности они сильно уступают той же Medusa. Тут ChatGPT снова начал предлагать странные слова, но этому я ещё могу придумать оправдание, в отличие от “Корейки”. Я думаю, что это пошло из игр времён первых The International, когда Оффлейнер часто стоял один на линии – соло. В 7.28 прям в точку – хорошие герои для Оффлейна, особенно Mars. В 7.33 Axe выигрывает по популярности и винрейту – 53.94%. Вместе с ним очень популярна Legion Commander (win 55.24%). У Mars дела похуже – он не очень популярен и винрейт у него 44.66%. Здесь я сразу предъявляю претензию, потому что саппортов в игре два: semi и full, или hard и soft, или четвёрка и пятёрка. Разница между ними большая. Герои в этих случаях нужны разные. В 7.28 Lion и Witch Doctor часто встречались именно на позицию пятёрки. Тот же Rubick тоже часто встречался, но уже на позиции четвёрки. В 7.33 ситуация немного поменялась. Кроме Chen – он был популярен и в 7.28, и в 7.33. Я удивилась, что ChatGPT не предложил его. Пятёрка – это Crystal Maiden, одна из самых популярных с винрейтом 52.59%. Четвёрка – это Pudge, также один из самых популярных с винрейтом 49.86%. Lion популярен на пятёрку, но винрейт уже похуже – 45.53%, на четвёрку – 45.26%. Witch Doctor также популярен на пятёрку, но его винрейт сильно приятнее – 49.06%, на четвёрку – 49.60%. Ну и отдельно отмечу, что ChatGPT сам упомянул про то, что метовые герои меняются от патча к патчу и что нужно дополнительно искать информацию по мете. Дальше узнаю рекомендации ChatGPT по сборкам предметов для тех метовых саппортов, которых он выбрал. Словарь ChatGPTТак, давайте разбираться со словарём ChatGPT. Я слабо понимаю, что такое “Защитный щит”, “Сапфировая палочка”, “Гримуар Глиммера”. Я попросила ChatGPT предложить мне английские названия. Составлю для вас словарь: ChatGPT русскийПредмет из Dota 2МангустMangoТангосыTangoЗащитный щитIron BranchПосох АганимаAghanim's ScepterСапфировая палочкаGlimmer CapeСапфировый шарEul's Scepter of DivinityФортельForce StaffМедаль СветоносногоAether LensКапюшонHood of DefianceГримуар ГлиммераGlimmer CapeМеканаMekansmПро сборки – в целом не так плохо. Есть два критичных момента: Нет ни одного слова про Варды, а это важно – за обзор на карте всегда отвечают саппорты.Нет никаких упоминаний про сапоги – на Lion обычно собирают Tranquil Boots, а на Witch Doctor – Arcane Boots.Дальше хочу пройтись по саппортам более конкретно. Гайд для саппорта Я всё-таки спрошу у ChatGPT про разделения по саппортам.Он непреклонен. Никак не делит на четвёрку и пятёрку и при этом ещё указывает: “Роль саппорта не имеет строгого разделения на подроли”. Я спрашиваю конкретно: в чём разница между пятёркой и четвёркой? Получается, что какое-то деление всё же есть. Ещё у нас новое слово появилось – “ферма”. Я так понимаю, что речь идёт про “Фарм”. Что рассказывает нам ChatGPT про пятёрку:В целом тут всё понятно – фарма мало, поддержки много. Тут уже появляется фраза про то, что саппорт несёт ответственность за размещение “наблюдателей” и “сканеров”. В целом, хорошее описание, но проблема со словарём сохраняется. Тут тоже всё ясно – фарма чуть больше, больше возможности инициировать. Я только не согласна про контроль: у большинства саппортов-пятёрок он есть. Например, EARTH SPIKE у того же Lion или FROSTBITE у Crystal Maiden, которая часто появляется в играх именно как пятёрка. У этих героев нет скиллов на лечение, а ChatGPT предписывает навык или предмет как обязательный атрибут для soft support. Мои два самых любимых героя – Crystal Maiden и Lich. Я попросила ChatGPT написать гайд для этих персонажей с учётом того, что они находятся именно на позиции пятёрки. Начнём с Crystal Maiden и её начальных предметов. Вот в таком запросе мы уже получаем достаточно конкретную сборку. Только я бы ещё Mango добавила сюда. Но опять нет ничего про варды, а это важно – нужно обеспечить своему Керри хороший обзор.Тут у меня тоже претензий нет, стандартно идём к сапогу. Только я бы добавила фразу, что по мере необходимости покупаем расходники как для себя, так и для своего Керри.Здесь я бы только добавила, что Magic Wand можно собрать, если вы решили купить Magic Stick на более ранних этапах. Собирать с нуля после 10-й минуты Magic Wand для меня звучит не очень хорошей идеей. За Tranquil Boots – лайк, другой сапог на этом герое представить сложно. Вместо Aether Lens я собрала бы Force Staff, а лучше Blink Dagger. Появился Blink Dagger. Только не очень понятно, почему люксовый. Также как и Glimmer Cape – это один из самых важных предметов для Crystal Maiden, и он совсем не люксовый никакой. Под люксовый больше подходит Aghanim's Scepter или Aghanim’s Shard. Эти предметы будут сильно полезны и для команды, и для самой Crystal Maiden. С завершающими предметами тоже странно. Force Staff тут точно не должен быть, да и Scythe of Vyse очень сомнителен здесь. Вот как раз сюда тоже подходят Aghanim's Scepter или Aghanim’s Shard. Или та же Aether Lens.Дальше я спрашиваю у ChatGPT про Lich. Тут тоже довольно стандартно всё. Также снова не хватает про варды: ChatGPT забыл, что пятёрка несёт ответственность за размещение “наблюдателей” и “сканеров”. Не знаю, насколько это правильно, но на Lich я люблю собирать Arcane Boots, а здесь он снова предложил Tranquil Boots. Мне нравятся Arcane Boots тем, что потом из них можно будет собрать Aeon Disk или Octarine Core.Ну и вот они – Tranquil Boots. Абсолютно одинаково с гайдом на Crystal Maiden. Хорошие предметы на Lich. Не хватает снова что-то вроде Force Staff или Blink Dagger. Вот тут место Scythe of Vyse мне нравится. Это и правда люксовый предмет, который можно собрать на любого саппорта как дополнительный контроль, если тебе позволяет опыт и фарм. По завершающим – не очень нравятся они мне тут. Их как раз хорошо собирать из Arcane Boots, а ChatGPT рекомендует Tranquil Boots, что неудобно. Ещё мне нравится здесь Octarine Core, но его тоже очень удобно собрать из Arcane Boots. ВыводыНикто не может научить тебя играть в Dota 2. ChatGPT может выдать лишь какую-то общую информацию со скидкой на то, что пока там старые патчи. Правило “Конкретно формулируй запрос” работает и для Dota 2. Чем конкретнее ты его напишешь, тем конкретнее информацию выдаст тебе ChatGPT. В целом, он понимает, что это за игра такая, кто её сделал и даже про The International рассказал. Обновление ChatGPT на сентябрь 2021 года точно не поможет научиться играть. Основные и нейтральные предметы сильно меняются, а иногда и вовсе пропадают от патча к патчу. Приходят новые герои или их сильно меняют, а это напрямую влияет на ход игры. Ответы ChatGPT из-за их специфики точно не помогут новичку, который никогда не открывал Dota 2. А опытным игрокам они будут просто бесполезны, потому что ChatGPT выдаёт достаточно общую информацию, которую опытный игрок и так уже знает.  ",2.7K,4
Протокол идентификации Шнорра совместим с режимом моментальной цифровой подписи,https://habr.com/ru/articles/743206/,2023-06-22T14:55:04.000Z,"В настоящей публикации приводиться описание  модификации протокола идентификации Шнорра, совместимого с режимом моментальной цифровой подписи.ВведениеВ публикации приводится описание интерактивного протокола идентификации Шнорра (далее протокол Шнорра) и сформулирована проблема совместимости этого протокола с режимом моментальной электронной цифровой подписи (МЭЦП).  В настоящей заметке показано как модифицировать протокола Шнорра так, чтобы обеспечить такую совместимость.МЭЦП-совместимый протокол ШнорраНапомним, что некоторые сведения об арифметике точек эллиптической кривой, а также пояснение относительно ECDLP и DLP, можно почерпнуть из этой публикации. В целях упрощения оставим без изменения всё, что касается протокола Шнорра, в том числе пару ключей  и сертификат Если интерпретировать необходимое условие, то МЭЦП-совместимость возможна тогда, когда  и  способны вычислить некоторую общую секретную компоненту. Такой компонентой может быть общий сессионный секретный ключ.  Покажем как это сделать на примере протокола Шнорра. Обозначим криптографическую хеш-функцию как  Внесём в протокол некоторые изменения. Воспользуемся следующим приёмом: “спрячем” эфемериду  в точке кривой. Это приводит к тому, что  также превращается в точку. В результате получим следующий протокол.Сообщения протокола.Действия сторон. выбирает   вычисляет   проверяет условие  Если  то выбирает новое  и заново выполняет необходимые вычисления и проверки; считывает действующий сертификат  и проверяет ЭЦП  где  — общедоступный ключ доверенной стороны  Если  то сессия завершается. Если  то выбирает  и вычисляет   Если  то выбирает новое  и заново выполняет необходимые вычисления и проверки; проверяет условие  если выполняется, то вычисляет   в противном случае сессия завершается; вычисляет  Затем проверяет  Если равно, то доказательство принимается, и отвергается в противном случае.Итак, предположим, что этап идентификации завершился успешно и  принял предоставленное  доказательство. К этому моменту  располагает следующим набором данных:   по построению),  (получил от  в качестве  знает  (получил от  в качестве   (по построению),  (получил от  в качестве  Стороны независимо формируют общий сессионный секретный ключ в соответствии со следующими правилами: вычисляет   вычисляет  Очевидно, что  в силу коммутативности. Следовательно,  Что касается МЭЦП, то формирует подпись  для заданного сообщения  при помощи  с учётом значения хеш-функции  проверяет подпись для некоторого сообщения  при помощи  с учётом значения хеш-функции Преимущество рассмотренного решения в том, что изменения касаются арифметических операций и не затрагивают логику протокола. Это важное обстоятельство, так как любые принципиальные модификации инициируют исследование криптостойкости новой конструкции со всеми вытекающими последствиями.Ранее мы отмечали, что в нашем распоряжении имеется протокол с аналогичной функциональностью, который решает поставленную задачу, но при этом позволяет распознать принадлежность доказывающего к локальному сообществу зарегистрированных участников.Предварительный анализЗаметим, что   известны только  и  соответственно. Тогда  злоумышленник может получить  или  отыскав решение ECDLP при условии   или  соответственно. Также можно найти решение DLP при условии  или  Назначение функции  а также связь между ECDLP и DLP объясняется в Приложении A.  Поскольку  то сформировать МЭЦП может только тот, кто знает  или  а также секретный ключ выбранной для этой цели произвольной схемы ЭЦП.Активный злоумышленник, используя технику перехвата и блокировки, способен навязать  вместо   вместо  и  вместо  при   Например, это могут быть сообщения другой сессии. Однако  с подавляющей вероятностью отвергнет предоставленное доказательство, так как  и  с пренебрежимо малой вероятностью. Кроме этого,   и  с пренебрежимо малой вероятностью.Количество передаваемой информацииВоспользуемся компактным представлением точки кривой [Cohen_etc._2006] (раздел 13.2.5).Точка  задана парой координат  Если  лежит на кривой, то справедливо уравнение  для некоторых  Это уравнение можно интерпретировать как  где  — квадратичный вычет и тогда существует ровно два решения  При  решение  Если  — чётное, то  — нечётное, и наоборот. В общем случае из двух решений всегда одно — чётное, а другое — нечётное.Пусть точка  представлена координатой  Будем передавать по каналу связи  где  — бинарный признак, который позволяет определить нужное решение из пары возможных. Примем за правило, что  означает чётность, а  — нечётность. Например, для  следует получить пару  и затем из этой пары выбрать нечётное решение.Для хранения произвольной точки кривой потребуется  бит памяти. В  квадратный корень вычисляется по алгоритму Tonelli-Shanks асимптотической трудоёмкости  [Cohen_etc._2006] (раздел 11.1.5).Тогда для восстановления точки  выполняют следующие действия. Для заданной координаты  при помощи алгоритма Tonelli-Shanks вычисляют некоторую координату  Если   то  Если   то  Если   то  Если   то Если применяется компактное представление, то для доставки   и  в совокупности потребуется передать  бит. Для сравнения, в исходном протоколе Шнорра при тех же условиях для доставки   и  в совокупности необходимо передать  бит.Оптимизация вычисленийТрудоёмкость зависит от методов вычисления скалярного произведения, а также выбора вычислительной платформы. Так, например, испытания, которые проводились при помощи тестового стенда, реализованного на языке программирования Scala v2.13.6 на аппаратной платформе MacBook Pro (15-inch, 2016) c 4-ядерным процессором Intel Core i7 под управлением macOS Monterey версия 12.6.2  с привлечением примитивов из библиотеки PBC показывают, что за счёт предобработки, возможной для константы  скалярное произведение  в среднем на % эффективнее скалярного произведения  где  Результаты замеров сведены в Таблицу 1.Режим МЭЦП и сертификаты общедоступных ключейСторона  владеет парой ключей  Для общедоступного ключа  выпускается соответствующий сертификат. Необходимо ответить на следующий вопрос: какую пользу можно извлечь из режима МЭЦП при наличии сертификата общедоступного ключа?Пусть для общедоступного ключа  выпущен сертификат  где  — ЭЦП доверенной стороны   — персональные данные  и  — секретный ключ  предназначенный для формирования ЭЦП сертификата.Поставим мысленный эксперимент и в отсутствии режима МЭЦП предположим, что  передаёт  некоторые заверенные ЭЦП персональные данные  При этом  утверждает, что  — его собственные персональные данные. Следует сразу оговориться, эти данные могли быть получены различными способами, например, в результате сговора, применения методов социальной инженерии или просто скопированы из общедоступной долговременной памяти. Для проверки ЭЦП сторона  должна воспользоваться общедоступным ключом  из сертификата  Предположим также, что все необходимые проверки подтвердили подлинность и целостность  В итоге  располагает двумя сертификатами  и  каждый из которых выпущен доверенным удостоверяющим центром. Рассмотрим случай, когда  ( трактуется однозначно). В результате  не в состоянии сделать правильный выбор относительно  или  поскольку отсутствует критерий принятия решения. Очевидно, что подобная неопределённость недопустима.  Если задействовать режим МЭЦП, то принимая решение относительно  или  сторона  опирается на доказательство, которое было предоставлено  на этапе идентификации. Рассуждения стороны  подчиняются следующей логике. Сформировать ЭЦП для персональных данных может только тот, кто одновременно имеет доступ к переменным отдельной сессии протокола идентификации, как общедоступным, так и секретным,  включая секретный ключ для формирования ЭЦП.Предположим, что ЭЦП получена в результате сговора. Для этого злоумышленник должен вынудить владельца персональных данных заверить их своей ЭЦП, но с включением некоторых вспомогательных данных, про которые заверителю ничего не известно. Если исходить из модели сопутствующих рисков, то такое событие представляется неправдоподобным. С подавляющей вероятностью заверитель откажется от включения таких данных, опасаясь нежелательных последствий, например, неявного навязывания долговых обязательств, занесения в чёрные списки, блокировки учётных записей и так далее.Открытая верификации в протоколе ШнорраОсобенность протокола Шнорра в том, что любой, кто имеет доступ в переменным   и  может проверить корректность вердикта, вынесенного  относительно доказательства, предоставленного Действительно, упомянутые выше переменные передаются по незащищённому каналу связи и, следовательно, общедоступны. Поскольку каждый может воспользоваться общедоступным ключом  то совсем не обязательно обладать полномочиями  для того, чтобы вычислить  и затем проверить Практический смысл здесь в возможности контроля действий как со стороны  так и  Это важно при возникновении разногласий. Например, когда  утверждает, что предоставил корректное доказательство, а  это отрицает.ВыводыОценки количества передаваемой информации для исходного протокола Шнорра и описанной выше модификации различаются несущественно.В исходном протоколе Шнорра задействовано три скалярных произведения, тогда как в модификации протокола таких произведений четыре. Поскольку скалярные произведения  и  вычисляются относительно образующего элемента  то возможна оптимизация, которая позволяет сократить вычислительные трудозатраты в обмен на выделение дополнительной памяти, предназначенной для хранения промежуточных результатов (см. Таблицу 1).  Заметим, что в исходном протоколе при вычислении точки  допустима только частичная оптимизация. Однако в модифицированном протоколе при вычислении точек  и  а также скалярных произведений  и  такая оптимизация невозможна.В предложенной модификации протокола открытая верификация нереализуема.ЗаключениеСледует засвидетельствовать, что нам удалось решить поставленную задачу для конкретного протокола сравнительно простым способом. Однако для произвольного протокола идентификации проблема совместимости с режимом МЭЦП продолжает оставаться открытой. В дальнейшем мы предпримем попытку отыскать решение задачи для некоторого известного протокола идентификации, отличного от протокола Шнорра.Текст статьи в формате pdf можно скачать тут.Список литературы[Cohen_etc._2006] Cohen, H., Frey, G., Roberto Avanzi, R., Doche C., Lange, T., Nguyen, K. and Vercauteren, F. Handbook of Elliptic and Hyperelliptic Curve Cryptography, Chapman and Hall/CRC, 2006.  ",827,8
Нейросеть «Жириновский» — на что способна новая нейросеть от Наносемантики,https://habr.com/ru/articles/743394/,2023-06-22T14:54:10.000Z,"Не так давно компания «Наносемантика» представила свою новую нейросеть - «Жириновский», способную на синтез текста, синтез речи, и компиляцию всего названного в видеоряд (Lip Sync).«Наносемантика» пока не раскрывает всех тонкостей нового проекта, однако некоторые детали о ""текстовой"" основе нейросети известны - это генеративная нейросеть с 1 миллиардом параметров, которую с нуля обучали на русскоязычных текстах, и потом подгоняли под формат ассистента датасетом в 150 тысяч инструкций вида ""вопрос/ответ"".Но пока всё это слова, не показывающие результат работы на деле.Поэтому, специалисты компании решили продемонстрировать возможности нейросети, и обратились к «Жириновскому» с вопросом по теме, для раскрытия которой вы и пришли читать данную статью:Достигнет ли Россия такого же уровня развития в области искусственного интеллекта, как США и Китай?И вот, что ответил ""цифровой ученик"" Жириновского: «Конечно, Россия имеет все возможности догнать и превзойти США и Китай по уровню развития рынка искусственного интеллекта. Наша страна обладает богатым научным потенциалом и талантливыми специалистами, которые уже сегодня разрабатывают инновационные решения в области ИИ. С увеличением инвестиций и государственной поддержкой, Россия сможет стать лидером в этой сфере и внести свой вклад в мировые технологические достижения. Пусть наш ИИ говорит по-русски!»Далее компания «Наносемантика» привела в пример ещё 5 областей, где могут быть применены нейросети на тех же технологиях, что и «Жириновский».Конкретно, это нейросети на основе синтеза голоса и машинного обучения:•  Реклама и маркетингБлагодаря технологиям голосового синтеза и машинного обучения вы сможете получать персонализированную рекламу, озвученную любой исторической личностью, персонажем, актером, общественным деятелем. Такой способ может улучшить эффективность кампании и еще больше заинтересовать пользователей в бренде.Вы когда-нибудь слышали, как Киану Ривз разговаривает на русском? Можете представить, что Никола Тесла прорекламирует вам новые розетки в дом при обновлении ремонта?Благодаря технологиям синтеза голоса, это возможно.Итак, давайте представим, как «Цифровой ученик» рекомендовал бы клиентам магазина техники оптимальный вариант планшета по их запросам:«При покупке планшета для рисования нужно учитывать важные характеристики, чтобы не купить полный отстой! Смотрите на размер экрана - чем больше, тем лучше видно свои шедевры. Разрешение должно быть высоким, чтобы каждая деталь была видна. И не забудьте про чувствительность касания — надо, чтобы планшет реагировал на каждый ваш штрих, иначе какой толк от него?»•  ОбразованиеРазработчики систем обучения и дистанционного образования могут использовать нейросети на основе голосового синтеза для создания текстовых материалов и голосовых лекций.Это уменьшит трудозатраты преподавателей, а также поможет добиться интереса учеников к дисциплине. Предположим, вашим преподавателем по истории будет нейросеть по прототипу Владимира Вольфовича, которая расскажет о периоде перестройки:«В период перестройки, в 1989 году, впервые были проведены свободные многопартийные выборы в СССР. Это открыло двери для политического плюрализма и демократических преобразований в стране. Однако, несмотря на это, перестройка так же сопровождалась социальными и экономическими трудностями, которые оказались вызовом для нашего народа»•  ЗдравоохранениеНейросети на основе голосового синтеза могут использоваться в медицине, ведь голосовая коммуникация в приложениях больниц, и других сервисах онлайн-консультаций, будет доступна гораздо большему количеству пациентов.В голосовом формате гораздо удобнее составлять курсы лечения для клиентов, и объяснять им все детали, которые необходимо предпринять для выздоровления.Голосовой ассистент всегда сможет посоветовать курс лекарств и витаминов, а также напомнить об их приеме в нужный срок.А вот и ваш лечащий врач, Владимир Жириновский напоминает о приеме витамина С всем читателям: «Ну, витамин С — это, конечно, полезная штука. Некоторые говорят, что он может укрепить иммунитет, помочь бороться, с простудой и инфекциями. Так что, если хотите быть здоровыми, можете попробовать принимать витамин С в виде добавки или увеличить потребление продуктов, богатых этим витамином, таких как цитрусовые, киви, красный перец. Но, конечно, не забывайте, что витамин С — это всего лишь один из многих важных питательных веществ, так что не забывайте о сбалансированном питании и здоровом образе жизни! Всегда следите за своим здоровьем, чтобы быть готовыми к любым вызовам!»•  ТелекоммуникацииТехнология машинного обучения в телекоммуникации используется уже давно. Вы звоните в банк, искусственный интеллект распознает запрос и направляет вас в нужный отдел.Но что было бы если трубку банка брал Скрудж Макдак, Леонардо ДиКаприо или Владимир Вольфович Жириновский:«Конечно, конечно, как же без банка? Так, дай-ка подумаю. Ах, да! Сейчас все решается одним щелчком пальца. Просто позвони на номер, который обычно указан на сайте банка или на обратной стороне банковской карты. Там тебе оператор с радостью поможет. Не забудь улыбнуться, когда будешь говорить с ними, они такие чуткие, эти операторы банка. Удачи тебе, несчастный клиент!»•  Игровая индустрияРазработчики игр могут использовать нейросети на основе голосового синтеза для создания персонажей и диалогов в играх, а также для распознавания голоса игроков. «Представьте себе игру вроде «Ведьмак 3», только при общении с персонажами, оно свободное по тому же принципу, как у «Жириновского». Персонаж сможет сводить диалог к принятию квеста или определенному сюжету, но вместо выбора строчки текста, у игрока будет возможность писать текстом или говорить с нейросетью. Также, представляем сразу картину, где с помощью наших технологий добавляется голосовое управление. Это кстати один из наших реальных проектов. Вот существует отряд из десяти NPC в шутере, ты говоришь им «лежать», или «зайди в дом», и они выполняют команду. Скоро эта технология будет в большинстве таких игр» Илья Иванов, руководитель видеоигрового направления.И в качестве эпилога, приведу жизнеутверждающую цитату Григория Шершукова - руководителя голосовых проектов и машинного обучения, ведущего разработчика проекта «Цифровой ученик» в компании “Наносемантика”:«Наша компания не останавливается на достигнутом. Это далеко не самая продвинутая модель, у неё лишь порядка 1 миллиарда параметров, и 340 слоёв.В будущем мы планируем модифицировать нейросеть и внедрить 7 миллиардов параметров, разница колоссальная» - Григорий-------------------------------------------Спасибо за ваше внимание!Мой телеграм-канал - там могут быть ещё новости  ",5.3K,30
